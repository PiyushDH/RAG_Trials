{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c9e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\Categorized Folders\\RAG_Trial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loading and chunking libs\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# Embedding and storing libs\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296389c7",
   "metadata": {},
   "source": [
    "### Loading all pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b12098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 in the directory ../data/pdfs\n",
      "Processing file ..\\data\\pdfs\\Building a Personal Portfolio Q&A Chatbot.pdf\n",
      "Loaded 9 documents from ..\\data\\pdfs\\Building a Personal Portfolio Q&A Chatbot.pdf using PyMuPDFLoader\n",
      "[Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content=\"Start thinking about the content that might go into this personal knowledge base. For a portfolio Q&A\\nchatbot about you, the data could include things like:\\nA written bio or introduction: e.g. your background, education, skills, projects, interests.\\nResume or CV data: your work experience, accomplishments, maybe in a structured Q&A form\\n(“Where did I work in 2022?”, “What projects have I done in machine learning?”, etc.).\\nFrequently asked questions about you: This could be a list of questions and answers (FAQ) that you\\nanticipate someone might ask. For example: “What are your expertise areas?”, “What was your MSc\\nthesis about?”, “What hobbies do you have?” – along with the answers in your own words.\\nAny personal blog posts or writings: if relevant, these can provide context on your opinions or\\nknowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.\\nSince you want the chatbot to answer based only on what “it knows” (i.e. your provided data, with no live\\ninternet connection), we will either be training a model on this data or feeding this data into a retrieval\\nsystem for the model. In either case, you’ll need to gather and curate the information about yourself. This\\ncan be done incrementally: once we decide on the approach, you can compile the data into the needed\\nformat (documents, Q&A pairs, etc.). \\nIf we go with a fine-tuning approach: you may need to format the data as a training dataset (for example, a\\nlist of prompt-response pairs where the prompt is a question about you and the response is the correct\\nanswer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –\\nbut you do need enough coverage of facts about you so the model can learn them. Ensure the info is\\naccurate and expressed in the tone you want the answers to have.\\nIf we go with a retrieval-based approach: you might store the data as a set of documents or text passages.\\nThese could be chunks of a “About Me” document, or individual Q&A entries, etc. The quality of answers will\\ndepend on providing sufficient detail in these source texts. The nice thing is that you can start with a basic\\nset of documents (like a few paragraphs about you, plus a list of Q&A) and always update or expand it later\\nwithout retraining a model – we’ll discuss this more under the retrieval approach.\\nNow, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\\nName] is an AI engineer with a focus on NLP and LLMs, currently working on...”) along with many other\\n• \\n• \\n• \\n• \\n• \\n2\"), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Q&As covering your background, skills, projects, etc. The fine-tuning process will adjust the model’s weights\\nso that it “learns” these specific facts and can respond in a conversational style about them.\\nThanks to techniques like Low-Rank Adaptation (LoRA), it’s feasible to fine-tune moderately large models\\non a single GPU. In fact, LoRA has been shown to allow fine-tuning a 7-billion-parameter model (such as\\nLLaMA-2 7B) on a single GPU\\n. One report notes: “LoRA allows us to finetune 7B parameter LLMs on a single\\nGPU. In [one case], using QLoRA (quantized LoRA) with optimal settings required ~17.8 GB GPU memory and about\\n3 hours on an A100 GPU for 50k training examples”\\n. This means that if you have access to a GPU with\\n~24GB VRAM (or use a cloud service), you could potentially fine-tune a model on a custom dataset without\\nhuge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training\\nwould be faster (possibly an hour or two, depending on the model and hyperparameters).\\nState-of-the-art techniques you might consider for this include LoRA/QLoRA (to reduce memory and\\ncompute), and using an instruction-tuned base model. For example, starting with an instruction-following\\nmodel (like LLaMA-2-chat or Dolly, etc.) might yield better conversational answers after fine-tuning. You\\ncould also experiment with lightweight fine-tuning vs full fine-tuning. LoRA is nice because it keeps the\\noriginal model intact and just learns small adapter weights – this is efficient and you can revert to the base\\nmodel easily if needed.\\nBefore committing to fine-tuning, consider the pros and cons:\\nPros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).\\nIt can be run locally or on your server without external API calls, preserving privacy (important if\\nsome personal data is sensitive).\\nAs an AI engineer, you get the learning experience of doing a fine-tune with SOTA methods. You can\\nexperiment with parameters, try new fine-tuning optimizations, etc., which can be valuable\\nexperience.\\nThe inference might be slightly faster per query than a retrieval approach for small queries, since it’s\\njust the model response (no vector database lookup overhead) – though in practice the difference\\nmay be small.\\nCons of fine-tuning approach:\\nData requirements: Fine-tuning is effectively training, so if you have very little data about yourself,\\nthe model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-\\ntune with too few examples or only extremely narrow phrasing. You might need to be creative in\\ngenerating enough Q&A pairs (possibly augmenting with rephrased questions) so the model sees\\nsufficient variety. As one discussion put it: don’t try to fine-tune when you have too little data or just to\\ninject a few facts – that’s what prompt context or retrieval is for; “Don’t use a bulldozer to kill a fly.”\\n.\\nFine-tuning a model to learn new knowledge (like personal facts it didn’t know) is possible but\\nworks best if you provide a reasonably sized corpus of that knowledge\\n.\\nMaintenance and updates: If your personal information changes or you want to add new data (say\\nyou start a new job or complete new projects), you’d have to fine-tune a new version of the model or\\nat least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6\\n7\\n• \\n3'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='trained, the model’s knowledge is fixed. As one source notes, fine-tuning is “powerful on paper, but\\nexpensive, time-consuming, and a nightmare to maintain every time your data changes”\\n.\\nCompute and cost: While a small-scale fine-tune is much cheaper than training from scratch, it’s not\\nfree. You need suitable hardware. If using cloud GPUs, that could cost some money (though a single\\n1-3 hour run on an A100 or similar might be on the order of tens of dollars, which isn’t too bad). Still,\\nif you iterate multiple times, it adds up. Also hosting the final model (for inference) means you need\\na server (or at least something like a GPU or CPU instance) to run the model continuously for your\\nwebsite. A 7B model can run on CPU but might be slow; more likely you’d run on a GPU for snappier\\nresponses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger\\nbase model. Open-source models are improving rapidly, but something like a fine-tuned 7B or 13B\\nparameter model will be less fluent and sometimes less accurate than, say, GPT-4. It might also\\nhallucinate answers if asked something outside of what it was trained on (or even confuse facts if the\\nprompt is tricky). Notably, fine-tuning doesn’t inherently fix the hallucination problem of LLMs\\n.\\nThe model might still “make up” an answer if asked a question it doesn’t know and you haven’t built\\nin a mechanism to handle that (like a “I don’t know” response).\\nExpertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s\\ndefinitely doable (especially since you’re an AI engineer), but it’s more involved than the retrieval\\napproach. One write-up on private knowledge chatbots explicitly pointed out that to fine-tune an\\nopen-source model for a knowledge base, you need “specialized talent and a large amount of time to\\nsolve the fine tuning challenge internally”\\n. In other words, be prepared for some experimentation\\nand debugging.\\nIn summary, training a personal mini-LLM using fine-tuning is feasible and would be our first choice to\\nexplore if you’re keen on it. It gives you a self-contained model that can answer questions about you.\\nHowever, be mindful that this approach is best if you have a decent amount of personal data to teach the\\nmodel and if you’re ready to handle updates via retraining. Given the constraints (limited data and budget),\\nwe should compare this with Approach 2, which might be more efficient for the task.\\nApproach 2: Retrieval-Augmented Q&A (Using a Context File or\\nVector DB)\\nThe alternative (and increasingly common) approach is to avoid training altogether and instead use a pre-\\ntrained model with a retrieval mechanism. This is often called Retrieval-Augmented Generation (RAG). In\\nthis setup, you  provide the model with relevant information at query time by retrieving it from a\\nknowledge base of your documents. The model then generates an answer using that information as\\ncontext.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal\\ndata (the same kind of data we discussed: your bio, Q&A pairs, etc.). When a user asks a question on your\\nportfolio site, the system will pull out the parts of that data that are most relevant to the question and feed\\nthose, along with the question, into the prompt for the LLM. The LLM (which could be a large pre-trained\\nmodel like GPT-3.5, GPT-4, or an open-source model you host) then answers based only on that provided\\ncontext.\\n8\\n• \\n• \\n9\\n• \\n9\\n4'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Example architecture of a retrieval-augmented Q&A system. A user’s query is first used to fetch relevant knowledge\\n(documents or data) from a store (vector database or search index), and that knowledge is combined with the\\nquery as input to the LLM. The LLM then produces an answer grounded in the provided data\\n.\\nThis approach has several advantages for your use case: - No model training needed: The heavy lifting\\nhas been done by the base model. You don’t adjust its weights; you just give it information. This avoids the\\nexpense and complexity of fine-tuning. As one source explains, you “keep the model as it is and simply plug it\\ninto your knowledge sources… the model retrieves the latest info at runtime, answers in context, and stays\\naccurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the\\ndocuments or knowledge base. The next question asked will then retrieve from the new data immediately.\\nNo need to retrain a model or deploy new weights. This makes the system much easier to keep up-to-date\\n(no “nightmare to maintain every time your data changes” as with constant fine-tuning\\n). -  Smaller\\ndeployment footprint: Instead of hosting a custom model, you could use an API (like OpenAI’s) or host a\\nsmaller model just for inference. The retrieval step might need a vector database, but there are lightweight\\noptions (even in-memory). Many real-world chatbot systems use this pattern because it scales well – you\\ncan  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and\\ngrounding: Because the answers are drawn from provided text, you reduce the chance of the model\\nhallucinating incorrect facts about you. Essentially, the model is forced to base its answer on the snippets of\\ntext you supply. (Of course, the model could still do a poor job or hallucinate connections between facts, but\\nif prompted to only use the given info, it tends to stick to it). A Reddit user summarizing best practices\\nnoted that “Prompt design matters as much as retrieval. Instruct the model to stick to provided excerpts… This\\nreduces hallucinations and builds trust with users.”\\n. You can even have the model cite the sources (in a\\nmore advanced implementation), which is common in RAG setups for enterprise knowledge bases. - Speed\\nand cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper\\nand faster. You’re not paying the cost of training. At query time, vector search is usually fast (milliseconds to\\na few tens of ms), and then you call the model to generate an answer. If using an API like OpenAI, you pay\\nper call (plus maybe a small cost for vector DB if using a cloud one). If using an open source model, the\\ncompute to run it is similar to if it were fine-tuned. Many have concluded that “RAG has become the go-to\\nchoice for many use cases: it’s faster, cheaper, and way more practical for real-world teams”\\n.\\nHow it can be implemented: The general flow is: 1. Indexing your personal data: Take your documents\\nor Q&A pairs about yourself and break them into chunks (e.g. paragraphs or individual Q&A entries). For\\neach chunk, generate an  embedding vector (a numeric representation of the text meaning) using an\\n10\\n8\\n8\\n11\\n12\\n5'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='embedding model\\n. Store these vectors in a vector database (or use a simple in-memory vector index if\\nthe data is small) along with the chunk text. There are many tools for this; libraries like  LangChain or\\nLlamaIndex can automate a lot of it, and vector DBs like Pinecone, Weaviate, or open-source ones like FAISS\\nor Qdrant can store the data\\n. A Reddit comment succinctly described this: “chunk your docs, embed them,\\nand store in a vector database. At query time, retrieve the most relevant chunks and pass them into the\\nLLM...”\\n. 2. Retrieval on query: When a question is asked (“How many years of experience does [Your\\nName] have in AI?” for example), the system creates an embedding for the query and searches for similar\\nvectors in your vector DB. It might return, say, the top 3 chunks that are most relevant – maybe one of those\\nchunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct\\nprompt with context: The retrieved text chunks are then combined with the user’s question to form the\\nprompt for the LLM. For instance, the prompt might be something like:  “Context: [excerpt from your bio\\nsaying you have 5 years in AI]. Q: How many years of experience do I have in AI? A:” – and the model will\\nhopefully respond: “You have 5 years of experience in AI.” 4.  Model answer: The LLM (which could be\\nrunning via an API or a local model) generates an answer using the context. If all goes well, the answer will\\nbe correct, because the needed info was in the provided context. We can also instruct the model with a\\nsystem/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component\\n(in that diagram, “Azure AI Search” plays the role of vector DB/search engine) which returns knowledge, and\\nthat knowledge + query go into the LLM to get a final answer\\n. Notably, this approach requires no fine-\\ntuning of the LLM’s weights – the model remains as-is (pretrained on general data), and we just augment\\nits input with relevant data at runtime\\n.\\nTools and options: Since you’re an AI engineer, you might enjoy building the pieces yourself, or you can use\\nexisting frameworks: - LangChain and LlamaIndex (formerly GPT Index) are high-level libraries that let you set\\nup a QA chain over your documents very quickly. They handle splitting text, embedding (you can choose\\nmodels like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow\\n. For example, LangChain has a RetrievalQA  chain that does exactly this once you provide it a vector\\nstore and an LLM. These libraries also help with prompt management. - Vector database choices: If you\\nprefer not to rely on an external cloud service, you can use an open-source solution. FAISS (by Facebook)\\ncan run in-memory or on disk and is often used for small to medium cases. For larger scale or convenience,\\nservices like Pinecone or Weaviate can host it (though for a personal portfolio, that’s probably overkill).\\nThere are lightweight options like an SQLite + embeddings or even just computing cosine similarity on the\\nfly for small data. Given your data will be relatively small (maybe a few pages of text in total), even a simple\\napproach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to\\nfetch real-time info from the internet. All knowledge is static in your provided data. That’s perfectly in line\\nwith RAG – the knowledge base is whatever you load into the vector store. It won’t go out and search\\nbeyond that. If a question is asked that isn’t answerable from your data, the ideal behavior is to say “I don’t\\n13\\n14\\n15\\n10\\n16\\n17\\n18\\n6'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='know” or some graceful fallback. You can program the prompt to encourage that (e.g., “If the answer is not\\nin the provided context, say you don’t know.”).\\nPotential downsides of retrieval approach: - It is a bit more moving parts: you have to set up an\\nembedding process and store. However, for a one-person project, this is fairly straightforward and many\\ntutorials exist. It’s arguably less work than doing a fine-tune from scratch. - At query time, the model’s\\nresponse is limited by what it sees in the context. If your context window (the prompt length) of the model\\nis, say, 4,000 tokens, and your entire personal knowledge base is larger than that, the retrieval step must be\\neffective at picking the right pieces of info. If it misses something relevant, the answer might be incomplete.\\nBut since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is\\nmanageable. - If not properly instructed, the model could ignore the context and hallucinate. But in practice,\\nif you supply a relevant context chunk, models like GPT-3.5 or LLaMA-2-chat will use it when answering. -\\nOne consideration: if you want the Q&A to have some memory or multi-turn conversation about you, you’d\\nhave to include previous Q&A in context as well. But since it’s mostly fact-based about you, each question\\ncan probably be handled independently (stateless Q&A).\\nGiven the above, the retrieval-based approach is quite appealing for simplicity and robustness. It’s generally\\nthe  preferred method in industry for Q&A bots on custom data because of the maintenance and\\naccuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed\\nthe model to deeply internalize a style or do complex transformations, or if retrieval latency was a big\\nissue).\\nComparison and Recommendation\\nBoth approaches can ultimately achieve your goal: a chatbot that answers questions about you, without\\nhooking into live external sources. The best choice depends on your priorities (learning experience vs.\\nsimplicity, one-time effort vs. ongoing flexibility).\\nTraining a Personal LLM might be your first inclination as an AI engineer because it’s an interesting\\nproject. It will let you experiment with the latest fine-tuning techniques (LoRA, QLoRA, etc.) and truly “own”\\nthe model that results. If you go this route, try to leverage existing models and do a relatively lightweight\\nfine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat\\n(for instance, LLaMA-2 13B Chat has good performance). Use LoRA to fine-tune it on a curated set of Q&A\\nabout you. Monitor for overfitting – since your dataset might be small, you might only do one or two epochs\\nover it\\n. It’s even possible that just a few hundred training steps could suffice if using a high-quality base\\nmodel. - Ensure your fine-tuning dataset is high quality and diverse within the realm of “about you.” If\\nthere are specific phrasings or tricky factual questions (like dates, spellings of names, etc.), include those.\\nThe model will memorize those facts. Be cautious: the model might generalize in unintended ways (you\\nwouldn’t want it to start answering beyond your data and being wrong). - After training, you’ll need to\\ndeploy the model. For a portfolio site, you might run the model on a server that the site can send requests\\nto. Running a 7B model with int8 quantization on CPU is possible but may be slow (several seconds per\\nanswer). Running on a GPU (even a cheap one) or using a model served via an API (like HuggingFace\\nInference Endpoint or similar) could be better for snappy responses.\\nUsing Retrieval (RAG) is, in contrast, more of a software engineering solution than a model-training\\nsolution. My recommendation is to strongly consider this approach, because it aligns well with having\\nlimited data and budget but needing accurate results. Here’s how you might implement it step by step: 1.\\n19\\n7'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Begin with an open-source model or an API. For example, try OpenAI’s GPT-3.5 Turbo on some manually\\ncrafted prompts using your data (even before setting up any vector DB) to see how it performs when given\\ncontext. This costs very little and gives a baseline. You can later swap to an open model if you want to self-\\nhost. 2. Use a library like LangChain to index your personal info. You could literally have a Python script\\nwhere you input a bunch of strings (your bio, some Q&As) and it uses an embedding model (say OpenAI’s\\nembeddings,  or  SentenceTransformers  locally)  to  create  vectors  and  store  them  in  something  like\\nChromaDB (which is an easy local vector store that LangChain supports). 3. Hook up a simple API route (if\\nusing  Next.js,  for  example)  that  takes  a  user’s  question,  does  the  retrieval,  and  returns  the  answer.\\nLangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only\\na few dozen lines of code once the environment is set up. 4. Test and refine: see if the answers are accurate.\\nIf the bot ever says “I don’t know that” for something you did provide in the data, you might need to ensure\\nthe embedding is picking it up or add more context. You can also tweak prompts (e.g., add a system\\nmessage: “You are a chatbot that answers questions only using the provided context about [Your Name]. If\\nyou cannot find the answer in the context, say you do not know.”).\\nCost-wise, retrieval approach can be very cheap, especially if you use local models. If using an API, you pay\\nper call but the usage for a personal portfolio (with presumably low traffic) is negligible. Fine-tuning has an\\nupfront cost (compute for training) but then usage of the model is just the cost of running a server.\\nIt’s worth noting one hybrid idea: you could fine-tune a smaller model and use retrieval with it. For example,\\nfine-tune a 7B model on a small dataset just to give it some familiarity with your style or key facts, but still\\nuse a vector store to feed it more detailed or less frequently used facts. This is probably unnecessary here –\\nit’s more complex and the pure retrieval method should suffice – but it’s an option if you find the fine-tuned\\nmodel alone isn’t reliable for less common questions.\\nIn conclusion, if we “look at both options” as you requested: - Option 1 (Personal LLM via fine-tuning):\\nFeasible with LoRA on a 7B/13B model; provides a self-contained model; requires more upfront work and\\ndoesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use\\ncase if done right, but keep expectations reasonable in terms of model accuracy. - Option 2 (RAG with no\\ntraining): More straightforward and likely to give accurate, up-to-date answers; leverages powerful existing\\nmodels; minimal cost to maintain; easier to scale or improve incrementally. For a “simple ask questions\\nabout me and get answers” goal, this is arguably the best way to complete the task with least friction. The\\nfact that industry solutions favor RAG for Q&A on custom data\\n is a strong indicator.\\nGiven  the  constraints  (limited  data  and  budget)  and  the  desire  for  state-of-the-art  techniques,  my\\nsuggestion would be: Why not do both, sequentially? You could implement the RAG approach first to have\\na working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same\\ndata to see how it compares. This way, your portfolio has a reliable Q&A function (backed by retrieval and a\\nrobust model), and you still get to play with training a model as a side project (which you can swap in if it\\nbecomes good enough, or at least blog about the process as an AI engineer!). This combined approach\\nleverages the strength of RAG for now, while keeping the door open for a custom LLM when it’s viable.\\nTo directly answer your question:  Yes, you can use Vercel and host on your own domain (just add a\\ncustom domain in Vercel settings and point your DNS records accordingly)\\n. For the chatbot, using\\nNext.js would streamline the integration of your AI backend and frontend. Start gathering your personal\\ndata in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8'), Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='use LoRA to keep it cheap and see how well it performs. By comparing both, you’ll see which one meets\\nyour needs in practice. Many have found RAG to be “faster, cheaper, and way more practical” for Q&A\\nbots\\n, but with your expertise you might get a surprisingly good result with a tailored mini-LLM as well.\\nGood luck with building your personal AI-powered portfolio site!\\nSources:\\nVercel Custom Domain Documentation\\nContentful Blog – Advantages of Next.js (built-in routing and backend)\\nSebastian Raschka – LoRA Fine-tuning 7B models on single GPU\\nReddit (LocalLLaMA) – Advice on knowledge-base chatbots (RAG workflow)\\nStack AI Blog – Fine-tuning vs RAG for custom chatbots\\nSwirlAI Newsletter – Challenges with fine-tuning vs retrieval\\nMicrosoft Learn (Azure AI) – RAG architecture and description\\nNext.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\\nWhat is the best way to create a knowledge-base specific LLM chatbot ? : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/\\n12\\n• \\n3\\n4\\n• \\n2\\n• \\n5\\n• \\n15\\n11\\n• \\n8\\n12\\n• \\n9\\n13\\n• \\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n19\\n8\\n12\\n9\\n13\\n10\\n16\\n17\\n11\\n14\\n15\\n18\\n9')]\n",
      "Loaded 9 pages\n",
      "Processing file ..\\data\\pdfs\\Building an Advanced RAG Portfolio.pdf\n",
      "Loaded 18 documents from ..\\data\\pdfs\\Building an Advanced RAG Portfolio.pdf using PyMuPDFLoader\n",
      "[Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Architecting the Agentic Portfolio: A \\nComprehensive Technical Report on \\nRAG, Knowledge Graphs, and \\nGenerative 3D Interfaces \\n \\n \\n1. Introduction: The Paradigm Shift to Agentic Personal \\nBranding \\n \\nThe digital portfolio has long served as the static representation of a developer\\'s capability—a \\npassive repository of resumes, project links, and code snippets waiting to be discovered. \\nHowever, the rapid ascent of Large Language Models (LLMs) and Retrieval-Augmented \\nGeneration (RAG) is forcing a fundamental re-evaluation of how professional competence is \\ndemonstrated. We are witnessing a transition from the \"Portfolio as Document\" to the \\n\"Portfolio as Agent.\" In this emerging paradigm, the portfolio is no longer an artifact to be \\nread but an active, intelligent system capable of reasoning about its owner\\'s history, \\nanswering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent. \\nThis transformation is not merely aesthetic; it is a direct response to the increasing complexity \\nof modern software engineering. A static list of skills (e.g., \"React, Python, AWS\") fails to \\nconvey the depth of application—how a candidate handled race conditions in a distributed \\nsystem or optimized a render loop in a graphics application. An Agentic Portfolio, powered by \\nadvanced RAG, bridges this gap by allowing recruiters and technical managers to interrogate \\nthe data: \"How did this candidate optimize database queries in their 2023 e-commerce \\nproject?\" \\nThis report provides an exhaustive architectural blueprint for constructing such a system. It \\nmoves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative \\n3D User Interfaces (GenUI). It rigorously examines the data engineering required to prevent \\nhallucinations, the mathematical principles behind advanced retrieval strategies, and the \\nsoftware engineering practices—specifically unit testing and evaluation—necessary to deploy'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='a reliable AI agent. \\n \\n2. Foundational Data Engineering: The bedrock of RAG \\n \\nThe axiom \"Garbage In, Garbage Out\" is the governing law of Retrieval-Augmented \\nGeneration. A RAG system’s intelligence is deterministically limited by the structure and \\nsemantic clarity of its underlying data. While many Minimum Viable Products (MVPs) rely on \\nsimple text extraction from PDFs, this approach is fundamentally flawed for high-stakes \\napplications like professional portfolios, where precision is paramount. \\n \\n2.1 The Fallacy of Unstructured Ingestion \\n \\nStandard ingestion pipelines often utilize libraries such as PyMuPDF or pypdf to strip text from \\nPDF resumes.1 While computationally efficient, these tools discard the semantic signals \\nembedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a \\nhierarchical relationship to the header above them. Linear text extraction flattens this \\nhierarchy. A column-based layout, for instance, might be read line-by-line across columns, \\nmerging a \"Skills\" list with a \"Work History\" description, creating nonsensical chunks such as \\n\"Python 2018-Present Manager at Company X.\" \\nThis loss of structure leads to \"Context Dissociation.\" When an LLM retrieves a chunk \\ncontaining \"reduced latency by 50%,\" but the header identifying the specific project or \\ncompany was stripped during ingestion, the model cannot accurately attribute the \\nachievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the \\nuse of Vision-Language Models (VLMs) like GPT-4o or Claude 3.5 Sonnet for document \\ningestion.3 Unlike OCR (Optical Character Recognition), which identifies characters, VLMs \\nunderstand layout semantics.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='The ingestion pipeline for the Agentic Portfolio functions as follows: \\n1.\\u200b Rasterization: The PDF resume is converted into high-resolution images (e.g., 300 DPI \\nPNGs). \\n2.\\u200b Vision Prompting: The VLM is prompted to transcribe the image into a strictly defined \\nJSON schema. The prompt must explicitly instruct the model to respect visual hierarchy: \\n\"Identify the date ranges associated with each role and nest them within the \\'experience\\' \\nobject. Extract skills listed in sidebars and categorize them.\" \\n3.\\u200b Schema Validation: The output is validated against a Zod or Pydantic schema to ensure \\ntype safety before entering the database. \\nThis approach, while incurring a higher initial computational cost (token usage for image \\nprocessing), ensures a \"Golden Source\" of truth. It allows for the normalization of \\nentities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5 \\n \\n2.3 Defining the Knowledge Schema \\n \\nTo facilitate advanced retrieval strategies like filtering and graph traversal, the unstructured \\nbio data must be mapped to a rigorous schema. We recommend adopting and extending the \\nJSON Resume standard, augmenting it with vector-specific fields. \\nTable 1: Proposed Data Schema for RAG Optimization \\nEntity Field \\nData Type \\nDescription & RAG Utility \\nbasics.summary \\nString \\nA high-level bio used for \\n\"Who are you?\" queries. \\nEmbedded as a single \\nchunk. \\nwork.highlights \\nArray<String> \\nGranular achievements \\n(e.g., \"Optimized SQL \\nqueries\"). Each string is an \\nindividual \"Child Chunk.\" \\nwork.tech_stack \\nArray<String> \\nA list of technologies used \\nin that specific role. Used \\nfor metadata filtering (e.g.,'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='filter: { tech_stack: { $in: \\n[\"Python\"] } }). \\nwork.context_blob \\nString \\nA synthetic paragraph \\ngenerated by an LLM that \\ncombines the role, \\ncompany, and dates into a \\nnarrative format. This \\nserves as the \"Parent \\nChunk\" for retrieval. \\nskills.keywords \\nArray<String> \\nSynonyms and related \\nterms (e.g., Skill: \"AWS\", \\nKeywords:). Enhances \\nrecall for varied user \\nqueries. \\nprojects.emb_text \\nString \\nA \"dense\" description \\nspecifically engineered for \\nembedding, removing stop \\nwords and focusing on \\nsemantic keywords. \\nThis structured approach allows for Metadata Filtering.7 Instead of relying solely on cosine \\nsimilarity, the system can execute a pre-filter step. If a user asks, \"What was your experience \\nat Google?\", the retriever creates a filter company == \"Google\" before performing the vector \\nsearch, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent \\n \\nChunking—the process of breaking text into manageable pieces for embedding—is the single \\nmost critical hyperparameter in a RAG pipeline. For a technical portfolio, where the \\nconnection between a specific tool (e.g., Redis) and a specific outcome (e.g., caching layer) is \\nvital, standard fixed-size chunking (e.g., every 500 characters) is disastrous. It risks severing'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='the subject from the predicate.9 \\n \\n3.1 Semantic Chunking \\n \\nSemantic chunking moves beyond arbitrary character counts to respect the \"semantic \\nboundaries\" of the text. The algorithm operates by embedding sequential sentences and \\ncalculating the cosine similarity between them. \\nThe mechanism is as follows: \\n1.\\u200b Sentence Splitting: The text is broken into individual sentences using a natural language \\ntokenizer (e.g., NLTK or SpaCy). \\n2.\\u200b Sequential Embedding: Each sentence is embedded using a lightweight model (e.g., \\nall-MiniLM-L6-v2). \\n3.\\u200b Coherence Calculation: The algorithm calculates the similarity score $S$ between \\nSentence $N$ and Sentence $N+1$. \\n4.\\u200b Breakpoint Detection: If $S$ drops below a defined threshold (e.g., 0.7), it indicates a \\n\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point. \\nThis ensures that each retrieved chunk represents a self-contained thought or topic, \\nsignificantly improving the \"Faithfulness\" metric of the generated answers.11 \\n \\n3.2 The Parent-Document Retrieval Pattern \\n \\nTechnical resumes often contain bullet points that are semantically dense but contextually \\nsparse. A bullet point might read: \"Migrated legacy codebase to TypeScript.\" \\nIf this single sentence is retrieved in isolation, the LLM lacks crucial context: Which company \\nwas this for? When did it happen? What was the impact? Embedding the entire project \\ndescription, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9 \\n●\\u200b Indexing: The system splits the document into small \"Child Chunks\" (individual bullet \\npoints) and embeds them. These are optimized for high-precision matching. \\n●\\u200b Storage: The full \"Parent Document\" (the entire project narrative or work history block) is'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='stored in a separate key-value store (e.g., Redis or the Supabase text column), referenced \\nby a parent_id. \\n●\\u200b Retrieval: The user query matches the dense Child Chunk. The system then uses the \\nparent_id to fetch the full Parent Document. \\n●\\u200b Generation: The LLM receives the full Parent Document as context. \\nThis architectural pattern decouples the retrieval unit (optimized for search) from the \\ngeneration unit (optimized for context), providing the best of both worlds. \\n \\n3.3 Post-Chunking and Late Interaction \\n \\nEmerging research suggests a shift toward \"Post-Chunking\" or Late Interaction models (like \\nColBERT).10 In this paradigm, the entire document is embedded at the token level, and \\ninteraction with the query happens before reducing to a single vector score. While highly \\neffective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic \\nChunking combined with Parent-Document retrieval offers the optimal trade-off between \\nperformance and complexity. \\n \\n4. The Retrieval Engine: Moving Beyond Cosine \\nSimilarity \\n \\nA \"naive\" RAG system relies exclusively on dense vector search (embeddings). While powerful \\nfor semantic conceptual matching, dense retrieval struggles with precise keyword matching, \\nparticularly with technical acronyms (e.g., \"C#\" vs. \"C++\", \"AWS\" vs. \"GCP\"). To achieve \\nindustry standards, the portfolio must implement a Hybrid Search architecture. \\n \\n4.1 Hybrid Search Architecture \\n \\nHybrid search combines the strengths of two distinct retrieval algorithms: \\n1.\\u200b Dense Retrieval (Vector Search): Utilizes embeddings (e.g., OpenAI \\ntext-embedding-3-small) to understand semantic intent. It excels at queries like \"Tell me'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='about your leadership style\" where exact keyword matches are less important than \\nconceptual alignment. \\n2.\\u200b Sparse Retrieval (Keyword Search): Utilizes algorithms like BM25 (Best Matching 25) or \\nSPLADE to match exact tokens. It excels at technical queries like \"Do you know Redux \\nToolkit?\" where the presence of the specific term is non-negotiable.7 \\n \\n4.1.1 Reciprocal Rank Fusion (RRF) \\n \\nTo combine the results from these two disparate algorithms, we employ Reciprocal Rank \\nFusion (RRF). RRF does not rely on the absolute scores (which are incomparable between \\nCosine Similarity and BM25); instead, it relies on the rank of the document in each list. \\nThe RRF score for a document $d$ is calculated as: \\n \\n \\n$$RRFscore(d) = \\\\sum_{r \\\\in R} \\\\frac{1}{k + r(d)}$$ \\n \\nWhere: \\n●\\u200b $R$ is the set of rank lists (one from Dense, one from Sparse). \\n●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$. \\nThis mathematical fusion ensures that a document appearing in the top 5 results of both \\nsearch methods is prioritized over a document that is #1 in vector search but absent in \\nkeyword search.13 \\n \\n4.2 Vector Database Selection: Supabase vs. Pinecone \\n \\nSelecting the right vector store is a critical infrastructure decision. The market is divided \\nbetween specialized vector databases (Pinecone, Weaviate) and integrated vector extensions \\nfor traditional databases (Supabase/Postgres). \\nTable 2: Vector Database Comparison for Portfolio Use Case \\n \\nFeature \\nSupabase \\nPinecone \\nAnalysis for'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='(pgvector) \\nPortfolio \\nArchitecture \\nIntegrated \\n(Postgres \\nExtension) \\nSpecialized \\nManaged Service \\nSupabase allows \\nstoring relational \\ndata (Projects, \\nUsers) alongside \\nvectors in a single \\nDB, simplifying the \\nstack.15 \\nFree Tier \\n500MB DB Size \\n(Millions of vectors) \\nLimited to 1 index, \\nrestricted \\nthroughput \\nSupabase\\'s 500MB \\nlimit is sufficient for \\nvast amounts of \\ntext data (resumes \\nare KBs). \\nPinecone\\'s free tier \\nis restrictive for \\nmultiple projects.17 \\nHybrid Search \\nNative support \\n(tsvector + \\npgvector) \\nSupported via \\n\"Sparse-Dense\" \\nvectors \\nSupabase allows \\nfusing SQL WHERE \\nclauses with vector \\nsearch naturally. \\nPinecone requires \\nmanaging \\nmetadata \\nseparately.19 \\nLatency \\n~150-200ms (Cold \\nboot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20 \\nConclusion: For a personal portfolio, Supabase is the superior architectural choice. It \\neliminates the \"data synchronization\" problem—the risk that the metadata in the vector store \\ndrifts from the primary application database.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='4.3 Reranking: The Precision Layer \\n \\nEven with Hybrid Search, the \"Top K\" retrieved documents may include irrelevant noise. To \\nfilter this, we introduce a Reranking step using a Cross-Encoder model (e.g., \\nbge-reranker-v2-m3 or Cohere Rerank). \\nUnlike Bi-Encoders (embedding models) which process the query and document \\nindependently, Cross-Encoders process them simultaneously, outputting a single scalar score \\nindicating relevance. \\n●\\u200b Workflow: Retrieve top 25 documents via Hybrid Search -> Pass to Reranker -> Keep Top \\n5. \\n●\\u200b Impact: This dramatically increases the density of relevant information in the LLM\\'s \\ncontext window, reducing the likelihood of \"Lost in the Middle\" phenomenon where the \\nLLM ignores context buried in the center of the prompt.7 \\n \\n5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to \\ndatabase design, API development, or server management? Direct retrieval on such queries \\noften yields poor results. To address this, we implement Query Transformation strategies. \\n \\n5.1 Hypothetical Document Embeddings (HyDE) \\n \\nHyDE is a technique that leverages the LLM\\'s internal knowledge to bridge the gap between a \\nshort query and a detailed document.1 \\nMechanism: \\n1.\\u200b Prompt: The system prompts an LLM: \"Write a hypothetical resume entry for a senior \\nengineer answering the query: \\'backend experience\\'.\" \\n2.\\u200b Hallucination: The LLM generates a fictional but semantically rich paragraph: \\n\"Experienced in building RESTful APIs using Node.js and Express, managing PostgreSQL \\ndatabases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector. \\n4.\\u200b Retrieval: The vector search is performed using this hypothetical vector.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Because the hypothetical document shares the same semantic structure (technical \\nvocabulary, sentence structure) as the actual resume chunks, the retrieval accuracy is \\nsignificantly higher than using the raw query \"backend experience.\" \\n \\n5.2 Multi-Query Decomposition \\n \\nFor complex questions like \"Compare your experience with React and Vue,\" a single vector \\nsearch often fails to retrieve sufficient context for both topics. \\nMechanism: \\n1.\\u200b Decomposition: An LLM splits the query into sub-queries:. \\n2.\\u200b Parallel Retrieval: Both queries are executed independently. \\n3.\\u200b Fusion: The unique documents from both retrieval sets are combined (deduplicated). \\n4.\\u200b Generation: The combined context is passed to the LLM to answer the comparison \\nquestion. \\nThis ensures that the system doesn\\'t latch onto one concept (e.g., React) at the expense of \\nthe other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured \\nrelationships. A vector search for \"Frameworks\" might miss \"Next.js\" if the embedding model \\ndoesn\\'t explicitly map the \"is-a\" relationship. GraphRAG addresses this by modeling the \\nportfolio as a Knowledge Graph (KG).21 \\n \\n6.1 Graph Schema Design \\n \\nThe Knowledge Graph structures the portfolio into Entities and Relationships. \\n●\\u200b Nodes: Person, Skill, Project, Company, Role, Concept. \\n●\\u200b Edges: \\n○\\u200b (Project)-->(Skill) \\n○\\u200b (Skill)-->(Concept)'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='○\\u200b (Person)-->(Company) \\n○\\u200b (Project)-->(Outcome) \\nThis structure enables Multi-hop Reasoning. A user might ask: \"Which projects used a \\ncloud-native database?\" \\n●\\u200b Vector Search: Finds \"Cloud-native database\" $\\\\rightarrow$ matches DynamoDB node. \\n●\\u200b Graph Traversal: (DynamoDB)<--(Project A) and (DynamoDB)<--(Project B). \\n●\\u200b Result: The system identifies Project A and Project B, even if the project descriptions \\nnever explicitly used the phrase \"cloud-native database.\" \\n \\n6.2 Implementation: NetworkX vs. Neo4j \\n \\nFor enterprise applications, graph databases like Neo4j are standard. However, for a personal \\nportfolio with a limited dataset (< 1000 nodes), running a dedicated Neo4j instance is often \\noverkill in terms of cost and maintenance. \\nLightweight Alternative: NetworkX \\nWe recommend using NetworkX (a Python library) to build and query the graph in-memory or \\nserialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the \\nnode-edge list. This is saved as graph.json. \\n2.\\u200b Run Time: The Next.js API route loads graph.json. Simple traversal algorithms (e.g., \\nfinding all neighbors of a matched node) are executed in JavaScript/TypeScript. \\n3.\\u200b Integration: The graph results are textually formatted (e.g., \"Related Skills: X, Y, Z\") and \\nappended to the context window alongside vector results. \\nThis \"Client-Side Graph\" approach delivers 80% of the value of Neo4j with 0% of the \\ninfrastructure cost.25 \\n \\n7. Generative UI: The Interactive 3D Interface \\n \\nThe defining feature of the Agentic Portfolio is Generative UI—the ability of the AI to not just \\ntalk, but to show and control the interface. We utilize the Vercel AI SDK and React Server \\nComponents (RSC) to stream UI elements.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='7.1 Generative UI Architecture \\n \\nIn a standard chatbot, the output is Markdown. In a Generative UI system, the LLM can output \\nReact components. \\n●\\u200b Mechanism: The Vercel AI SDK\\'s streamUI function allows the server to stream \\nrenderable React components to the client.26 \\n●\\u200b Use Case: \\n○\\u200b User: \"Show me your design skills.\" \\n○\\u200b LLM Response: Instead of listing skills, it streams a <SkillCloud category=\"design\" /> \\ncomponent. The client renders this interactive 3D cloud immediately within the chat \\nstream. \\n○\\u200b Fallback: If the client doesn\\'t support the component, it falls back to a text \\ndescription. \\n \\n7.2 Controlling React Three Fiber via Tool Calling \\n \\nThe portfolio features a 3D scene (built with React Three Fiber) that acts as an immersive \\nbackground. The Chat Agent acts as the controller for this scene. \\nState Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28 \\nImplementation Steps: \\n1.\\u200b Store Definition:\\u200b\\nTypeScript\\u200b\\ninterface State {\\u200b\\n  cameraTarget: Vector3;\\u200b\\n  setCameraTarget: (v: Vector3) => void;\\u200b\\n}\\u200b\\nconst useStore = create<State>((set) => ({... }));\\u200b\\n \\n2.\\u200b Tool Definition: The LLM is provided with a tool moveCamera.\\u200b\\nTypeScript\\u200b\\nconst tools = {\\u200b\\n  moveCamera: tool({\\u200b\\n    description: \\'Move the 3D camera to focus on a specific section.\\',\\u200b\\n    parameters: z.object({ section: z.enum([\\'home\\', \\'projects\\', \\'about\\']) }),\\u200b'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='execute: async ({ section }) => {\\u200b\\n       const coords = SECTION_COORDINATES[section];\\u200b\\n       useStore.getState().setCameraTarget(coords);\\u200b\\n       return `Moved camera to ${section}.`;\\u200b\\n    }\\u200b\\n  })\\u200b\\n};\\u200b\\n \\n3.\\u200b Scene Reaction: Inside the R3F Canvas, a component subscribes to the store.\\u200b\\nTypeScript\\u200b\\nfunction CameraController() {\\u200b\\n  const target = useStore((state) => state.cameraTarget);\\u200b\\n  useFrame((state, delta) => {\\u200b\\n    easing.damp3(state.camera.position, target, 0.5, delta); // Smooth interpolation\\u200b\\n  });\\u200b\\n  return null;\\u200b\\n}\\u200b\\n \\nThis architecture creates a seamless \"Magical\" experience where the user\\'s conversation \\nphysically drives the exploration of the 3D space.30 \\n \\n8. Coding Agents and Prompt Engineering \\n \\nDeveloping this complex system requires leveraging \"Coding Agents\" (like Cursor or GitHub \\nCopilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy \\n \\nTo ensure the coding agent generates code that adheres to the specific architectural \\nconstraints (Next.js 14, Tailwind, Supabase), we place a .cursorrules file in the project root.32 \\nSample Rule Configuration:'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy. \\n \\n9.1 Tier 1: Deterministic Unit Testing (pytest) \\n \\nWe mock the LLM and Vector Store to test the logic surrounding the AI.36 \\n●\\u200b Test Case: Verify that the moveCamera tool is correctly invoked when the input text \\ncontains directional intent. \\n●\\u200b Test Case: Verify that the PDF parser correctly extracts email addresses from a mock \\nresume file.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='●\\u200b Mocking: Use unittest.mock to simulate the Supabase client, ensuring that the retrieval \\nlogic handles empty result sets gracefully without crashing. \\n \\n9.2 Tier 2: LLM-based Evaluation (Ragas & DeepEval) \\n \\nTo evaluate the quality of the AI\\'s answers, we use the Ragas framework (Retrieval \\nAugmented Generation Assessment). Ragas uses an \"LLM-as-a-Judge\" (typically GPT-4) to \\nscore the system on specific metrics.38 \\nKey Metrics: \\n1.\\u200b Context Recall: Does the retrieved context contain the ground truth answer? (Measures \\nRetrieval quality). \\n2.\\u200b Faithfulness: Is the generated answer derived solely from the retrieved context, or did \\nthe model hallucinate? (Measures Generation quality). \\n3.\\u200b Answer Relevance: Does the answer directly address the user\\'s query? \\nCI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions. \\n●\\u200b Assertion: assert ragas_score[\\'context_recall\\'] > 0.8 \\n●\\u200b Outcome: If a code change (e.g., changing the chunking size) causes the score to drop \\nbelow 0.8, the PR is blocked. This prevents regressions in the agent\\'s intelligence.40 \\n \\n10. Conclusion \\nThe architecture described in this report represents the convergence of modern Full Stack \\nEngineering and Applied AI. By moving from naive text extraction to Vision-based Parsing, \\nfrom simple vector search to Hybrid Graph Retrieval, and from static text output to \\nGenerative 3D Interfaces, we transform the personal portfolio into a sophisticated Agentic \\nSystem. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate\\'s history. But more importantly, the existence of the \\nsystem itself serves as the ultimate proof of competence. It demonstrates mastery not just of \\ncode, but of data engineering, system architecture, AI orchestration, and user experience \\ndesign—the defining skills of the next generation of software engineers. The Agentic Portfolio \\nis not just a showcase of work; it is the work itself.'), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content=\"Works cited \\n1.\\u200b Optimizing RAG. RAG Demystified: A Hands-On Guide to… | by Skanda Vivek | \\nEMAlpha, accessed November 20, 2025, \\nhttps://medium.com/emalpha/optimizing-rag-bd65ebc5e51a \\n2.\\u200b Developing Retrieval Augmented Generation (RAG) based LLM Systems from \\nPDFs: An Experience Report - arXiv, accessed November 20, 2025, \\nhttps://arxiv.org/html/2410.15944v1 \\n3.\\u200b Using Azure OpenAI GPT-4o to extract structured JSON data from PDF \\ndocuments, accessed November 20, 2025, \\nhttps://learn.microsoft.com/en-us/samples/azure-samples/azure-openai-gpt-4-vi\\nsion-pdf-extraction-sample/using-azure-openai-gpt-4o-to-extract-structured-js\\non-data-from-pdf-documents/ \\n4.\\u200b How can I process a pdf using OpenAI's APIs (GPTs)? - Stack Overflow, accessed \\nNovember 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL \\nquerying, and agents with Amazon Bedrock, accessed November 20, 2025, \\nhttps://aws.amazon.com/blogs/machine-learning/boosting-rag-based-intelligent-\\ndocument-assistants-using-entity-extraction-sql-querying-and-agents-with-am\\nazon-bedrock/ \\n6.\\u200b Conversion of entire PDF into JSON Format - API - OpenAI Developer \\nCommunity, accessed November 20, 2025, \\nhttps://community.openai.com/t/conversion-of-entire-pdf-into-json-format/1066\\n551 \\n7.\\u200b 9 advanced RAG techniques to know & how to implement them - Meilisearch, \\naccessed November 20, 2025, https://www.meilisearch.com/blog/rag-techniques \\n8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025, \\nhttps://www.datasciencecentral.com/best-practices-for-structuring-large-datas\\nets-in-retrieval-augmented-generation-rag/ \\n9.\\u200b Top 13 Advanced RAG Techniques for Your Next Project - Analytics Vidhya, \\naccessed November 20, 2025, \\nhttps://www.analyticsvidhya.com/blog/2025/04/advanced-rag-techniques/ \\n10.\\u200bBeyond Basic Chunking: The Critical Timing Decision in RAG Systems That \\nEveryone Is Getting Wrong, accessed November 20, 2025, \\nhttps://skngrp.medium.com/beyond-basic-chunking-the-critical-timing-decision-\\nin-rag-systems-that-everyone-is-getting-wrong-19febb2ee062 \\n11.\\u200bComparing Chunking Strategies for RAG: From Naive Splits to Striding Windows | \\nby Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116 \\n12.\\u200bMastering Chunking Strategies for RAG: Best Practices & Code Examples - \\nDatabricks Community, accessed November 20, 2025,\"), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content=\"https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/ \\n19.\\u200bpgvector vs Pinecone: cost and performance - Supabase, accessed November \\n20, 2025, https://supabase.com/blog/pgvector-vs-pinecone \\n20.\\u200bSupabase vs Pinecone: I Migrated My Production AI System and Here's What \\nActually Matters - Dee, accessed November 20, 2025, \\nhttps://deeflect.medium.com/supabase-vs-pinecone-i-migrated-my-production-\\nai-system-and-heres-what-actually-matters-7b2f2ebd59ee \\n21.\\u200bGraphRAG for Devs: Graph-Code Demo Overview - Memgraph, accessed \\nNovember 20, 2025, \\nhttps://memgraph.com/blog/graphrag-for-devs-coding-assistant \\n22.\\u200bThe GraphRAG Manifesto: Adding Knowledge to GenAI - Neo4j, accessed \\nNovember 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber \\n29.\\u200bAccessing the Camera in React Three Fiber out of the canvas - Questions, \\naccessed November 20, 2025,\"), Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='https://discourse.threejs.org/t/accessing-the-camera-in-react-three-fiber-out-of\\n-the-canvas/39137 \\n30.\\u200bMove camera to face an object in React Three Fiber! - Questions, accessed \\nNovember 20, 2025, \\nhttps://discourse.threejs.org/t/move-camera-to-face-an-object-in-react-three-fi\\nber/81269 \\n31.\\u200bCamera Controls - Wawa Sensei, accessed November 20, 2025, \\nhttps://wawasensei.dev/courses/react-three-fiber/lessons/camera-controls \\n32.\\u200bPatrickJS/awesome-cursorrules: Configuration files that enhance Cursor AI editor \\nexperience with custom rules and behaviors - GitHub, accessed November 20, \\n2025, https://github.com/PatrickJS/awesome-cursorrules \\n33.\\u200bHow Cursor project rules can improve Next.js app development - LogRocket \\nBlog, accessed November 20, 2025, \\nhttps://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025, \\nhttps://cursorrules.org/article/nextjs15-react19-vercelai-tailwind-cursorrules-pro\\nmpt-file \\n35.\\u200bThe ultimate .cursorrules for TypeScript, React 19, Next.js 15, Vercel AI SDK, \\nShadcn UI, Radix UI, and Tailwind CSS : r/cursor - Reddit, accessed November 20, \\n2025, \\nhttps://www.reddit.com/r/cursor/comments/1gjd96h/the_ultimate_cursorrules_for\\n_typescript_react_19/ \\n36.\\u200bHow to Properly Mock LangChain LLM Execution in Unit Tests | Python - Medium, \\naccessed November 20, 2025, \\nhttps://medium.com/@matgmc/how-to-properly-mock-langchain-llm-execution-\\nin-unit-tests-python-76efe1b8707e \\n37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG')]\n",
      "Loaded 18 pages\n",
      "Processing file ..\\data\\pdfs\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf\n",
      "Loaded 1 documents from ..\\data\\pdfs\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf using PyMuPDFLoader\n",
      "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0}, page_content='Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025 \\nMaster of Science in Artificial Intelligence (Concentration: Machine Learning), GPA – 4.0                                                                                                                               Denton,TX  \\nCoursework : Machine Learning, Deep Learning, NLP, Information Retrieval, Big Data & Data Science, AI Software Development \\n.    \\nBirla Institute of Technology and Science                                                                                                                                                            June 2019 \\nBachelor of Science in Mechanical Engineering with Honours, GPA – 3.7                                                                                                                                                        Dubai, UAE  \\n \\nTECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present \\n• Cut BA story update effort by 80-90% by deploying a multi-agent pipeline (Fal.ai STT + GPT-4.1 in n8n) that updates Jira Cloud tickets end-to-end \\n(Description + Acceptance Criteria + Status/Assignee) via REST APIs. \\n• Built a Chrome extension (JS/HTML/CSS + REST) to capture voice → transcript → LLM output → structured Jira updates, improving time-to-update \\nfrom ~12 min → ~2 min. \\n• Authored an Acceptance Criteria (AC) Playbook and an NLP variance/coverage harness (Jaccard/F1) to benchmark LLM AC vs. senior BA standards; \\nimproved consistency and completeness of ACs. \\n• Tracked latency/cost per run; instrumented token usage and error categories to guide prompt and model selection decisions. \\nArtificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025 \\n• Assessed and provided actionable feedback on peer reviews of key AI research papers, enhancing students’ critical thinking and technical writing across \\nmultiple evaluation cycles. \\n• Led design and experimentation of ML solutions including GANs, transformers, and optimized CNNs for NLP and GenAI tasks. Applied CRISP-DM \\nmethodology and collaborated with academic teams on scalable prototype development for potential deployment.  \\n• Mentored students on applied AI projects, delivering personalized technical guidance that improved project success rates and supported milestone \\nachievement in coursework and capstone deliverables. \\n \\nML/AI Engineer – AI Document Workflow Automation                                                                                            Strawberry Labs, Dubai | Jan.2023 – Jan 2024 \\n• Deployed an OCR + extraction pipeline (Tesseract + CNN post-processing) processing docs in ~20s each, delivering 96% OCR accuracy and 93% \\nfield extraction precision; manual data-entry time −75% (5 min → <1 min). \\n• In controlled tests, achieved 4× faster completion (20s vs 90s) and −70% QA checks, with 96% data correctness vs 85% in manual entry. \\n• Implemented validation rules (format, range, cross-field), exception routing, and confidence scoring to reduce human review and raise straight-through \\nprocessing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS \\n  \\nConditional GAN for Ethnicity-Based Face Generation                                                                                                                             UNT, Denton, TX | Sep2024 – Dec 2024 \\n• Engineered a generative adversarial network (GAN) from the ground up to create facial images across six ethnicities (4,000 color images each), enabling data-\\ndriven insights into cross-cultural face generation.  \\n• Transitioned from an initial linear-layer design to convolutional layers for more robust feature extraction, improving model fidelity and output quality. \\n• Leveraged AWS to meet high computational (GPU) demands, streamlining large-scale model training and accelerating development cycles. \\nAutomated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024 \\n• Developed a pipeline that automatically identifies and classifies product components as “good” or “bad,” reducing manual inspection time and cost. \\n• Implemented advanced edge detection (Sobel, Laplacian of Gaussian) and custom convolution kernels to achieve a 100% detection rate (mIoU=0.8925). \\n• Leveraged both spatial (Probabilistic Hough Transform) and frequency domain (Fourier magnitude) methods to capture subtle texture patterns. \\n• Achieved 77% accuracy, 87% precision, and AUC of 0.85 on real-world data, with future improvements planned via data augmentation and refined \\nlighting/occlusion handling.')]\n",
      "Loaded 1 pages\n",
      "Total documents loaded: 28\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs in teh directory\n",
    "\n",
    "def process_pdfs(pdf_directory):\n",
    "\n",
    "    all_docs = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} in the directory {pdf_directory}\")\n",
    "\n",
    "    # return pdf_files\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file {pdf_file}\")\n",
    "\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} documents from {pdf_file} using PyMuPDFLoader\")\n",
    "            print(documents)\n",
    "\n",
    "            # Add Metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_path'] = 'pdf'\n",
    "\n",
    "            all_docs.extend(documents)\n",
    "            print(f'Loaded {len(documents)} pages')\n",
    "            # break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {pdf_file} with PyMuPDFLoader due to {e}, trying PyPDFLoader\")\n",
    "\n",
    "    print(f\"Total documents loaded: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "all_pdf_documents = process_pdfs(\"../data/pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edda76ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content=\"Start thinking about the content that might go into this personal knowledge base. For a portfolio Q&A\\nchatbot about you, the data could include things like:\\nA written bio or introduction: e.g. your background, education, skills, projects, interests.\\nResume or CV data: your work experience, accomplishments, maybe in a structured Q&A form\\n(“Where did I work in 2022?”, “What projects have I done in machine learning?”, etc.).\\nFrequently asked questions about you: This could be a list of questions and answers (FAQ) that you\\nanticipate someone might ask. For example: “What are your expertise areas?”, “What was your MSc\\nthesis about?”, “What hobbies do you have?” – along with the answers in your own words.\\nAny personal blog posts or writings: if relevant, these can provide context on your opinions or\\nknowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.\\nSince you want the chatbot to answer based only on what “it knows” (i.e. your provided data, with no live\\ninternet connection), we will either be training a model on this data or feeding this data into a retrieval\\nsystem for the model. In either case, you’ll need to gather and curate the information about yourself. This\\ncan be done incrementally: once we decide on the approach, you can compile the data into the needed\\nformat (documents, Q&A pairs, etc.). \\nIf we go with a fine-tuning approach: you may need to format the data as a training dataset (for example, a\\nlist of prompt-response pairs where the prompt is a question about you and the response is the correct\\nanswer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –\\nbut you do need enough coverage of facts about you so the model can learn them. Ensure the info is\\naccurate and expressed in the tone you want the answers to have.\\nIf we go with a retrieval-based approach: you might store the data as a set of documents or text passages.\\nThese could be chunks of a “About Me” document, or individual Q&A entries, etc. The quality of answers will\\ndepend on providing sufficient detail in these source texts. The nice thing is that you can start with a basic\\nset of documents (like a few paragraphs about you, plus a list of Q&A) and always update or expand it later\\nwithout retraining a model – we’ll discuss this more under the retrieval approach.\\nNow, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\\nName] is an AI engineer with a focus on NLP and LLMs, currently working on...”) along with many other\\n• \\n• \\n• \\n• \\n• \\n2\"),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Q&As covering your background, skills, projects, etc. The fine-tuning process will adjust the model’s weights\\nso that it “learns” these specific facts and can respond in a conversational style about them.\\nThanks to techniques like Low-Rank Adaptation (LoRA), it’s feasible to fine-tune moderately large models\\non a single GPU. In fact, LoRA has been shown to allow fine-tuning a 7-billion-parameter model (such as\\nLLaMA-2 7B) on a single GPU\\n. One report notes: “LoRA allows us to finetune 7B parameter LLMs on a single\\nGPU. In [one case], using QLoRA (quantized LoRA) with optimal settings required ~17.8 GB GPU memory and about\\n3 hours on an A100 GPU for 50k training examples”\\n. This means that if you have access to a GPU with\\n~24GB VRAM (or use a cloud service), you could potentially fine-tune a model on a custom dataset without\\nhuge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training\\nwould be faster (possibly an hour or two, depending on the model and hyperparameters).\\nState-of-the-art techniques you might consider for this include LoRA/QLoRA (to reduce memory and\\ncompute), and using an instruction-tuned base model. For example, starting with an instruction-following\\nmodel (like LLaMA-2-chat or Dolly, etc.) might yield better conversational answers after fine-tuning. You\\ncould also experiment with lightweight fine-tuning vs full fine-tuning. LoRA is nice because it keeps the\\noriginal model intact and just learns small adapter weights – this is efficient and you can revert to the base\\nmodel easily if needed.\\nBefore committing to fine-tuning, consider the pros and cons:\\nPros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).\\nIt can be run locally or on your server without external API calls, preserving privacy (important if\\nsome personal data is sensitive).\\nAs an AI engineer, you get the learning experience of doing a fine-tune with SOTA methods. You can\\nexperiment with parameters, try new fine-tuning optimizations, etc., which can be valuable\\nexperience.\\nThe inference might be slightly faster per query than a retrieval approach for small queries, since it’s\\njust the model response (no vector database lookup overhead) – though in practice the difference\\nmay be small.\\nCons of fine-tuning approach:\\nData requirements: Fine-tuning is effectively training, so if you have very little data about yourself,\\nthe model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-\\ntune with too few examples or only extremely narrow phrasing. You might need to be creative in\\ngenerating enough Q&A pairs (possibly augmenting with rephrased questions) so the model sees\\nsufficient variety. As one discussion put it: don’t try to fine-tune when you have too little data or just to\\ninject a few facts – that’s what prompt context or retrieval is for; “Don’t use a bulldozer to kill a fly.”\\n.\\nFine-tuning a model to learn new knowledge (like personal facts it didn’t know) is possible but\\nworks best if you provide a reasonably sized corpus of that knowledge\\n.\\nMaintenance and updates: If your personal information changes or you want to add new data (say\\nyou start a new job or complete new projects), you’d have to fine-tune a new version of the model or\\nat least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6\\n7\\n• \\n3'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='trained, the model’s knowledge is fixed. As one source notes, fine-tuning is “powerful on paper, but\\nexpensive, time-consuming, and a nightmare to maintain every time your data changes”\\n.\\nCompute and cost: While a small-scale fine-tune is much cheaper than training from scratch, it’s not\\nfree. You need suitable hardware. If using cloud GPUs, that could cost some money (though a single\\n1-3 hour run on an A100 or similar might be on the order of tens of dollars, which isn’t too bad). Still,\\nif you iterate multiple times, it adds up. Also hosting the final model (for inference) means you need\\na server (or at least something like a GPU or CPU instance) to run the model continuously for your\\nwebsite. A 7B model can run on CPU but might be slow; more likely you’d run on a GPU for snappier\\nresponses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger\\nbase model. Open-source models are improving rapidly, but something like a fine-tuned 7B or 13B\\nparameter model will be less fluent and sometimes less accurate than, say, GPT-4. It might also\\nhallucinate answers if asked something outside of what it was trained on (or even confuse facts if the\\nprompt is tricky). Notably, fine-tuning doesn’t inherently fix the hallucination problem of LLMs\\n.\\nThe model might still “make up” an answer if asked a question it doesn’t know and you haven’t built\\nin a mechanism to handle that (like a “I don’t know” response).\\nExpertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s\\ndefinitely doable (especially since you’re an AI engineer), but it’s more involved than the retrieval\\napproach. One write-up on private knowledge chatbots explicitly pointed out that to fine-tune an\\nopen-source model for a knowledge base, you need “specialized talent and a large amount of time to\\nsolve the fine tuning challenge internally”\\n. In other words, be prepared for some experimentation\\nand debugging.\\nIn summary, training a personal mini-LLM using fine-tuning is feasible and would be our first choice to\\nexplore if you’re keen on it. It gives you a self-contained model that can answer questions about you.\\nHowever, be mindful that this approach is best if you have a decent amount of personal data to teach the\\nmodel and if you’re ready to handle updates via retraining. Given the constraints (limited data and budget),\\nwe should compare this with Approach 2, which might be more efficient for the task.\\nApproach 2: Retrieval-Augmented Q&A (Using a Context File or\\nVector DB)\\nThe alternative (and increasingly common) approach is to avoid training altogether and instead use a pre-\\ntrained model with a retrieval mechanism. This is often called Retrieval-Augmented Generation (RAG). In\\nthis setup, you  provide the model with relevant information at query time by retrieving it from a\\nknowledge base of your documents. The model then generates an answer using that information as\\ncontext.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal\\ndata (the same kind of data we discussed: your bio, Q&A pairs, etc.). When a user asks a question on your\\nportfolio site, the system will pull out the parts of that data that are most relevant to the question and feed\\nthose, along with the question, into the prompt for the LLM. The LLM (which could be a large pre-trained\\nmodel like GPT-3.5, GPT-4, or an open-source model you host) then answers based only on that provided\\ncontext.\\n8\\n• \\n• \\n9\\n• \\n9\\n4'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Example architecture of a retrieval-augmented Q&A system. A user’s query is first used to fetch relevant knowledge\\n(documents or data) from a store (vector database or search index), and that knowledge is combined with the\\nquery as input to the LLM. The LLM then produces an answer grounded in the provided data\\n.\\nThis approach has several advantages for your use case: - No model training needed: The heavy lifting\\nhas been done by the base model. You don’t adjust its weights; you just give it information. This avoids the\\nexpense and complexity of fine-tuning. As one source explains, you “keep the model as it is and simply plug it\\ninto your knowledge sources… the model retrieves the latest info at runtime, answers in context, and stays\\naccurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the\\ndocuments or knowledge base. The next question asked will then retrieve from the new data immediately.\\nNo need to retrain a model or deploy new weights. This makes the system much easier to keep up-to-date\\n(no “nightmare to maintain every time your data changes” as with constant fine-tuning\\n). -  Smaller\\ndeployment footprint: Instead of hosting a custom model, you could use an API (like OpenAI’s) or host a\\nsmaller model just for inference. The retrieval step might need a vector database, but there are lightweight\\noptions (even in-memory). Many real-world chatbot systems use this pattern because it scales well – you\\ncan  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and\\ngrounding: Because the answers are drawn from provided text, you reduce the chance of the model\\nhallucinating incorrect facts about you. Essentially, the model is forced to base its answer on the snippets of\\ntext you supply. (Of course, the model could still do a poor job or hallucinate connections between facts, but\\nif prompted to only use the given info, it tends to stick to it). A Reddit user summarizing best practices\\nnoted that “Prompt design matters as much as retrieval. Instruct the model to stick to provided excerpts… This\\nreduces hallucinations and builds trust with users.”\\n. You can even have the model cite the sources (in a\\nmore advanced implementation), which is common in RAG setups for enterprise knowledge bases. - Speed\\nand cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper\\nand faster. You’re not paying the cost of training. At query time, vector search is usually fast (milliseconds to\\na few tens of ms), and then you call the model to generate an answer. If using an API like OpenAI, you pay\\nper call (plus maybe a small cost for vector DB if using a cloud one). If using an open source model, the\\ncompute to run it is similar to if it were fine-tuned. Many have concluded that “RAG has become the go-to\\nchoice for many use cases: it’s faster, cheaper, and way more practical for real-world teams”\\n.\\nHow it can be implemented: The general flow is: 1. Indexing your personal data: Take your documents\\nor Q&A pairs about yourself and break them into chunks (e.g. paragraphs or individual Q&A entries). For\\neach chunk, generate an  embedding vector (a numeric representation of the text meaning) using an\\n10\\n8\\n8\\n11\\n12\\n5'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='embedding model\\n. Store these vectors in a vector database (or use a simple in-memory vector index if\\nthe data is small) along with the chunk text. There are many tools for this; libraries like  LangChain or\\nLlamaIndex can automate a lot of it, and vector DBs like Pinecone, Weaviate, or open-source ones like FAISS\\nor Qdrant can store the data\\n. A Reddit comment succinctly described this: “chunk your docs, embed them,\\nand store in a vector database. At query time, retrieve the most relevant chunks and pass them into the\\nLLM...”\\n. 2. Retrieval on query: When a question is asked (“How many years of experience does [Your\\nName] have in AI?” for example), the system creates an embedding for the query and searches for similar\\nvectors in your vector DB. It might return, say, the top 3 chunks that are most relevant – maybe one of those\\nchunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct\\nprompt with context: The retrieved text chunks are then combined with the user’s question to form the\\nprompt for the LLM. For instance, the prompt might be something like:  “Context: [excerpt from your bio\\nsaying you have 5 years in AI]. Q: How many years of experience do I have in AI? A:” – and the model will\\nhopefully respond: “You have 5 years of experience in AI.” 4.  Model answer: The LLM (which could be\\nrunning via an API or a local model) generates an answer using the context. If all goes well, the answer will\\nbe correct, because the needed info was in the provided context. We can also instruct the model with a\\nsystem/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component\\n(in that diagram, “Azure AI Search” plays the role of vector DB/search engine) which returns knowledge, and\\nthat knowledge + query go into the LLM to get a final answer\\n. Notably, this approach requires no fine-\\ntuning of the LLM’s weights – the model remains as-is (pretrained on general data), and we just augment\\nits input with relevant data at runtime\\n.\\nTools and options: Since you’re an AI engineer, you might enjoy building the pieces yourself, or you can use\\nexisting frameworks: - LangChain and LlamaIndex (formerly GPT Index) are high-level libraries that let you set\\nup a QA chain over your documents very quickly. They handle splitting text, embedding (you can choose\\nmodels like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow\\n. For example, LangChain has a RetrievalQA  chain that does exactly this once you provide it a vector\\nstore and an LLM. These libraries also help with prompt management. - Vector database choices: If you\\nprefer not to rely on an external cloud service, you can use an open-source solution. FAISS (by Facebook)\\ncan run in-memory or on disk and is often used for small to medium cases. For larger scale or convenience,\\nservices like Pinecone or Weaviate can host it (though for a personal portfolio, that’s probably overkill).\\nThere are lightweight options like an SQLite + embeddings or even just computing cosine similarity on the\\nfly for small data. Given your data will be relatively small (maybe a few pages of text in total), even a simple\\napproach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to\\nfetch real-time info from the internet. All knowledge is static in your provided data. That’s perfectly in line\\nwith RAG – the knowledge base is whatever you load into the vector store. It won’t go out and search\\nbeyond that. If a question is asked that isn’t answerable from your data, the ideal behavior is to say “I don’t\\n13\\n14\\n15\\n10\\n16\\n17\\n18\\n6'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='know” or some graceful fallback. You can program the prompt to encourage that (e.g., “If the answer is not\\nin the provided context, say you don’t know.”).\\nPotential downsides of retrieval approach: - It is a bit more moving parts: you have to set up an\\nembedding process and store. However, for a one-person project, this is fairly straightforward and many\\ntutorials exist. It’s arguably less work than doing a fine-tune from scratch. - At query time, the model’s\\nresponse is limited by what it sees in the context. If your context window (the prompt length) of the model\\nis, say, 4,000 tokens, and your entire personal knowledge base is larger than that, the retrieval step must be\\neffective at picking the right pieces of info. If it misses something relevant, the answer might be incomplete.\\nBut since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is\\nmanageable. - If not properly instructed, the model could ignore the context and hallucinate. But in practice,\\nif you supply a relevant context chunk, models like GPT-3.5 or LLaMA-2-chat will use it when answering. -\\nOne consideration: if you want the Q&A to have some memory or multi-turn conversation about you, you’d\\nhave to include previous Q&A in context as well. But since it’s mostly fact-based about you, each question\\ncan probably be handled independently (stateless Q&A).\\nGiven the above, the retrieval-based approach is quite appealing for simplicity and robustness. It’s generally\\nthe  preferred method in industry for Q&A bots on custom data because of the maintenance and\\naccuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed\\nthe model to deeply internalize a style or do complex transformations, or if retrieval latency was a big\\nissue).\\nComparison and Recommendation\\nBoth approaches can ultimately achieve your goal: a chatbot that answers questions about you, without\\nhooking into live external sources. The best choice depends on your priorities (learning experience vs.\\nsimplicity, one-time effort vs. ongoing flexibility).\\nTraining a Personal LLM might be your first inclination as an AI engineer because it’s an interesting\\nproject. It will let you experiment with the latest fine-tuning techniques (LoRA, QLoRA, etc.) and truly “own”\\nthe model that results. If you go this route, try to leverage existing models and do a relatively lightweight\\nfine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat\\n(for instance, LLaMA-2 13B Chat has good performance). Use LoRA to fine-tune it on a curated set of Q&A\\nabout you. Monitor for overfitting – since your dataset might be small, you might only do one or two epochs\\nover it\\n. It’s even possible that just a few hundred training steps could suffice if using a high-quality base\\nmodel. - Ensure your fine-tuning dataset is high quality and diverse within the realm of “about you.” If\\nthere are specific phrasings or tricky factual questions (like dates, spellings of names, etc.), include those.\\nThe model will memorize those facts. Be cautious: the model might generalize in unintended ways (you\\nwouldn’t want it to start answering beyond your data and being wrong). - After training, you’ll need to\\ndeploy the model. For a portfolio site, you might run the model on a server that the site can send requests\\nto. Running a 7B model with int8 quantization on CPU is possible but may be slow (several seconds per\\nanswer). Running on a GPU (even a cheap one) or using a model served via an API (like HuggingFace\\nInference Endpoint or similar) could be better for snappy responses.\\nUsing Retrieval (RAG) is, in contrast, more of a software engineering solution than a model-training\\nsolution. My recommendation is to strongly consider this approach, because it aligns well with having\\nlimited data and budget but needing accurate results. Here’s how you might implement it step by step: 1.\\n19\\n7'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Begin with an open-source model or an API. For example, try OpenAI’s GPT-3.5 Turbo on some manually\\ncrafted prompts using your data (even before setting up any vector DB) to see how it performs when given\\ncontext. This costs very little and gives a baseline. You can later swap to an open model if you want to self-\\nhost. 2. Use a library like LangChain to index your personal info. You could literally have a Python script\\nwhere you input a bunch of strings (your bio, some Q&As) and it uses an embedding model (say OpenAI’s\\nembeddings,  or  SentenceTransformers  locally)  to  create  vectors  and  store  them  in  something  like\\nChromaDB (which is an easy local vector store that LangChain supports). 3. Hook up a simple API route (if\\nusing  Next.js,  for  example)  that  takes  a  user’s  question,  does  the  retrieval,  and  returns  the  answer.\\nLangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only\\na few dozen lines of code once the environment is set up. 4. Test and refine: see if the answers are accurate.\\nIf the bot ever says “I don’t know that” for something you did provide in the data, you might need to ensure\\nthe embedding is picking it up or add more context. You can also tweak prompts (e.g., add a system\\nmessage: “You are a chatbot that answers questions only using the provided context about [Your Name]. If\\nyou cannot find the answer in the context, say you do not know.”).\\nCost-wise, retrieval approach can be very cheap, especially if you use local models. If using an API, you pay\\nper call but the usage for a personal portfolio (with presumably low traffic) is negligible. Fine-tuning has an\\nupfront cost (compute for training) but then usage of the model is just the cost of running a server.\\nIt’s worth noting one hybrid idea: you could fine-tune a smaller model and use retrieval with it. For example,\\nfine-tune a 7B model on a small dataset just to give it some familiarity with your style or key facts, but still\\nuse a vector store to feed it more detailed or less frequently used facts. This is probably unnecessary here –\\nit’s more complex and the pure retrieval method should suffice – but it’s an option if you find the fine-tuned\\nmodel alone isn’t reliable for less common questions.\\nIn conclusion, if we “look at both options” as you requested: - Option 1 (Personal LLM via fine-tuning):\\nFeasible with LoRA on a 7B/13B model; provides a self-contained model; requires more upfront work and\\ndoesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use\\ncase if done right, but keep expectations reasonable in terms of model accuracy. - Option 2 (RAG with no\\ntraining): More straightforward and likely to give accurate, up-to-date answers; leverages powerful existing\\nmodels; minimal cost to maintain; easier to scale or improve incrementally. For a “simple ask questions\\nabout me and get answers” goal, this is arguably the best way to complete the task with least friction. The\\nfact that industry solutions favor RAG for Q&A on custom data\\n is a strong indicator.\\nGiven  the  constraints  (limited  data  and  budget)  and  the  desire  for  state-of-the-art  techniques,  my\\nsuggestion would be: Why not do both, sequentially? You could implement the RAG approach first to have\\na working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same\\ndata to see how it compares. This way, your portfolio has a reliable Q&A function (backed by retrieval and a\\nrobust model), and you still get to play with training a model as a side project (which you can swap in if it\\nbecomes good enough, or at least blog about the process as an AI engineer!). This combined approach\\nleverages the strength of RAG for now, while keeping the door open for a custom LLM when it’s viable.\\nTo directly answer your question:  Yes, you can use Vercel and host on your own domain (just add a\\ncustom domain in Vercel settings and point your DNS records accordingly)\\n. For the chatbot, using\\nNext.js would streamline the integration of your AI backend and frontend. Start gathering your personal\\ndata in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='use LoRA to keep it cheap and see how well it performs. By comparing both, you’ll see which one meets\\nyour needs in practice. Many have found RAG to be “faster, cheaper, and way more practical” for Q&A\\nbots\\n, but with your expertise you might get a surprisingly good result with a tailored mini-LLM as well.\\nGood luck with building your personal AI-powered portfolio site!\\nSources:\\nVercel Custom Domain Documentation\\nContentful Blog – Advantages of Next.js (built-in routing and backend)\\nSebastian Raschka – LoRA Fine-tuning 7B models on single GPU\\nReddit (LocalLLaMA) – Advice on knowledge-base chatbots (RAG workflow)\\nStack AI Blog – Fine-tuning vs RAG for custom chatbots\\nSwirlAI Newsletter – Challenges with fine-tuning vs retrieval\\nMicrosoft Learn (Azure AI) – RAG architecture and description\\nNext.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\\nWhat is the best way to create a knowledge-base specific LLM chatbot ? : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/\\n12\\n• \\n3\\n4\\n• \\n2\\n• \\n5\\n• \\n15\\n11\\n• \\n8\\n12\\n• \\n9\\n13\\n• \\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n19\\n8\\n12\\n9\\n13\\n10\\n16\\n17\\n11\\n14\\n15\\n18\\n9'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Architecting the Agentic Portfolio: A \\nComprehensive Technical Report on \\nRAG, Knowledge Graphs, and \\nGenerative 3D Interfaces \\n \\n \\n1. Introduction: The Paradigm Shift to Agentic Personal \\nBranding \\n \\nThe digital portfolio has long served as the static representation of a developer\\'s capability—a \\npassive repository of resumes, project links, and code snippets waiting to be discovered. \\nHowever, the rapid ascent of Large Language Models (LLMs) and Retrieval-Augmented \\nGeneration (RAG) is forcing a fundamental re-evaluation of how professional competence is \\ndemonstrated. We are witnessing a transition from the \"Portfolio as Document\" to the \\n\"Portfolio as Agent.\" In this emerging paradigm, the portfolio is no longer an artifact to be \\nread but an active, intelligent system capable of reasoning about its owner\\'s history, \\nanswering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent. \\nThis transformation is not merely aesthetic; it is a direct response to the increasing complexity \\nof modern software engineering. A static list of skills (e.g., \"React, Python, AWS\") fails to \\nconvey the depth of application—how a candidate handled race conditions in a distributed \\nsystem or optimized a render loop in a graphics application. An Agentic Portfolio, powered by \\nadvanced RAG, bridges this gap by allowing recruiters and technical managers to interrogate \\nthe data: \"How did this candidate optimize database queries in their 2023 e-commerce \\nproject?\" \\nThis report provides an exhaustive architectural blueprint for constructing such a system. It \\nmoves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative \\n3D User Interfaces (GenUI). It rigorously examines the data engineering required to prevent \\nhallucinations, the mathematical principles behind advanced retrieval strategies, and the \\nsoftware engineering practices—specifically unit testing and evaluation—necessary to deploy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='a reliable AI agent. \\n \\n2. Foundational Data Engineering: The bedrock of RAG \\n \\nThe axiom \"Garbage In, Garbage Out\" is the governing law of Retrieval-Augmented \\nGeneration. A RAG system’s intelligence is deterministically limited by the structure and \\nsemantic clarity of its underlying data. While many Minimum Viable Products (MVPs) rely on \\nsimple text extraction from PDFs, this approach is fundamentally flawed for high-stakes \\napplications like professional portfolios, where precision is paramount. \\n \\n2.1 The Fallacy of Unstructured Ingestion \\n \\nStandard ingestion pipelines often utilize libraries such as PyMuPDF or pypdf to strip text from \\nPDF resumes.1 While computationally efficient, these tools discard the semantic signals \\nembedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a \\nhierarchical relationship to the header above them. Linear text extraction flattens this \\nhierarchy. A column-based layout, for instance, might be read line-by-line across columns, \\nmerging a \"Skills\" list with a \"Work History\" description, creating nonsensical chunks such as \\n\"Python 2018-Present Manager at Company X.\" \\nThis loss of structure leads to \"Context Dissociation.\" When an LLM retrieves a chunk \\ncontaining \"reduced latency by 50%,\" but the header identifying the specific project or \\ncompany was stripped during ingestion, the model cannot accurately attribute the \\nachievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the \\nuse of Vision-Language Models (VLMs) like GPT-4o or Claude 3.5 Sonnet for document \\ningestion.3 Unlike OCR (Optical Character Recognition), which identifies characters, VLMs \\nunderstand layout semantics.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='The ingestion pipeline for the Agentic Portfolio functions as follows: \\n1.\\u200b Rasterization: The PDF resume is converted into high-resolution images (e.g., 300 DPI \\nPNGs). \\n2.\\u200b Vision Prompting: The VLM is prompted to transcribe the image into a strictly defined \\nJSON schema. The prompt must explicitly instruct the model to respect visual hierarchy: \\n\"Identify the date ranges associated with each role and nest them within the \\'experience\\' \\nobject. Extract skills listed in sidebars and categorize them.\" \\n3.\\u200b Schema Validation: The output is validated against a Zod or Pydantic schema to ensure \\ntype safety before entering the database. \\nThis approach, while incurring a higher initial computational cost (token usage for image \\nprocessing), ensures a \"Golden Source\" of truth. It allows for the normalization of \\nentities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5 \\n \\n2.3 Defining the Knowledge Schema \\n \\nTo facilitate advanced retrieval strategies like filtering and graph traversal, the unstructured \\nbio data must be mapped to a rigorous schema. We recommend adopting and extending the \\nJSON Resume standard, augmenting it with vector-specific fields. \\nTable 1: Proposed Data Schema for RAG Optimization \\nEntity Field \\nData Type \\nDescription & RAG Utility \\nbasics.summary \\nString \\nA high-level bio used for \\n\"Who are you?\" queries. \\nEmbedded as a single \\nchunk. \\nwork.highlights \\nArray<String> \\nGranular achievements \\n(e.g., \"Optimized SQL \\nqueries\"). Each string is an \\nindividual \"Child Chunk.\" \\nwork.tech_stack \\nArray<String> \\nA list of technologies used \\nin that specific role. Used \\nfor metadata filtering (e.g.,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='filter: { tech_stack: { $in: \\n[\"Python\"] } }). \\nwork.context_blob \\nString \\nA synthetic paragraph \\ngenerated by an LLM that \\ncombines the role, \\ncompany, and dates into a \\nnarrative format. This \\nserves as the \"Parent \\nChunk\" for retrieval. \\nskills.keywords \\nArray<String> \\nSynonyms and related \\nterms (e.g., Skill: \"AWS\", \\nKeywords:). Enhances \\nrecall for varied user \\nqueries. \\nprojects.emb_text \\nString \\nA \"dense\" description \\nspecifically engineered for \\nembedding, removing stop \\nwords and focusing on \\nsemantic keywords. \\nThis structured approach allows for Metadata Filtering.7 Instead of relying solely on cosine \\nsimilarity, the system can execute a pre-filter step. If a user asks, \"What was your experience \\nat Google?\", the retriever creates a filter company == \"Google\" before performing the vector \\nsearch, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent \\n \\nChunking—the process of breaking text into manageable pieces for embedding—is the single \\nmost critical hyperparameter in a RAG pipeline. For a technical portfolio, where the \\nconnection between a specific tool (e.g., Redis) and a specific outcome (e.g., caching layer) is \\nvital, standard fixed-size chunking (e.g., every 500 characters) is disastrous. It risks severing'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='the subject from the predicate.9 \\n \\n3.1 Semantic Chunking \\n \\nSemantic chunking moves beyond arbitrary character counts to respect the \"semantic \\nboundaries\" of the text. The algorithm operates by embedding sequential sentences and \\ncalculating the cosine similarity between them. \\nThe mechanism is as follows: \\n1.\\u200b Sentence Splitting: The text is broken into individual sentences using a natural language \\ntokenizer (e.g., NLTK or SpaCy). \\n2.\\u200b Sequential Embedding: Each sentence is embedded using a lightweight model (e.g., \\nall-MiniLM-L6-v2). \\n3.\\u200b Coherence Calculation: The algorithm calculates the similarity score $S$ between \\nSentence $N$ and Sentence $N+1$. \\n4.\\u200b Breakpoint Detection: If $S$ drops below a defined threshold (e.g., 0.7), it indicates a \\n\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point. \\nThis ensures that each retrieved chunk represents a self-contained thought or topic, \\nsignificantly improving the \"Faithfulness\" metric of the generated answers.11 \\n \\n3.2 The Parent-Document Retrieval Pattern \\n \\nTechnical resumes often contain bullet points that are semantically dense but contextually \\nsparse. A bullet point might read: \"Migrated legacy codebase to TypeScript.\" \\nIf this single sentence is retrieved in isolation, the LLM lacks crucial context: Which company \\nwas this for? When did it happen? What was the impact? Embedding the entire project \\ndescription, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9 \\n●\\u200b Indexing: The system splits the document into small \"Child Chunks\" (individual bullet \\npoints) and embeds them. These are optimized for high-precision matching. \\n●\\u200b Storage: The full \"Parent Document\" (the entire project narrative or work history block) is'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='stored in a separate key-value store (e.g., Redis or the Supabase text column), referenced \\nby a parent_id. \\n●\\u200b Retrieval: The user query matches the dense Child Chunk. The system then uses the \\nparent_id to fetch the full Parent Document. \\n●\\u200b Generation: The LLM receives the full Parent Document as context. \\nThis architectural pattern decouples the retrieval unit (optimized for search) from the \\ngeneration unit (optimized for context), providing the best of both worlds. \\n \\n3.3 Post-Chunking and Late Interaction \\n \\nEmerging research suggests a shift toward \"Post-Chunking\" or Late Interaction models (like \\nColBERT).10 In this paradigm, the entire document is embedded at the token level, and \\ninteraction with the query happens before reducing to a single vector score. While highly \\neffective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic \\nChunking combined with Parent-Document retrieval offers the optimal trade-off between \\nperformance and complexity. \\n \\n4. The Retrieval Engine: Moving Beyond Cosine \\nSimilarity \\n \\nA \"naive\" RAG system relies exclusively on dense vector search (embeddings). While powerful \\nfor semantic conceptual matching, dense retrieval struggles with precise keyword matching, \\nparticularly with technical acronyms (e.g., \"C#\" vs. \"C++\", \"AWS\" vs. \"GCP\"). To achieve \\nindustry standards, the portfolio must implement a Hybrid Search architecture. \\n \\n4.1 Hybrid Search Architecture \\n \\nHybrid search combines the strengths of two distinct retrieval algorithms: \\n1.\\u200b Dense Retrieval (Vector Search): Utilizes embeddings (e.g., OpenAI \\ntext-embedding-3-small) to understand semantic intent. It excels at queries like \"Tell me'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='about your leadership style\" where exact keyword matches are less important than \\nconceptual alignment. \\n2.\\u200b Sparse Retrieval (Keyword Search): Utilizes algorithms like BM25 (Best Matching 25) or \\nSPLADE to match exact tokens. It excels at technical queries like \"Do you know Redux \\nToolkit?\" where the presence of the specific term is non-negotiable.7 \\n \\n4.1.1 Reciprocal Rank Fusion (RRF) \\n \\nTo combine the results from these two disparate algorithms, we employ Reciprocal Rank \\nFusion (RRF). RRF does not rely on the absolute scores (which are incomparable between \\nCosine Similarity and BM25); instead, it relies on the rank of the document in each list. \\nThe RRF score for a document $d$ is calculated as: \\n \\n \\n$$RRFscore(d) = \\\\sum_{r \\\\in R} \\\\frac{1}{k + r(d)}$$ \\n \\nWhere: \\n●\\u200b $R$ is the set of rank lists (one from Dense, one from Sparse). \\n●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$. \\nThis mathematical fusion ensures that a document appearing in the top 5 results of both \\nsearch methods is prioritized over a document that is #1 in vector search but absent in \\nkeyword search.13 \\n \\n4.2 Vector Database Selection: Supabase vs. Pinecone \\n \\nSelecting the right vector store is a critical infrastructure decision. The market is divided \\nbetween specialized vector databases (Pinecone, Weaviate) and integrated vector extensions \\nfor traditional databases (Supabase/Postgres). \\nTable 2: Vector Database Comparison for Portfolio Use Case \\n \\nFeature \\nSupabase \\nPinecone \\nAnalysis for'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='(pgvector) \\nPortfolio \\nArchitecture \\nIntegrated \\n(Postgres \\nExtension) \\nSpecialized \\nManaged Service \\nSupabase allows \\nstoring relational \\ndata (Projects, \\nUsers) alongside \\nvectors in a single \\nDB, simplifying the \\nstack.15 \\nFree Tier \\n500MB DB Size \\n(Millions of vectors) \\nLimited to 1 index, \\nrestricted \\nthroughput \\nSupabase\\'s 500MB \\nlimit is sufficient for \\nvast amounts of \\ntext data (resumes \\nare KBs). \\nPinecone\\'s free tier \\nis restrictive for \\nmultiple projects.17 \\nHybrid Search \\nNative support \\n(tsvector + \\npgvector) \\nSupported via \\n\"Sparse-Dense\" \\nvectors \\nSupabase allows \\nfusing SQL WHERE \\nclauses with vector \\nsearch naturally. \\nPinecone requires \\nmanaging \\nmetadata \\nseparately.19 \\nLatency \\n~150-200ms (Cold \\nboot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20 \\nConclusion: For a personal portfolio, Supabase is the superior architectural choice. It \\neliminates the \"data synchronization\" problem—the risk that the metadata in the vector store \\ndrifts from the primary application database.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='4.3 Reranking: The Precision Layer \\n \\nEven with Hybrid Search, the \"Top K\" retrieved documents may include irrelevant noise. To \\nfilter this, we introduce a Reranking step using a Cross-Encoder model (e.g., \\nbge-reranker-v2-m3 or Cohere Rerank). \\nUnlike Bi-Encoders (embedding models) which process the query and document \\nindependently, Cross-Encoders process them simultaneously, outputting a single scalar score \\nindicating relevance. \\n●\\u200b Workflow: Retrieve top 25 documents via Hybrid Search -> Pass to Reranker -> Keep Top \\n5. \\n●\\u200b Impact: This dramatically increases the density of relevant information in the LLM\\'s \\ncontext window, reducing the likelihood of \"Lost in the Middle\" phenomenon where the \\nLLM ignores context buried in the center of the prompt.7 \\n \\n5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to \\ndatabase design, API development, or server management? Direct retrieval on such queries \\noften yields poor results. To address this, we implement Query Transformation strategies. \\n \\n5.1 Hypothetical Document Embeddings (HyDE) \\n \\nHyDE is a technique that leverages the LLM\\'s internal knowledge to bridge the gap between a \\nshort query and a detailed document.1 \\nMechanism: \\n1.\\u200b Prompt: The system prompts an LLM: \"Write a hypothetical resume entry for a senior \\nengineer answering the query: \\'backend experience\\'.\" \\n2.\\u200b Hallucination: The LLM generates a fictional but semantically rich paragraph: \\n\"Experienced in building RESTful APIs using Node.js and Express, managing PostgreSQL \\ndatabases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector. \\n4.\\u200b Retrieval: The vector search is performed using this hypothetical vector.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Because the hypothetical document shares the same semantic structure (technical \\nvocabulary, sentence structure) as the actual resume chunks, the retrieval accuracy is \\nsignificantly higher than using the raw query \"backend experience.\" \\n \\n5.2 Multi-Query Decomposition \\n \\nFor complex questions like \"Compare your experience with React and Vue,\" a single vector \\nsearch often fails to retrieve sufficient context for both topics. \\nMechanism: \\n1.\\u200b Decomposition: An LLM splits the query into sub-queries:. \\n2.\\u200b Parallel Retrieval: Both queries are executed independently. \\n3.\\u200b Fusion: The unique documents from both retrieval sets are combined (deduplicated). \\n4.\\u200b Generation: The combined context is passed to the LLM to answer the comparison \\nquestion. \\nThis ensures that the system doesn\\'t latch onto one concept (e.g., React) at the expense of \\nthe other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured \\nrelationships. A vector search for \"Frameworks\" might miss \"Next.js\" if the embedding model \\ndoesn\\'t explicitly map the \"is-a\" relationship. GraphRAG addresses this by modeling the \\nportfolio as a Knowledge Graph (KG).21 \\n \\n6.1 Graph Schema Design \\n \\nThe Knowledge Graph structures the portfolio into Entities and Relationships. \\n●\\u200b Nodes: Person, Skill, Project, Company, Role, Concept. \\n●\\u200b Edges: \\n○\\u200b (Project)-->(Skill) \\n○\\u200b (Skill)-->(Concept)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='○\\u200b (Person)-->(Company) \\n○\\u200b (Project)-->(Outcome) \\nThis structure enables Multi-hop Reasoning. A user might ask: \"Which projects used a \\ncloud-native database?\" \\n●\\u200b Vector Search: Finds \"Cloud-native database\" $\\\\rightarrow$ matches DynamoDB node. \\n●\\u200b Graph Traversal: (DynamoDB)<--(Project A) and (DynamoDB)<--(Project B). \\n●\\u200b Result: The system identifies Project A and Project B, even if the project descriptions \\nnever explicitly used the phrase \"cloud-native database.\" \\n \\n6.2 Implementation: NetworkX vs. Neo4j \\n \\nFor enterprise applications, graph databases like Neo4j are standard. However, for a personal \\nportfolio with a limited dataset (< 1000 nodes), running a dedicated Neo4j instance is often \\noverkill in terms of cost and maintenance. \\nLightweight Alternative: NetworkX \\nWe recommend using NetworkX (a Python library) to build and query the graph in-memory or \\nserialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the \\nnode-edge list. This is saved as graph.json. \\n2.\\u200b Run Time: The Next.js API route loads graph.json. Simple traversal algorithms (e.g., \\nfinding all neighbors of a matched node) are executed in JavaScript/TypeScript. \\n3.\\u200b Integration: The graph results are textually formatted (e.g., \"Related Skills: X, Y, Z\") and \\nappended to the context window alongside vector results. \\nThis \"Client-Side Graph\" approach delivers 80% of the value of Neo4j with 0% of the \\ninfrastructure cost.25 \\n \\n7. Generative UI: The Interactive 3D Interface \\n \\nThe defining feature of the Agentic Portfolio is Generative UI—the ability of the AI to not just \\ntalk, but to show and control the interface. We utilize the Vercel AI SDK and React Server \\nComponents (RSC) to stream UI elements.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='7.1 Generative UI Architecture \\n \\nIn a standard chatbot, the output is Markdown. In a Generative UI system, the LLM can output \\nReact components. \\n●\\u200b Mechanism: The Vercel AI SDK\\'s streamUI function allows the server to stream \\nrenderable React components to the client.26 \\n●\\u200b Use Case: \\n○\\u200b User: \"Show me your design skills.\" \\n○\\u200b LLM Response: Instead of listing skills, it streams a <SkillCloud category=\"design\" /> \\ncomponent. The client renders this interactive 3D cloud immediately within the chat \\nstream. \\n○\\u200b Fallback: If the client doesn\\'t support the component, it falls back to a text \\ndescription. \\n \\n7.2 Controlling React Three Fiber via Tool Calling \\n \\nThe portfolio features a 3D scene (built with React Three Fiber) that acts as an immersive \\nbackground. The Chat Agent acts as the controller for this scene. \\nState Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28 \\nImplementation Steps: \\n1.\\u200b Store Definition:\\u200b\\nTypeScript\\u200b\\ninterface State {\\u200b\\n  cameraTarget: Vector3;\\u200b\\n  setCameraTarget: (v: Vector3) => void;\\u200b\\n}\\u200b\\nconst useStore = create<State>((set) => ({... }));\\u200b\\n \\n2.\\u200b Tool Definition: The LLM is provided with a tool moveCamera.\\u200b\\nTypeScript\\u200b\\nconst tools = {\\u200b\\n  moveCamera: tool({\\u200b\\n    description: \\'Move the 3D camera to focus on a specific section.\\',\\u200b\\n    parameters: z.object({ section: z.enum([\\'home\\', \\'projects\\', \\'about\\']) }),\\u200b'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='execute: async ({ section }) => {\\u200b\\n       const coords = SECTION_COORDINATES[section];\\u200b\\n       useStore.getState().setCameraTarget(coords);\\u200b\\n       return `Moved camera to ${section}.`;\\u200b\\n    }\\u200b\\n  })\\u200b\\n};\\u200b\\n \\n3.\\u200b Scene Reaction: Inside the R3F Canvas, a component subscribes to the store.\\u200b\\nTypeScript\\u200b\\nfunction CameraController() {\\u200b\\n  const target = useStore((state) => state.cameraTarget);\\u200b\\n  useFrame((state, delta) => {\\u200b\\n    easing.damp3(state.camera.position, target, 0.5, delta); // Smooth interpolation\\u200b\\n  });\\u200b\\n  return null;\\u200b\\n}\\u200b\\n \\nThis architecture creates a seamless \"Magical\" experience where the user\\'s conversation \\nphysically drives the exploration of the 3D space.30 \\n \\n8. Coding Agents and Prompt Engineering \\n \\nDeveloping this complex system requires leveraging \"Coding Agents\" (like Cursor or GitHub \\nCopilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy \\n \\nTo ensure the coding agent generates code that adheres to the specific architectural \\nconstraints (Next.js 14, Tailwind, Supabase), we place a .cursorrules file in the project root.32 \\nSample Rule Configuration:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy. \\n \\n9.1 Tier 1: Deterministic Unit Testing (pytest) \\n \\nWe mock the LLM and Vector Store to test the logic surrounding the AI.36 \\n●\\u200b Test Case: Verify that the moveCamera tool is correctly invoked when the input text \\ncontains directional intent. \\n●\\u200b Test Case: Verify that the PDF parser correctly extracts email addresses from a mock \\nresume file.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='●\\u200b Mocking: Use unittest.mock to simulate the Supabase client, ensuring that the retrieval \\nlogic handles empty result sets gracefully without crashing. \\n \\n9.2 Tier 2: LLM-based Evaluation (Ragas & DeepEval) \\n \\nTo evaluate the quality of the AI\\'s answers, we use the Ragas framework (Retrieval \\nAugmented Generation Assessment). Ragas uses an \"LLM-as-a-Judge\" (typically GPT-4) to \\nscore the system on specific metrics.38 \\nKey Metrics: \\n1.\\u200b Context Recall: Does the retrieved context contain the ground truth answer? (Measures \\nRetrieval quality). \\n2.\\u200b Faithfulness: Is the generated answer derived solely from the retrieved context, or did \\nthe model hallucinate? (Measures Generation quality). \\n3.\\u200b Answer Relevance: Does the answer directly address the user\\'s query? \\nCI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions. \\n●\\u200b Assertion: assert ragas_score[\\'context_recall\\'] > 0.8 \\n●\\u200b Outcome: If a code change (e.g., changing the chunking size) causes the score to drop \\nbelow 0.8, the PR is blocked. This prevents regressions in the agent\\'s intelligence.40 \\n \\n10. Conclusion \\nThe architecture described in this report represents the convergence of modern Full Stack \\nEngineering and Applied AI. By moving from naive text extraction to Vision-based Parsing, \\nfrom simple vector search to Hybrid Graph Retrieval, and from static text output to \\nGenerative 3D Interfaces, we transform the personal portfolio into a sophisticated Agentic \\nSystem. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate\\'s history. But more importantly, the existence of the \\nsystem itself serves as the ultimate proof of competence. It demonstrates mastery not just of \\ncode, but of data engineering, system architecture, AI orchestration, and user experience \\ndesign—the defining skills of the next generation of software engineers. The Agentic Portfolio \\nis not just a showcase of work; it is the work itself.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"Works cited \\n1.\\u200b Optimizing RAG. RAG Demystified: A Hands-On Guide to… | by Skanda Vivek | \\nEMAlpha, accessed November 20, 2025, \\nhttps://medium.com/emalpha/optimizing-rag-bd65ebc5e51a \\n2.\\u200b Developing Retrieval Augmented Generation (RAG) based LLM Systems from \\nPDFs: An Experience Report - arXiv, accessed November 20, 2025, \\nhttps://arxiv.org/html/2410.15944v1 \\n3.\\u200b Using Azure OpenAI GPT-4o to extract structured JSON data from PDF \\ndocuments, accessed November 20, 2025, \\nhttps://learn.microsoft.com/en-us/samples/azure-samples/azure-openai-gpt-4-vi\\nsion-pdf-extraction-sample/using-azure-openai-gpt-4o-to-extract-structured-js\\non-data-from-pdf-documents/ \\n4.\\u200b How can I process a pdf using OpenAI's APIs (GPTs)? - Stack Overflow, accessed \\nNovember 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL \\nquerying, and agents with Amazon Bedrock, accessed November 20, 2025, \\nhttps://aws.amazon.com/blogs/machine-learning/boosting-rag-based-intelligent-\\ndocument-assistants-using-entity-extraction-sql-querying-and-agents-with-am\\nazon-bedrock/ \\n6.\\u200b Conversion of entire PDF into JSON Format - API - OpenAI Developer \\nCommunity, accessed November 20, 2025, \\nhttps://community.openai.com/t/conversion-of-entire-pdf-into-json-format/1066\\n551 \\n7.\\u200b 9 advanced RAG techniques to know & how to implement them - Meilisearch, \\naccessed November 20, 2025, https://www.meilisearch.com/blog/rag-techniques \\n8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025, \\nhttps://www.datasciencecentral.com/best-practices-for-structuring-large-datas\\nets-in-retrieval-augmented-generation-rag/ \\n9.\\u200b Top 13 Advanced RAG Techniques for Your Next Project - Analytics Vidhya, \\naccessed November 20, 2025, \\nhttps://www.analyticsvidhya.com/blog/2025/04/advanced-rag-techniques/ \\n10.\\u200bBeyond Basic Chunking: The Critical Timing Decision in RAG Systems That \\nEveryone Is Getting Wrong, accessed November 20, 2025, \\nhttps://skngrp.medium.com/beyond-basic-chunking-the-critical-timing-decision-\\nin-rag-systems-that-everyone-is-getting-wrong-19febb2ee062 \\n11.\\u200bComparing Chunking Strategies for RAG: From Naive Splits to Striding Windows | \\nby Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116 \\n12.\\u200bMastering Chunking Strategies for RAG: Best Practices & Code Examples - \\nDatabricks Community, accessed November 20, 2025,\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/ \\n19.\\u200bpgvector vs Pinecone: cost and performance - Supabase, accessed November \\n20, 2025, https://supabase.com/blog/pgvector-vs-pinecone \\n20.\\u200bSupabase vs Pinecone: I Migrated My Production AI System and Here's What \\nActually Matters - Dee, accessed November 20, 2025, \\nhttps://deeflect.medium.com/supabase-vs-pinecone-i-migrated-my-production-\\nai-system-and-heres-what-actually-matters-7b2f2ebd59ee \\n21.\\u200bGraphRAG for Devs: Graph-Code Demo Overview - Memgraph, accessed \\nNovember 20, 2025, \\nhttps://memgraph.com/blog/graphrag-for-devs-coding-assistant \\n22.\\u200bThe GraphRAG Manifesto: Adding Knowledge to GenAI - Neo4j, accessed \\nNovember 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber \\n29.\\u200bAccessing the Camera in React Three Fiber out of the canvas - Questions, \\naccessed November 20, 2025,\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='https://discourse.threejs.org/t/accessing-the-camera-in-react-three-fiber-out-of\\n-the-canvas/39137 \\n30.\\u200bMove camera to face an object in React Three Fiber! - Questions, accessed \\nNovember 20, 2025, \\nhttps://discourse.threejs.org/t/move-camera-to-face-an-object-in-react-three-fi\\nber/81269 \\n31.\\u200bCamera Controls - Wawa Sensei, accessed November 20, 2025, \\nhttps://wawasensei.dev/courses/react-three-fiber/lessons/camera-controls \\n32.\\u200bPatrickJS/awesome-cursorrules: Configuration files that enhance Cursor AI editor \\nexperience with custom rules and behaviors - GitHub, accessed November 20, \\n2025, https://github.com/PatrickJS/awesome-cursorrules \\n33.\\u200bHow Cursor project rules can improve Next.js app development - LogRocket \\nBlog, accessed November 20, 2025, \\nhttps://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025, \\nhttps://cursorrules.org/article/nextjs15-react19-vercelai-tailwind-cursorrules-pro\\nmpt-file \\n35.\\u200bThe ultimate .cursorrules for TypeScript, React 19, Next.js 15, Vercel AI SDK, \\nShadcn UI, Radix UI, and Tailwind CSS : r/cursor - Reddit, accessed November 20, \\n2025, \\nhttps://www.reddit.com/r/cursor/comments/1gjd96h/the_ultimate_cursorrules_for\\n_typescript_react_19/ \\n36.\\u200bHow to Properly Mock LangChain LLM Execution in Unit Tests | Python - Medium, \\naccessed November 20, 2025, \\nhttps://medium.com/@matgmc/how-to-properly-mock-langchain-llm-execution-\\nin-unit-tests-python-76efe1b8707e \\n37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025 \\nMaster of Science in Artificial Intelligence (Concentration: Machine Learning), GPA – 4.0                                                                                                                               Denton,TX  \\nCoursework : Machine Learning, Deep Learning, NLP, Information Retrieval, Big Data & Data Science, AI Software Development \\n.    \\nBirla Institute of Technology and Science                                                                                                                                                            June 2019 \\nBachelor of Science in Mechanical Engineering with Honours, GPA – 3.7                                                                                                                                                        Dubai, UAE  \\n \\nTECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present \\n• Cut BA story update effort by 80-90% by deploying a multi-agent pipeline (Fal.ai STT + GPT-4.1 in n8n) that updates Jira Cloud tickets end-to-end \\n(Description + Acceptance Criteria + Status/Assignee) via REST APIs. \\n• Built a Chrome extension (JS/HTML/CSS + REST) to capture voice → transcript → LLM output → structured Jira updates, improving time-to-update \\nfrom ~12 min → ~2 min. \\n• Authored an Acceptance Criteria (AC) Playbook and an NLP variance/coverage harness (Jaccard/F1) to benchmark LLM AC vs. senior BA standards; \\nimproved consistency and completeness of ACs. \\n• Tracked latency/cost per run; instrumented token usage and error categories to guide prompt and model selection decisions. \\nArtificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025 \\n• Assessed and provided actionable feedback on peer reviews of key AI research papers, enhancing students’ critical thinking and technical writing across \\nmultiple evaluation cycles. \\n• Led design and experimentation of ML solutions including GANs, transformers, and optimized CNNs for NLP and GenAI tasks. Applied CRISP-DM \\nmethodology and collaborated with academic teams on scalable prototype development for potential deployment.  \\n• Mentored students on applied AI projects, delivering personalized technical guidance that improved project success rates and supported milestone \\nachievement in coursework and capstone deliverables. \\n \\nML/AI Engineer – AI Document Workflow Automation                                                                                            Strawberry Labs, Dubai | Jan.2023 – Jan 2024 \\n• Deployed an OCR + extraction pipeline (Tesseract + CNN post-processing) processing docs in ~20s each, delivering 96% OCR accuracy and 93% \\nfield extraction precision; manual data-entry time −75% (5 min → <1 min). \\n• In controlled tests, achieved 4× faster completion (20s vs 90s) and −70% QA checks, with 96% data correctness vs 85% in manual entry. \\n• Implemented validation rules (format, range, cross-field), exception routing, and confidence scoring to reduce human review and raise straight-through \\nprocessing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS \\n  \\nConditional GAN for Ethnicity-Based Face Generation                                                                                                                             UNT, Denton, TX | Sep2024 – Dec 2024 \\n• Engineered a generative adversarial network (GAN) from the ground up to create facial images across six ethnicities (4,000 color images each), enabling data-\\ndriven insights into cross-cultural face generation.  \\n• Transitioned from an initial linear-layer design to convolutional layers for more robust feature extraction, improving model fidelity and output quality. \\n• Leveraged AWS to meet high computational (GPU) demands, streamlining large-scale model training and accelerating development cycles. \\nAutomated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024 \\n• Developed a pipeline that automatically identifies and classifies product components as “good” or “bad,” reducing manual inspection time and cost. \\n• Implemented advanced edge detection (Sobel, Laplacian of Gaussian) and custom convolution kernels to achieve a 100% detection rate (mIoU=0.8925). \\n• Leveraged both spatial (Probabilistic Hough Transform) and frequency domain (Fourier magnitude) methods to capture subtle texture patterns. \\n• Achieved 77% accuracy, 87% precision, and AUC of 0.85 on real-world data, with future improvements planned via data augmentation and refined \\nlighting/occlusion handling.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d7e13",
   "metadata": {},
   "source": [
    "### Chunking the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876a768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 28 documents into 98 chunks\n",
      "Example Chunk\n",
      "Content: Building a Personal Portfolio Q&A Chatbot\n",
      "Framework and Hosting Considerations\n",
      "Choosing the right framework for your portfolio site is important for ease of development and deployment.\n",
      "Next.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\n",
      "built-in routing, and easy integration of backend logic via API routes\n",
      ". These features can improve\n",
      "performance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\n",
      "configuration work out of the box). For example, Next.js allows you to “easily create your custom backend\n",
      "functionalities with API Routes to power your own front end”, all without extra client-side bloat\n",
      ". This\n",
      "means you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \n",
      "By contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\n",
      "Metadata: {'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='By contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='automatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='infrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n•'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Start thinking about the content that might go into this personal knowledge base. For a portfolio Q&A\\nchatbot about you, the data could include things like:\\nA written bio or introduction: e.g. your background, education, skills, projects, interests.\\nResume or CV data: your work experience, accomplishments, maybe in a structured Q&A form\\n(“Where did I work in 2022?”, “What projects have I done in machine learning?”, etc.).\\nFrequently asked questions about you: This could be a list of questions and answers (FAQ) that you\\nanticipate someone might ask. For example: “What are your expertise areas?”, “What was your MSc\\nthesis about?”, “What hobbies do you have?” – along with the answers in your own words.\\nAny personal blog posts or writings: if relevant, these can provide context on your opinions or\\nknowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='knowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.\\nSince you want the chatbot to answer based only on what “it knows” (i.e. your provided data, with no live\\ninternet connection), we will either be training a model on this data or feeding this data into a retrieval\\nsystem for the model. In either case, you’ll need to gather and curate the information about yourself. This\\ncan be done incrementally: once we decide on the approach, you can compile the data into the needed\\nformat (documents, Q&A pairs, etc.). \\nIf we go with a fine-tuning approach: you may need to format the data as a training dataset (for example, a\\nlist of prompt-response pairs where the prompt is a question about you and the response is the correct\\nanswer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='answer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –\\nbut you do need enough coverage of facts about you so the model can learn them. Ensure the info is\\naccurate and expressed in the tone you want the answers to have.\\nIf we go with a retrieval-based approach: you might store the data as a set of documents or text passages.\\nThese could be chunks of a “About Me” document, or individual Q&A entries, etc. The quality of answers will\\ndepend on providing sufficient detail in these source texts. The nice thing is that you can start with a basic\\nset of documents (like a few paragraphs about you, plus a list of Q&A) and always update or expand it later\\nwithout retraining a model – we’ll discuss this more under the retrieval approach.\\nNow, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content=\"Now, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\"),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content=\"example, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\\nName] is an AI engineer with a focus on NLP and LLMs, currently working on...”) along with many other\\n• \\n• \\n• \\n• \\n• \\n2\"),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Q&As covering your background, skills, projects, etc. The fine-tuning process will adjust the model’s weights\\nso that it “learns” these specific facts and can respond in a conversational style about them.\\nThanks to techniques like Low-Rank Adaptation (LoRA), it’s feasible to fine-tune moderately large models\\non a single GPU. In fact, LoRA has been shown to allow fine-tuning a 7-billion-parameter model (such as\\nLLaMA-2 7B) on a single GPU\\n. One report notes: “LoRA allows us to finetune 7B parameter LLMs on a single\\nGPU. In [one case], using QLoRA (quantized LoRA) with optimal settings required ~17.8 GB GPU memory and about\\n3 hours on an A100 GPU for 50k training examples”\\n. This means that if you have access to a GPU with\\n~24GB VRAM (or use a cloud service), you could potentially fine-tune a model on a custom dataset without\\nhuge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='huge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training\\nwould be faster (possibly an hour or two, depending on the model and hyperparameters).\\nState-of-the-art techniques you might consider for this include LoRA/QLoRA (to reduce memory and\\ncompute), and using an instruction-tuned base model. For example, starting with an instruction-following\\nmodel (like LLaMA-2-chat or Dolly, etc.) might yield better conversational answers after fine-tuning. You\\ncould also experiment with lightweight fine-tuning vs full fine-tuning. LoRA is nice because it keeps the\\noriginal model intact and just learns small adapter weights – this is efficient and you can revert to the base\\nmodel easily if needed.\\nBefore committing to fine-tuning, consider the pros and cons:\\nPros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Pros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).\\nIt can be run locally or on your server without external API calls, preserving privacy (important if\\nsome personal data is sensitive).\\nAs an AI engineer, you get the learning experience of doing a fine-tune with SOTA methods. You can\\nexperiment with parameters, try new fine-tuning optimizations, etc., which can be valuable\\nexperience.\\nThe inference might be slightly faster per query than a retrieval approach for small queries, since it’s\\njust the model response (no vector database lookup overhead) – though in practice the difference\\nmay be small.\\nCons of fine-tuning approach:\\nData requirements: Fine-tuning is effectively training, so if you have very little data about yourself,\\nthe model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='the model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-\\ntune with too few examples or only extremely narrow phrasing. You might need to be creative in\\ngenerating enough Q&A pairs (possibly augmenting with rephrased questions) so the model sees\\nsufficient variety. As one discussion put it: don’t try to fine-tune when you have too little data or just to\\ninject a few facts – that’s what prompt context or retrieval is for; “Don’t use a bulldozer to kill a fly.”\\n.\\nFine-tuning a model to learn new knowledge (like personal facts it didn’t know) is possible but\\nworks best if you provide a reasonably sized corpus of that knowledge\\n.\\nMaintenance and updates: If your personal information changes or you want to add new data (say\\nyou start a new job or complete new projects), you’d have to fine-tune a new version of the model or\\nat least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='at least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6\\n7\\n• \\n3'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='trained, the model’s knowledge is fixed. As one source notes, fine-tuning is “powerful on paper, but\\nexpensive, time-consuming, and a nightmare to maintain every time your data changes”\\n.\\nCompute and cost: While a small-scale fine-tune is much cheaper than training from scratch, it’s not\\nfree. You need suitable hardware. If using cloud GPUs, that could cost some money (though a single\\n1-3 hour run on an A100 or similar might be on the order of tens of dollars, which isn’t too bad). Still,\\nif you iterate multiple times, it adds up. Also hosting the final model (for inference) means you need\\na server (or at least something like a GPU or CPU instance) to run the model continuously for your\\nwebsite. A 7B model can run on CPU but might be slow; more likely you’d run on a GPU for snappier\\nresponses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='responses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger\\nbase model. Open-source models are improving rapidly, but something like a fine-tuned 7B or 13B\\nparameter model will be less fluent and sometimes less accurate than, say, GPT-4. It might also\\nhallucinate answers if asked something outside of what it was trained on (or even confuse facts if the\\nprompt is tricky). Notably, fine-tuning doesn’t inherently fix the hallucination problem of LLMs\\n.\\nThe model might still “make up” an answer if asked a question it doesn’t know and you haven’t built\\nin a mechanism to handle that (like a “I don’t know” response).\\nExpertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Expertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s\\ndefinitely doable (especially since you’re an AI engineer), but it’s more involved than the retrieval\\napproach. One write-up on private knowledge chatbots explicitly pointed out that to fine-tune an\\nopen-source model for a knowledge base, you need “specialized talent and a large amount of time to\\nsolve the fine tuning challenge internally”\\n. In other words, be prepared for some experimentation\\nand debugging.\\nIn summary, training a personal mini-LLM using fine-tuning is feasible and would be our first choice to\\nexplore if you’re keen on it. It gives you a self-contained model that can answer questions about you.\\nHowever, be mindful that this approach is best if you have a decent amount of personal data to teach the'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='However, be mindful that this approach is best if you have a decent amount of personal data to teach the\\nmodel and if you’re ready to handle updates via retraining. Given the constraints (limited data and budget),\\nwe should compare this with Approach 2, which might be more efficient for the task.\\nApproach 2: Retrieval-Augmented Q&A (Using a Context File or\\nVector DB)\\nThe alternative (and increasingly common) approach is to avoid training altogether and instead use a pre-\\ntrained model with a retrieval mechanism. This is often called Retrieval-Augmented Generation (RAG). In\\nthis setup, you  provide the model with relevant information at query time by retrieving it from a\\nknowledge base of your documents. The model then generates an answer using that information as\\ncontext.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='context.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal\\ndata (the same kind of data we discussed: your bio, Q&A pairs, etc.). When a user asks a question on your\\nportfolio site, the system will pull out the parts of that data that are most relevant to the question and feed\\nthose, along with the question, into the prompt for the LLM. The LLM (which could be a large pre-trained\\nmodel like GPT-3.5, GPT-4, or an open-source model you host) then answers based only on that provided\\ncontext.\\n8\\n• \\n• \\n9\\n• \\n9\\n4'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Example architecture of a retrieval-augmented Q&A system. A user’s query is first used to fetch relevant knowledge\\n(documents or data) from a store (vector database or search index), and that knowledge is combined with the\\nquery as input to the LLM. The LLM then produces an answer grounded in the provided data\\n.\\nThis approach has several advantages for your use case: - No model training needed: The heavy lifting\\nhas been done by the base model. You don’t adjust its weights; you just give it information. This avoids the\\nexpense and complexity of fine-tuning. As one source explains, you “keep the model as it is and simply plug it\\ninto your knowledge sources… the model retrieves the latest info at runtime, answers in context, and stays\\naccurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='accurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the\\ndocuments or knowledge base. The next question asked will then retrieve from the new data immediately.\\nNo need to retrain a model or deploy new weights. This makes the system much easier to keep up-to-date\\n(no “nightmare to maintain every time your data changes” as with constant fine-tuning\\n). -  Smaller\\ndeployment footprint: Instead of hosting a custom model, you could use an API (like OpenAI’s) or host a\\nsmaller model just for inference. The retrieval step might need a vector database, but there are lightweight\\noptions (even in-memory). Many real-world chatbot systems use this pattern because it scales well – you\\ncan  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='can  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and\\ngrounding: Because the answers are drawn from provided text, you reduce the chance of the model\\nhallucinating incorrect facts about you. Essentially, the model is forced to base its answer on the snippets of\\ntext you supply. (Of course, the model could still do a poor job or hallucinate connections between facts, but\\nif prompted to only use the given info, it tends to stick to it). A Reddit user summarizing best practices\\nnoted that “Prompt design matters as much as retrieval. Instruct the model to stick to provided excerpts… This\\nreduces hallucinations and builds trust with users.”\\n. You can even have the model cite the sources (in a\\nmore advanced implementation), which is common in RAG setups for enterprise knowledge bases. - Speed\\nand cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='and cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper\\nand faster. You’re not paying the cost of training. At query time, vector search is usually fast (milliseconds to\\na few tens of ms), and then you call the model to generate an answer. If using an API like OpenAI, you pay\\nper call (plus maybe a small cost for vector DB if using a cloud one). If using an open source model, the\\ncompute to run it is similar to if it were fine-tuned. Many have concluded that “RAG has become the go-to\\nchoice for many use cases: it’s faster, cheaper, and way more practical for real-world teams”\\n.\\nHow it can be implemented: The general flow is: 1. Indexing your personal data: Take your documents\\nor Q&A pairs about yourself and break them into chunks (e.g. paragraphs or individual Q&A entries). For\\neach chunk, generate an  embedding vector (a numeric representation of the text meaning) using an\\n10\\n8\\n8\\n11\\n12\\n5'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='embedding model\\n. Store these vectors in a vector database (or use a simple in-memory vector index if\\nthe data is small) along with the chunk text. There are many tools for this; libraries like  LangChain or\\nLlamaIndex can automate a lot of it, and vector DBs like Pinecone, Weaviate, or open-source ones like FAISS\\nor Qdrant can store the data\\n. A Reddit comment succinctly described this: “chunk your docs, embed them,\\nand store in a vector database. At query time, retrieve the most relevant chunks and pass them into the\\nLLM...”\\n. 2. Retrieval on query: When a question is asked (“How many years of experience does [Your\\nName] have in AI?” for example), the system creates an embedding for the query and searches for similar\\nvectors in your vector DB. It might return, say, the top 3 chunks that are most relevant – maybe one of those\\nchunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='chunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct\\nprompt with context: The retrieved text chunks are then combined with the user’s question to form the\\nprompt for the LLM. For instance, the prompt might be something like:  “Context: [excerpt from your bio\\nsaying you have 5 years in AI]. Q: How many years of experience do I have in AI? A:” – and the model will\\nhopefully respond: “You have 5 years of experience in AI.” 4.  Model answer: The LLM (which could be\\nrunning via an API or a local model) generates an answer using the context. If all goes well, the answer will\\nbe correct, because the needed info was in the provided context. We can also instruct the model with a\\nsystem/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='system/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component\\n(in that diagram, “Azure AI Search” plays the role of vector DB/search engine) which returns knowledge, and\\nthat knowledge + query go into the LLM to get a final answer\\n. Notably, this approach requires no fine-\\ntuning of the LLM’s weights – the model remains as-is (pretrained on general data), and we just augment\\nits input with relevant data at runtime\\n.\\nTools and options: Since you’re an AI engineer, you might enjoy building the pieces yourself, or you can use\\nexisting frameworks: - LangChain and LlamaIndex (formerly GPT Index) are high-level libraries that let you set\\nup a QA chain over your documents very quickly. They handle splitting text, embedding (you can choose\\nmodels like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='models like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow\\n. For example, LangChain has a RetrievalQA  chain that does exactly this once you provide it a vector\\nstore and an LLM. These libraries also help with prompt management. - Vector database choices: If you\\nprefer not to rely on an external cloud service, you can use an open-source solution. FAISS (by Facebook)\\ncan run in-memory or on disk and is often used for small to medium cases. For larger scale or convenience,\\nservices like Pinecone or Weaviate can host it (though for a personal portfolio, that’s probably overkill).\\nThere are lightweight options like an SQLite + embeddings or even just computing cosine similarity on the\\nfly for small data. Given your data will be relatively small (maybe a few pages of text in total), even a simple\\napproach would work. The key is the concept, not the specific tech. - Model choice for answering: You'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='approach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='capable at following instructions. - No live data needed: As you specified, this system does not need to\\nfetch real-time info from the internet. All knowledge is static in your provided data. That’s perfectly in line\\nwith RAG – the knowledge base is whatever you load into the vector store. It won’t go out and search\\nbeyond that. If a question is asked that isn’t answerable from your data, the ideal behavior is to say “I don’t\\n13\\n14\\n15\\n10\\n16\\n17\\n18\\n6'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='know” or some graceful fallback. You can program the prompt to encourage that (e.g., “If the answer is not\\nin the provided context, say you don’t know.”).\\nPotential downsides of retrieval approach: - It is a bit more moving parts: you have to set up an\\nembedding process and store. However, for a one-person project, this is fairly straightforward and many\\ntutorials exist. It’s arguably less work than doing a fine-tune from scratch. - At query time, the model’s\\nresponse is limited by what it sees in the context. If your context window (the prompt length) of the model\\nis, say, 4,000 tokens, and your entire personal knowledge base is larger than that, the retrieval step must be\\neffective at picking the right pieces of info. If it misses something relevant, the answer might be incomplete.\\nBut since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='But since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is\\nmanageable. - If not properly instructed, the model could ignore the context and hallucinate. But in practice,\\nif you supply a relevant context chunk, models like GPT-3.5 or LLaMA-2-chat will use it when answering. -\\nOne consideration: if you want the Q&A to have some memory or multi-turn conversation about you, you’d\\nhave to include previous Q&A in context as well. But since it’s mostly fact-based about you, each question\\ncan probably be handled independently (stateless Q&A).\\nGiven the above, the retrieval-based approach is quite appealing for simplicity and robustness. It’s generally\\nthe  preferred method in industry for Q&A bots on custom data because of the maintenance and\\naccuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='accuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed\\nthe model to deeply internalize a style or do complex transformations, or if retrieval latency was a big\\nissue).\\nComparison and Recommendation\\nBoth approaches can ultimately achieve your goal: a chatbot that answers questions about you, without\\nhooking into live external sources. The best choice depends on your priorities (learning experience vs.\\nsimplicity, one-time effort vs. ongoing flexibility).\\nTraining a Personal LLM might be your first inclination as an AI engineer because it’s an interesting\\nproject. It will let you experiment with the latest fine-tuning techniques (LoRA, QLoRA, etc.) and truly “own”\\nthe model that results. If you go this route, try to leverage existing models and do a relatively lightweight\\nfine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='fine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat\\n(for instance, LLaMA-2 13B Chat has good performance). Use LoRA to fine-tune it on a curated set of Q&A\\nabout you. Monitor for overfitting – since your dataset might be small, you might only do one or two epochs\\nover it\\n. It’s even possible that just a few hundred training steps could suffice if using a high-quality base\\nmodel. - Ensure your fine-tuning dataset is high quality and diverse within the realm of “about you.” If\\nthere are specific phrasings or tricky factual questions (like dates, spellings of names, etc.), include those.\\nThe model will memorize those facts. Be cautious: the model might generalize in unintended ways (you\\nwouldn’t want it to start answering beyond your data and being wrong). - After training, you’ll need to\\ndeploy the model. For a portfolio site, you might run the model on a server that the site can send requests'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='deploy the model. For a portfolio site, you might run the model on a server that the site can send requests\\nto. Running a 7B model with int8 quantization on CPU is possible but may be slow (several seconds per\\nanswer). Running on a GPU (even a cheap one) or using a model served via an API (like HuggingFace\\nInference Endpoint or similar) could be better for snappy responses.\\nUsing Retrieval (RAG) is, in contrast, more of a software engineering solution than a model-training\\nsolution. My recommendation is to strongly consider this approach, because it aligns well with having\\nlimited data and budget but needing accurate results. Here’s how you might implement it step by step: 1.\\n19\\n7'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Begin with an open-source model or an API. For example, try OpenAI’s GPT-3.5 Turbo on some manually\\ncrafted prompts using your data (even before setting up any vector DB) to see how it performs when given\\ncontext. This costs very little and gives a baseline. You can later swap to an open model if you want to self-\\nhost. 2. Use a library like LangChain to index your personal info. You could literally have a Python script\\nwhere you input a bunch of strings (your bio, some Q&As) and it uses an embedding model (say OpenAI’s\\nembeddings,  or  SentenceTransformers  locally)  to  create  vectors  and  store  them  in  something  like\\nChromaDB (which is an easy local vector store that LangChain supports). 3. Hook up a simple API route (if\\nusing  Next.js,  for  example)  that  takes  a  user’s  question,  does  the  retrieval,  and  returns  the  answer.\\nLangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='LangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only\\na few dozen lines of code once the environment is set up. 4. Test and refine: see if the answers are accurate.\\nIf the bot ever says “I don’t know that” for something you did provide in the data, you might need to ensure\\nthe embedding is picking it up or add more context. You can also tweak prompts (e.g., add a system\\nmessage: “You are a chatbot that answers questions only using the provided context about [Your Name]. If\\nyou cannot find the answer in the context, say you do not know.”).\\nCost-wise, retrieval approach can be very cheap, especially if you use local models. If using an API, you pay\\nper call but the usage for a personal portfolio (with presumably low traffic) is negligible. Fine-tuning has an\\nupfront cost (compute for training) but then usage of the model is just the cost of running a server.'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='upfront cost (compute for training) but then usage of the model is just the cost of running a server.\\nIt’s worth noting one hybrid idea: you could fine-tune a smaller model and use retrieval with it. For example,\\nfine-tune a 7B model on a small dataset just to give it some familiarity with your style or key facts, but still\\nuse a vector store to feed it more detailed or less frequently used facts. This is probably unnecessary here –\\nit’s more complex and the pure retrieval method should suffice – but it’s an option if you find the fine-tuned\\nmodel alone isn’t reliable for less common questions.\\nIn conclusion, if we “look at both options” as you requested: - Option 1 (Personal LLM via fine-tuning):\\nFeasible with LoRA on a 7B/13B model; provides a self-contained model; requires more upfront work and\\ndoesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='doesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use\\ncase if done right, but keep expectations reasonable in terms of model accuracy. - Option 2 (RAG with no\\ntraining): More straightforward and likely to give accurate, up-to-date answers; leverages powerful existing\\nmodels; minimal cost to maintain; easier to scale or improve incrementally. For a “simple ask questions\\nabout me and get answers” goal, this is arguably the best way to complete the task with least friction. The\\nfact that industry solutions favor RAG for Q&A on custom data\\n is a strong indicator.\\nGiven  the  constraints  (limited  data  and  budget)  and  the  desire  for  state-of-the-art  techniques,  my\\nsuggestion would be: Why not do both, sequentially? You could implement the RAG approach first to have\\na working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='a working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same\\ndata to see how it compares. This way, your portfolio has a reliable Q&A function (backed by retrieval and a\\nrobust model), and you still get to play with training a model as a side project (which you can swap in if it\\nbecomes good enough, or at least blog about the process as an AI engineer!). This combined approach\\nleverages the strength of RAG for now, while keeping the door open for a custom LLM when it’s viable.\\nTo directly answer your question:  Yes, you can use Vercel and host on your own domain (just add a\\ncustom domain in Vercel settings and point your DNS records accordingly)\\n. For the chatbot, using\\nNext.js would streamline the integration of your AI backend and frontend. Start gathering your personal\\ndata in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='data in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='use LoRA to keep it cheap and see how well it performs. By comparing both, you’ll see which one meets\\nyour needs in practice. Many have found RAG to be “faster, cheaper, and way more practical” for Q&A\\nbots\\n, but with your expertise you might get a surprisingly good result with a tailored mini-LLM as well.\\nGood luck with building your personal AI-powered portfolio site!\\nSources:\\nVercel Custom Domain Documentation\\nContentful Blog – Advantages of Next.js (built-in routing and backend)\\nSebastian Raschka – LoRA Fine-tuning 7B models on single GPU\\nReddit (LocalLLaMA) – Advice on knowledge-base chatbots (RAG workflow)\\nStack AI Blog – Fine-tuning vs RAG for custom chatbots\\nSwirlAI Newsletter – Challenges with fine-tuning vs retrieval\\nMicrosoft Learn (Azure AI) – RAG architecture and description\\nNext.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='Next.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': 'pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf'}, page_content='RAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\\nWhat is the best way to create a knowledge-base specific LLM chatbot ? : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/\\n12\\n• \\n3\\n4\\n• \\n2\\n• \\n5\\n• \\n15\\n11\\n• \\n8\\n12\\n• \\n9\\n13\\n• \\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n19\\n8\\n12\\n9\\n13\\n10\\n16\\n17\\n11\\n14\\n15\\n18\\n9'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Architecting the Agentic Portfolio: A \\nComprehensive Technical Report on \\nRAG, Knowledge Graphs, and \\nGenerative 3D Interfaces \\n \\n \\n1. Introduction: The Paradigm Shift to Agentic Personal \\nBranding \\n \\nThe digital portfolio has long served as the static representation of a developer\\'s capability—a \\npassive repository of resumes, project links, and code snippets waiting to be discovered. \\nHowever, the rapid ascent of Large Language Models (LLMs) and Retrieval-Augmented \\nGeneration (RAG) is forcing a fundamental re-evaluation of how professional competence is \\ndemonstrated. We are witnessing a transition from the \"Portfolio as Document\" to the \\n\"Portfolio as Agent.\" In this emerging paradigm, the portfolio is no longer an artifact to be \\nread but an active, intelligent system capable of reasoning about its owner\\'s history, \\nanswering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='answering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent. \\nThis transformation is not merely aesthetic; it is a direct response to the increasing complexity \\nof modern software engineering. A static list of skills (e.g., \"React, Python, AWS\") fails to \\nconvey the depth of application—how a candidate handled race conditions in a distributed \\nsystem or optimized a render loop in a graphics application. An Agentic Portfolio, powered by \\nadvanced RAG, bridges this gap by allowing recruiters and technical managers to interrogate \\nthe data: \"How did this candidate optimize database queries in their 2023 e-commerce \\nproject?\" \\nThis report provides an exhaustive architectural blueprint for constructing such a system. It \\nmoves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='moves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative \\n3D User Interfaces (GenUI). It rigorously examines the data engineering required to prevent \\nhallucinations, the mathematical principles behind advanced retrieval strategies, and the \\nsoftware engineering practices—specifically unit testing and evaluation—necessary to deploy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='a reliable AI agent. \\n \\n2. Foundational Data Engineering: The bedrock of RAG \\n \\nThe axiom \"Garbage In, Garbage Out\" is the governing law of Retrieval-Augmented \\nGeneration. A RAG system’s intelligence is deterministically limited by the structure and \\nsemantic clarity of its underlying data. While many Minimum Viable Products (MVPs) rely on \\nsimple text extraction from PDFs, this approach is fundamentally flawed for high-stakes \\napplications like professional portfolios, where precision is paramount. \\n \\n2.1 The Fallacy of Unstructured Ingestion \\n \\nStandard ingestion pipelines often utilize libraries such as PyMuPDF or pypdf to strip text from \\nPDF resumes.1 While computationally efficient, these tools discard the semantic signals \\nembedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='embedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a \\nhierarchical relationship to the header above them. Linear text extraction flattens this \\nhierarchy. A column-based layout, for instance, might be read line-by-line across columns, \\nmerging a \"Skills\" list with a \"Work History\" description, creating nonsensical chunks such as \\n\"Python 2018-Present Manager at Company X.\" \\nThis loss of structure leads to \"Context Dissociation.\" When an LLM retrieves a chunk \\ncontaining \"reduced latency by 50%,\" but the header identifying the specific project or \\ncompany was stripped during ingestion, the model cannot accurately attribute the \\nachievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='achievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the \\nuse of Vision-Language Models (VLMs) like GPT-4o or Claude 3.5 Sonnet for document \\ningestion.3 Unlike OCR (Optical Character Recognition), which identifies characters, VLMs \\nunderstand layout semantics.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='The ingestion pipeline for the Agentic Portfolio functions as follows: \\n1.\\u200b Rasterization: The PDF resume is converted into high-resolution images (e.g., 300 DPI \\nPNGs). \\n2.\\u200b Vision Prompting: The VLM is prompted to transcribe the image into a strictly defined \\nJSON schema. The prompt must explicitly instruct the model to respect visual hierarchy: \\n\"Identify the date ranges associated with each role and nest them within the \\'experience\\' \\nobject. Extract skills listed in sidebars and categorize them.\" \\n3.\\u200b Schema Validation: The output is validated against a Zod or Pydantic schema to ensure \\ntype safety before entering the database. \\nThis approach, while incurring a higher initial computational cost (token usage for image \\nprocessing), ensures a \"Golden Source\" of truth. It allows for the normalization of \\nentities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='entities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5 \\n \\n2.3 Defining the Knowledge Schema \\n \\nTo facilitate advanced retrieval strategies like filtering and graph traversal, the unstructured \\nbio data must be mapped to a rigorous schema. We recommend adopting and extending the \\nJSON Resume standard, augmenting it with vector-specific fields. \\nTable 1: Proposed Data Schema for RAG Optimization \\nEntity Field \\nData Type \\nDescription & RAG Utility \\nbasics.summary \\nString \\nA high-level bio used for \\n\"Who are you?\" queries. \\nEmbedded as a single \\nchunk. \\nwork.highlights \\nArray<String> \\nGranular achievements \\n(e.g., \"Optimized SQL \\nqueries\"). Each string is an \\nindividual \"Child Chunk.\" \\nwork.tech_stack \\nArray<String> \\nA list of technologies used \\nin that specific role. Used \\nfor metadata filtering (e.g.,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='filter: { tech_stack: { $in: \\n[\"Python\"] } }). \\nwork.context_blob \\nString \\nA synthetic paragraph \\ngenerated by an LLM that \\ncombines the role, \\ncompany, and dates into a \\nnarrative format. This \\nserves as the \"Parent \\nChunk\" for retrieval. \\nskills.keywords \\nArray<String> \\nSynonyms and related \\nterms (e.g., Skill: \"AWS\", \\nKeywords:). Enhances \\nrecall for varied user \\nqueries. \\nprojects.emb_text \\nString \\nA \"dense\" description \\nspecifically engineered for \\nembedding, removing stop \\nwords and focusing on \\nsemantic keywords. \\nThis structured approach allows for Metadata Filtering.7 Instead of relying solely on cosine \\nsimilarity, the system can execute a pre-filter step. If a user asks, \"What was your experience \\nat Google?\", the retriever creates a filter company == \"Google\" before performing the vector \\nsearch, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='search, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent \\n \\nChunking—the process of breaking text into manageable pieces for embedding—is the single \\nmost critical hyperparameter in a RAG pipeline. For a technical portfolio, where the \\nconnection between a specific tool (e.g., Redis) and a specific outcome (e.g., caching layer) is \\nvital, standard fixed-size chunking (e.g., every 500 characters) is disastrous. It risks severing'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='the subject from the predicate.9 \\n \\n3.1 Semantic Chunking \\n \\nSemantic chunking moves beyond arbitrary character counts to respect the \"semantic \\nboundaries\" of the text. The algorithm operates by embedding sequential sentences and \\ncalculating the cosine similarity between them. \\nThe mechanism is as follows: \\n1.\\u200b Sentence Splitting: The text is broken into individual sentences using a natural language \\ntokenizer (e.g., NLTK or SpaCy). \\n2.\\u200b Sequential Embedding: Each sentence is embedded using a lightweight model (e.g., \\nall-MiniLM-L6-v2). \\n3.\\u200b Coherence Calculation: The algorithm calculates the similarity score $S$ between \\nSentence $N$ and Sentence $N+1$. \\n4.\\u200b Breakpoint Detection: If $S$ drops below a defined threshold (e.g., 0.7), it indicates a \\n\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point. \\nThis ensures that each retrieved chunk represents a self-contained thought or topic, \\nsignificantly improving the \"Faithfulness\" metric of the generated answers.11 \\n \\n3.2 The Parent-Document Retrieval Pattern \\n \\nTechnical resumes often contain bullet points that are semantically dense but contextually \\nsparse. A bullet point might read: \"Migrated legacy codebase to TypeScript.\" \\nIf this single sentence is retrieved in isolation, the LLM lacks crucial context: Which company \\nwas this for? When did it happen? What was the impact? Embedding the entire project \\ndescription, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='description, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9 \\n●\\u200b Indexing: The system splits the document into small \"Child Chunks\" (individual bullet \\npoints) and embeds them. These are optimized for high-precision matching. \\n●\\u200b Storage: The full \"Parent Document\" (the entire project narrative or work history block) is'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='stored in a separate key-value store (e.g., Redis or the Supabase text column), referenced \\nby a parent_id. \\n●\\u200b Retrieval: The user query matches the dense Child Chunk. The system then uses the \\nparent_id to fetch the full Parent Document. \\n●\\u200b Generation: The LLM receives the full Parent Document as context. \\nThis architectural pattern decouples the retrieval unit (optimized for search) from the \\ngeneration unit (optimized for context), providing the best of both worlds. \\n \\n3.3 Post-Chunking and Late Interaction \\n \\nEmerging research suggests a shift toward \"Post-Chunking\" or Late Interaction models (like \\nColBERT).10 In this paradigm, the entire document is embedded at the token level, and \\ninteraction with the query happens before reducing to a single vector score. While highly \\neffective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='effective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic \\nChunking combined with Parent-Document retrieval offers the optimal trade-off between \\nperformance and complexity. \\n \\n4. The Retrieval Engine: Moving Beyond Cosine \\nSimilarity \\n \\nA \"naive\" RAG system relies exclusively on dense vector search (embeddings). While powerful \\nfor semantic conceptual matching, dense retrieval struggles with precise keyword matching, \\nparticularly with technical acronyms (e.g., \"C#\" vs. \"C++\", \"AWS\" vs. \"GCP\"). To achieve \\nindustry standards, the portfolio must implement a Hybrid Search architecture. \\n \\n4.1 Hybrid Search Architecture \\n \\nHybrid search combines the strengths of two distinct retrieval algorithms: \\n1.\\u200b Dense Retrieval (Vector Search): Utilizes embeddings (e.g., OpenAI \\ntext-embedding-3-small) to understand semantic intent. It excels at queries like \"Tell me'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='about your leadership style\" where exact keyword matches are less important than \\nconceptual alignment. \\n2.\\u200b Sparse Retrieval (Keyword Search): Utilizes algorithms like BM25 (Best Matching 25) or \\nSPLADE to match exact tokens. It excels at technical queries like \"Do you know Redux \\nToolkit?\" where the presence of the specific term is non-negotiable.7 \\n \\n4.1.1 Reciprocal Rank Fusion (RRF) \\n \\nTo combine the results from these two disparate algorithms, we employ Reciprocal Rank \\nFusion (RRF). RRF does not rely on the absolute scores (which are incomparable between \\nCosine Similarity and BM25); instead, it relies on the rank of the document in each list. \\nThe RRF score for a document $d$ is calculated as: \\n \\n \\n$$RRFscore(d) = \\\\sum_{r \\\\in R} \\\\frac{1}{k + r(d)}$$ \\n \\nWhere: \\n●\\u200b $R$ is the set of rank lists (one from Dense, one from Sparse). \\n●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$. \\nThis mathematical fusion ensures that a document appearing in the top 5 results of both \\nsearch methods is prioritized over a document that is #1 in vector search but absent in \\nkeyword search.13 \\n \\n4.2 Vector Database Selection: Supabase vs. Pinecone \\n \\nSelecting the right vector store is a critical infrastructure decision. The market is divided \\nbetween specialized vector databases (Pinecone, Weaviate) and integrated vector extensions \\nfor traditional databases (Supabase/Postgres). \\nTable 2: Vector Database Comparison for Portfolio Use Case \\n \\nFeature \\nSupabase \\nPinecone \\nAnalysis for'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='(pgvector) \\nPortfolio \\nArchitecture \\nIntegrated \\n(Postgres \\nExtension) \\nSpecialized \\nManaged Service \\nSupabase allows \\nstoring relational \\ndata (Projects, \\nUsers) alongside \\nvectors in a single \\nDB, simplifying the \\nstack.15 \\nFree Tier \\n500MB DB Size \\n(Millions of vectors) \\nLimited to 1 index, \\nrestricted \\nthroughput \\nSupabase\\'s 500MB \\nlimit is sufficient for \\nvast amounts of \\ntext data (resumes \\nare KBs). \\nPinecone\\'s free tier \\nis restrictive for \\nmultiple projects.17 \\nHybrid Search \\nNative support \\n(tsvector + \\npgvector) \\nSupported via \\n\"Sparse-Dense\" \\nvectors \\nSupabase allows \\nfusing SQL WHERE \\nclauses with vector \\nsearch naturally. \\nPinecone requires \\nmanaging \\nmetadata \\nseparately.19 \\nLatency \\n~150-200ms (Cold \\nboot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='boot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20 \\nConclusion: For a personal portfolio, Supabase is the superior architectural choice. It \\neliminates the \"data synchronization\" problem—the risk that the metadata in the vector store \\ndrifts from the primary application database.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='4.3 Reranking: The Precision Layer \\n \\nEven with Hybrid Search, the \"Top K\" retrieved documents may include irrelevant noise. To \\nfilter this, we introduce a Reranking step using a Cross-Encoder model (e.g., \\nbge-reranker-v2-m3 or Cohere Rerank). \\nUnlike Bi-Encoders (embedding models) which process the query and document \\nindependently, Cross-Encoders process them simultaneously, outputting a single scalar score \\nindicating relevance. \\n●\\u200b Workflow: Retrieve top 25 documents via Hybrid Search -> Pass to Reranker -> Keep Top \\n5. \\n●\\u200b Impact: This dramatically increases the density of relevant information in the LLM\\'s \\ncontext window, reducing the likelihood of \"Lost in the Middle\" phenomenon where the \\nLLM ignores context buried in the center of the prompt.7 \\n \\n5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to \\ndatabase design, API development, or server management? Direct retrieval on such queries \\noften yields poor results. To address this, we implement Query Transformation strategies. \\n \\n5.1 Hypothetical Document Embeddings (HyDE) \\n \\nHyDE is a technique that leverages the LLM\\'s internal knowledge to bridge the gap between a \\nshort query and a detailed document.1 \\nMechanism: \\n1.\\u200b Prompt: The system prompts an LLM: \"Write a hypothetical resume entry for a senior \\nengineer answering the query: \\'backend experience\\'.\" \\n2.\\u200b Hallucination: The LLM generates a fictional but semantically rich paragraph: \\n\"Experienced in building RESTful APIs using Node.js and Express, managing PostgreSQL \\ndatabases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='databases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector. \\n4.\\u200b Retrieval: The vector search is performed using this hypothetical vector.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Because the hypothetical document shares the same semantic structure (technical \\nvocabulary, sentence structure) as the actual resume chunks, the retrieval accuracy is \\nsignificantly higher than using the raw query \"backend experience.\" \\n \\n5.2 Multi-Query Decomposition \\n \\nFor complex questions like \"Compare your experience with React and Vue,\" a single vector \\nsearch often fails to retrieve sufficient context for both topics. \\nMechanism: \\n1.\\u200b Decomposition: An LLM splits the query into sub-queries:. \\n2.\\u200b Parallel Retrieval: Both queries are executed independently. \\n3.\\u200b Fusion: The unique documents from both retrieval sets are combined (deduplicated). \\n4.\\u200b Generation: The combined context is passed to the LLM to answer the comparison \\nquestion. \\nThis ensures that the system doesn\\'t latch onto one concept (e.g., React) at the expense of \\nthe other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='the other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured \\nrelationships. A vector search for \"Frameworks\" might miss \"Next.js\" if the embedding model \\ndoesn\\'t explicitly map the \"is-a\" relationship. GraphRAG addresses this by modeling the \\nportfolio as a Knowledge Graph (KG).21 \\n \\n6.1 Graph Schema Design \\n \\nThe Knowledge Graph structures the portfolio into Entities and Relationships. \\n●\\u200b Nodes: Person, Skill, Project, Company, Role, Concept. \\n●\\u200b Edges: \\n○\\u200b (Project)-->(Skill) \\n○\\u200b (Skill)-->(Concept)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='○\\u200b (Person)-->(Company) \\n○\\u200b (Project)-->(Outcome) \\nThis structure enables Multi-hop Reasoning. A user might ask: \"Which projects used a \\ncloud-native database?\" \\n●\\u200b Vector Search: Finds \"Cloud-native database\" $\\\\rightarrow$ matches DynamoDB node. \\n●\\u200b Graph Traversal: (DynamoDB)<--(Project A) and (DynamoDB)<--(Project B). \\n●\\u200b Result: The system identifies Project A and Project B, even if the project descriptions \\nnever explicitly used the phrase \"cloud-native database.\" \\n \\n6.2 Implementation: NetworkX vs. Neo4j \\n \\nFor enterprise applications, graph databases like Neo4j are standard. However, for a personal \\nportfolio with a limited dataset (< 1000 nodes), running a dedicated Neo4j instance is often \\noverkill in terms of cost and maintenance. \\nLightweight Alternative: NetworkX \\nWe recommend using NetworkX (a Python library) to build and query the graph in-memory or \\nserialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='serialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the \\nnode-edge list. This is saved as graph.json. \\n2.\\u200b Run Time: The Next.js API route loads graph.json. Simple traversal algorithms (e.g., \\nfinding all neighbors of a matched node) are executed in JavaScript/TypeScript. \\n3.\\u200b Integration: The graph results are textually formatted (e.g., \"Related Skills: X, Y, Z\") and \\nappended to the context window alongside vector results. \\nThis \"Client-Side Graph\" approach delivers 80% of the value of Neo4j with 0% of the \\ninfrastructure cost.25 \\n \\n7. Generative UI: The Interactive 3D Interface \\n \\nThe defining feature of the Agentic Portfolio is Generative UI—the ability of the AI to not just \\ntalk, but to show and control the interface. We utilize the Vercel AI SDK and React Server \\nComponents (RSC) to stream UI elements.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='7.1 Generative UI Architecture \\n \\nIn a standard chatbot, the output is Markdown. In a Generative UI system, the LLM can output \\nReact components. \\n●\\u200b Mechanism: The Vercel AI SDK\\'s streamUI function allows the server to stream \\nrenderable React components to the client.26 \\n●\\u200b Use Case: \\n○\\u200b User: \"Show me your design skills.\" \\n○\\u200b LLM Response: Instead of listing skills, it streams a <SkillCloud category=\"design\" /> \\ncomponent. The client renders this interactive 3D cloud immediately within the chat \\nstream. \\n○\\u200b Fallback: If the client doesn\\'t support the component, it falls back to a text \\ndescription. \\n \\n7.2 Controlling React Three Fiber via Tool Calling \\n \\nThe portfolio features a 3D scene (built with React Three Fiber) that acts as an immersive \\nbackground. The Chat Agent acts as the controller for this scene. \\nState Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"State Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28 \\nImplementation Steps: \\n1.\\u200b Store Definition:\\u200b\\nTypeScript\\u200b\\ninterface State {\\u200b\\n  cameraTarget: Vector3;\\u200b\\n  setCameraTarget: (v: Vector3) => void;\\u200b\\n}\\u200b\\nconst useStore = create<State>((set) => ({... }));\\u200b\\n \\n2.\\u200b Tool Definition: The LLM is provided with a tool moveCamera.\\u200b\\nTypeScript\\u200b\\nconst tools = {\\u200b\\n  moveCamera: tool({\\u200b\\n    description: 'Move the 3D camera to focus on a specific section.',\\u200b\\n    parameters: z.object({ section: z.enum(['home', 'projects', 'about']) }),\\u200b\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='execute: async ({ section }) => {\\u200b\\n       const coords = SECTION_COORDINATES[section];\\u200b\\n       useStore.getState().setCameraTarget(coords);\\u200b\\n       return `Moved camera to ${section}.`;\\u200b\\n    }\\u200b\\n  })\\u200b\\n};\\u200b\\n \\n3.\\u200b Scene Reaction: Inside the R3F Canvas, a component subscribes to the store.\\u200b\\nTypeScript\\u200b\\nfunction CameraController() {\\u200b\\n  const target = useStore((state) => state.cameraTarget);\\u200b\\n  useFrame((state, delta) => {\\u200b\\n    easing.damp3(state.camera.position, target, 0.5, delta); // Smooth interpolation\\u200b\\n  });\\u200b\\n  return null;\\u200b\\n}\\u200b\\n \\nThis architecture creates a seamless \"Magical\" experience where the user\\'s conversation \\nphysically drives the exploration of the 3D space.30 \\n \\n8. Coding Agents and Prompt Engineering \\n \\nDeveloping this complex system requires leveraging \"Coding Agents\" (like Cursor or GitHub \\nCopilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Copilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy \\n \\nTo ensure the coding agent generates code that adheres to the specific architectural \\nconstraints (Next.js 14, Tailwind, Supabase), we place a .cursorrules file in the project root.32 \\nSample Rule Configuration:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='are non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy. \\n \\n9.1 Tier 1: Deterministic Unit Testing (pytest) \\n \\nWe mock the LLM and Vector Store to test the logic surrounding the AI.36 \\n●\\u200b Test Case: Verify that the moveCamera tool is correctly invoked when the input text \\ncontains directional intent. \\n●\\u200b Test Case: Verify that the PDF parser correctly extracts email addresses from a mock \\nresume file.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='●\\u200b Mocking: Use unittest.mock to simulate the Supabase client, ensuring that the retrieval \\nlogic handles empty result sets gracefully without crashing. \\n \\n9.2 Tier 2: LLM-based Evaluation (Ragas & DeepEval) \\n \\nTo evaluate the quality of the AI\\'s answers, we use the Ragas framework (Retrieval \\nAugmented Generation Assessment). Ragas uses an \"LLM-as-a-Judge\" (typically GPT-4) to \\nscore the system on specific metrics.38 \\nKey Metrics: \\n1.\\u200b Context Recall: Does the retrieved context contain the ground truth answer? (Measures \\nRetrieval quality). \\n2.\\u200b Faithfulness: Is the generated answer derived solely from the retrieved context, or did \\nthe model hallucinate? (Measures Generation quality). \\n3.\\u200b Answer Relevance: Does the answer directly address the user\\'s query? \\nCI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='CI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions. \\n●\\u200b Assertion: assert ragas_score[\\'context_recall\\'] > 0.8 \\n●\\u200b Outcome: If a code change (e.g., changing the chunking size) causes the score to drop \\nbelow 0.8, the PR is blocked. This prevents regressions in the agent\\'s intelligence.40 \\n \\n10. Conclusion \\nThe architecture described in this report represents the convergence of modern Full Stack \\nEngineering and Applied AI. By moving from naive text extraction to Vision-based Parsing, \\nfrom simple vector search to Hybrid Graph Retrieval, and from static text output to \\nGenerative 3D Interfaces, we transform the personal portfolio into a sophisticated Agentic \\nSystem. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate\\'s history. But more importantly, the existence of the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"System. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate's history. But more importantly, the existence of the \\nsystem itself serves as the ultimate proof of competence. It demonstrates mastery not just of \\ncode, but of data engineering, system architecture, AI orchestration, and user experience \\ndesign—the defining skills of the next generation of software engineers. The Agentic Portfolio \\nis not just a showcase of work; it is the work itself.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"Works cited \\n1.\\u200b Optimizing RAG. RAG Demystified: A Hands-On Guide to… | by Skanda Vivek | \\nEMAlpha, accessed November 20, 2025, \\nhttps://medium.com/emalpha/optimizing-rag-bd65ebc5e51a \\n2.\\u200b Developing Retrieval Augmented Generation (RAG) based LLM Systems from \\nPDFs: An Experience Report - arXiv, accessed November 20, 2025, \\nhttps://arxiv.org/html/2410.15944v1 \\n3.\\u200b Using Azure OpenAI GPT-4o to extract structured JSON data from PDF \\ndocuments, accessed November 20, 2025, \\nhttps://learn.microsoft.com/en-us/samples/azure-samples/azure-openai-gpt-4-vi\\nsion-pdf-extraction-sample/using-azure-openai-gpt-4o-to-extract-structured-js\\non-data-from-pdf-documents/ \\n4.\\u200b How can I process a pdf using OpenAI's APIs (GPTs)? - Stack Overflow, accessed \\nNovember 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='November 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL \\nquerying, and agents with Amazon Bedrock, accessed November 20, 2025, \\nhttps://aws.amazon.com/blogs/machine-learning/boosting-rag-based-intelligent-\\ndocument-assistants-using-entity-extraction-sql-querying-and-agents-with-am\\nazon-bedrock/ \\n6.\\u200b Conversion of entire PDF into JSON Format - API - OpenAI Developer \\nCommunity, accessed November 20, 2025, \\nhttps://community.openai.com/t/conversion-of-entire-pdf-into-json-format/1066\\n551 \\n7.\\u200b 9 advanced RAG techniques to know & how to implement them - Meilisearch, \\naccessed November 20, 2025, https://www.meilisearch.com/blog/rag-techniques \\n8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025, \\nhttps://www.datasciencecentral.com/best-practices-for-structuring-large-datas\\nets-in-retrieval-augmented-generation-rag/ \\n9.\\u200b Top 13 Advanced RAG Techniques for Your Next Project - Analytics Vidhya, \\naccessed November 20, 2025, \\nhttps://www.analyticsvidhya.com/blog/2025/04/advanced-rag-techniques/ \\n10.\\u200bBeyond Basic Chunking: The Critical Timing Decision in RAG Systems That \\nEveryone Is Getting Wrong, accessed November 20, 2025, \\nhttps://skngrp.medium.com/beyond-basic-chunking-the-critical-timing-decision-\\nin-rag-systems-that-everyone-is-getting-wrong-19febb2ee062 \\n11.\\u200bComparing Chunking Strategies for RAG: From Naive Splits to Striding Windows | \\nby Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='by Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116 \\n12.\\u200bMastering Chunking Strategies for RAG: Best Practices & Code Examples - \\nDatabricks Community, accessed November 20, 2025,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content=\"17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/ \\n19.\\u200bpgvector vs Pinecone: cost and performance - Supabase, accessed November \\n20, 2025, https://supabase.com/blog/pgvector-vs-pinecone \\n20.\\u200bSupabase vs Pinecone: I Migrated My Production AI System and Here's What \\nActually Matters - Dee, accessed November 20, 2025, \\nhttps://deeflect.medium.com/supabase-vs-pinecone-i-migrated-my-production-\\nai-system-and-heres-what-actually-matters-7b2f2ebd59ee \\n21.\\u200bGraphRAG for Devs: Graph-Code Demo Overview - Memgraph, accessed \\nNovember 20, 2025, \\nhttps://memgraph.com/blog/graphrag-for-devs-coding-assistant \\n22.\\u200bThe GraphRAG Manifesto: Adding Knowledge to GenAI - Neo4j, accessed \\nNovember 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025,\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='November 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber \\n29.\\u200bAccessing the Camera in React Three Fiber out of the canvas - Questions, \\naccessed November 20, 2025,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='https://discourse.threejs.org/t/accessing-the-camera-in-react-three-fiber-out-of\\n-the-canvas/39137 \\n30.\\u200bMove camera to face an object in React Three Fiber! - Questions, accessed \\nNovember 20, 2025, \\nhttps://discourse.threejs.org/t/move-camera-to-face-an-object-in-react-three-fi\\nber/81269 \\n31.\\u200bCamera Controls - Wawa Sensei, accessed November 20, 2025, \\nhttps://wawasensei.dev/courses/react-three-fiber/lessons/camera-controls \\n32.\\u200bPatrickJS/awesome-cursorrules: Configuration files that enhance Cursor AI editor \\nexperience with custom rules and behaviors - GitHub, accessed November 20, \\n2025, https://github.com/PatrickJS/awesome-cursorrules \\n33.\\u200bHow Cursor project rules can improve Next.js app development - LogRocket \\nBlog, accessed November 20, 2025, \\nhttps://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025,'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='https://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025, \\nhttps://cursorrules.org/article/nextjs15-react19-vercelai-tailwind-cursorrules-pro\\nmpt-file \\n35.\\u200bThe ultimate .cursorrules for TypeScript, React 19, Next.js 15, Vercel AI SDK, \\nShadcn UI, Radix UI, and Tailwind CSS : r/cursor - Reddit, accessed November 20, \\n2025, \\nhttps://www.reddit.com/r/cursor/comments/1gjd96h/the_ultimate_cursorrules_for\\n_typescript_react_19/ \\n36.\\u200bHow to Properly Mock LangChain LLM Execution in Unit Tests | Python - Medium, \\naccessed November 20, 2025, \\nhttps://medium.com/@matgmc/how-to-properly-mock-langchain-llm-execution-\\nin-unit-tests-python-76efe1b8707e \\n37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-'),\n",
       " Document(metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf', 'file_path': 'pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'Building an Advanced RAG Portfolio', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17, 'source_file': 'Building an Advanced RAG Portfolio.pdf'}, page_content='37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='Master of Science in Artificial Intelligence (Concentration: Machine Learning), GPA – 4.0                                                                                                                               Denton,TX  \\nCoursework : Machine Learning, Deep Learning, NLP, Information Retrieval, Big Data & Data Science, AI Software Development \\n.    \\nBirla Institute of Technology and Science                                                                                                                                                            June 2019 \\nBachelor of Science in Mechanical Engineering with Honours, GPA – 3.7                                                                                                                                                        Dubai, UAE  \\n \\nTECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='TECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='• Cut BA story update effort by 80-90% by deploying a multi-agent pipeline (Fal.ai STT + GPT-4.1 in n8n) that updates Jira Cloud tickets end-to-end \\n(Description + Acceptance Criteria + Status/Assignee) via REST APIs. \\n• Built a Chrome extension (JS/HTML/CSS + REST) to capture voice → transcript → LLM output → structured Jira updates, improving time-to-update \\nfrom ~12 min → ~2 min. \\n• Authored an Acceptance Criteria (AC) Playbook and an NLP variance/coverage harness (Jaccard/F1) to benchmark LLM AC vs. senior BA standards; \\nimproved consistency and completeness of ACs. \\n• Tracked latency/cost per run; instrumented token usage and error categories to guide prompt and model selection decisions. \\nArtificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='Artificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025 \\n• Assessed and provided actionable feedback on peer reviews of key AI research papers, enhancing students’ critical thinking and technical writing across \\nmultiple evaluation cycles. \\n• Led design and experimentation of ML solutions including GANs, transformers, and optimized CNNs for NLP and GenAI tasks. Applied CRISP-DM \\nmethodology and collaborated with academic teams on scalable prototype development for potential deployment.  \\n• Mentored students on applied AI projects, delivering personalized technical guidance that improved project success rates and supported milestone \\nachievement in coursework and capstone deliverables.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='achievement in coursework and capstone deliverables. \\n \\nML/AI Engineer – AI Document Workflow Automation                                                                                            Strawberry Labs, Dubai | Jan.2023 – Jan 2024 \\n• Deployed an OCR + extraction pipeline (Tesseract + CNN post-processing) processing docs in ~20s each, delivering 96% OCR accuracy and 93% \\nfield extraction precision; manual data-entry time −75% (5 min → <1 min). \\n• In controlled tests, achieved 4× faster completion (20s vs 90s) and −70% QA checks, with 96% data correctness vs 85% in manual entry. \\n• Implemented validation rules (format, range, cross-field), exception routing, and confidence scoring to reduce human review and raise straight-through \\nprocessing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='processing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS \\n  \\nConditional GAN for Ethnicity-Based Face Generation                                                                                                                             UNT, Denton, TX | Sep2024 – Dec 2024 \\n• Engineered a generative adversarial network (GAN) from the ground up to create facial images across six ethnicities (4,000 color images each), enabling data-\\ndriven insights into cross-cultural face generation.  \\n• Transitioned from an initial linear-layer design to convolutional layers for more robust feature extraction, improving model fidelity and output quality. \\n• Leveraged AWS to meet high computational (GPU) demands, streamlining large-scale model training and accelerating development cycles. \\nAutomated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-07T20:46:36-04:00', 'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf', 'file_path': 'pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Hemnani, Piyush', 'subject': '', 'keywords': '', 'moddate': '2025-10-07T20:46:36-04:00', 'trapped': '', 'modDate': \"D:20251007204636-04'00'\", 'creationDate': \"D:20251007204636-04'00'\", 'page': 0, 'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'}, page_content='Automated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024 \\n• Developed a pipeline that automatically identifies and classifies product components as “good” or “bad,” reducing manual inspection time and cost. \\n• Implemented advanced edge detection (Sobel, Laplacian of Gaussian) and custom convolution kernels to achieve a 100% detection rate (mIoU=0.8925). \\n• Leveraged both spatial (Probabilistic Hough Transform) and frequency domain (Fourier magnitude) methods to capture subtle texture patterns. \\n• Achieved 77% accuracy, 87% precision, and AUC of 0.85 on real-world data, with future improvements planned via data augmentation and refined \\nlighting/occlusion handling.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Text splitting into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size = 1000, chunk_overlap = 200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Example chunk\n",
    "    if split_docs:\n",
    "        print(\"Example Chunk\")\n",
    "        print(f\"Content: {split_docs[0].page_content}\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "chunked_pdf_documents = split_documents(all_pdf_documents)\n",
    "chunked_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55908ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_pdf_documents[40].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5403c",
   "metadata": {},
   "source": [
    "### Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2db83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1c9f493bcb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    '''\n",
    "    Handles document embedding generation using SentenceTransformer.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained SentenceTransformer model to use.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        ''' Loads the SentenceTransformer model'''\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        '''Generates embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to embed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of embeddings.\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embedding for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embedding with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "# Initializing the embedding manager\n",
    "embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a897b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for 2 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (2, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_trials = [\"This is example 1\", \"This is example 2\"]\n",
    "\n",
    "embeddings_trials = embedding_manager.generate_embeddings(text_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8909ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.03826470e-03,  2.22325660e-02,  1.47959348e-02,  5.55793904e-02,\n",
       "        1.93927288e-02, -2.86618173e-02,  7.33681917e-02, -6.11751620e-03,\n",
       "       -3.03268861e-02, -1.17029417e-02,  4.74827848e-02, -2.76739988e-02,\n",
       "        8.52716342e-02, -6.68997294e-04, -3.39977965e-02,  1.82092097e-02,\n",
       "        3.26722972e-02, -2.81444751e-02, -8.22492093e-02, -1.60418451e-02,\n",
       "        1.11098230e-01, -2.68107168e-02, -2.23269947e-02,  1.21751614e-02,\n",
       "       -3.89853958e-04, -7.64439106e-02,  3.06463195e-03,  8.84773582e-02,\n",
       "        1.40486524e-01, -1.27440825e-01,  1.06240846e-02,  5.18650515e-03,\n",
       "        3.44171971e-02,  3.47072370e-02,  5.72374370e-03,  5.25662415e-02,\n",
       "        1.13464743e-02,  1.06352657e-01, -9.13371816e-02,  6.94956183e-02,\n",
       "       -1.48934859e-03, -7.01464638e-02,  3.17046884e-03, -1.56533532e-02,\n",
       "        6.59741908e-02, -6.73016980e-02,  8.53713416e-03,  2.51697060e-02,\n",
       "       -4.86696512e-02, -1.35161430e-01, -9.16747525e-02, -2.18729172e-02,\n",
       "       -1.38859287e-01,  1.23401331e-02,  2.59330645e-02, -3.34522799e-02,\n",
       "       -5.77293038e-02,  4.64849323e-02,  5.85254887e-03,  1.48342224e-02,\n",
       "        4.99756001e-02, -8.93104449e-03, -5.42309023e-02, -3.13826017e-02,\n",
       "        1.25603497e-01, -5.03558517e-02,  4.77816397e-03,  2.67675314e-02,\n",
       "       -4.73181941e-02,  8.42662826e-02, -7.77666494e-02,  3.72574441e-02,\n",
       "       -5.24177961e-02, -1.37338946e-02, -2.94873379e-02, -2.16076188e-02,\n",
       "       -3.94202583e-02, -1.33037809e-02,  1.05779869e-02, -2.67069824e-02,\n",
       "       -1.25980750e-01, -5.20680472e-02,  1.52092641e-02, -1.26871420e-02,\n",
       "       -5.51957265e-02,  4.82018851e-02, -1.54441949e-02, -7.65924826e-02,\n",
       "        6.62871748e-02,  4.43681097e-03,  2.76670121e-02,  3.74702178e-03,\n",
       "        4.95996922e-02, -3.97596508e-03, -9.93369799e-03,  2.81516258e-02,\n",
       "        1.34484181e-02, -4.58201393e-02, -3.69453281e-02,  1.34771049e-01,\n",
       "       -4.13077213e-02,  6.22363165e-02,  2.49948297e-02,  2.50774678e-02,\n",
       "       -1.77907683e-02, -8.95079523e-02,  2.82076630e-03, -4.74042632e-02,\n",
       "        3.68599780e-02,  1.65777188e-02, -7.03259483e-02, -6.06956752e-03,\n",
       "       -3.95421423e-02, -5.00824349e-03, -6.22257870e-03, -1.06017701e-01,\n",
       "       -2.54036169e-02,  3.58443856e-02, -1.48234107e-02,  5.28468601e-02,\n",
       "        3.85347009e-02,  1.83195826e-02, -5.08540720e-02, -1.60177071e-02,\n",
       "       -3.71383131e-02, -6.55036047e-02, -1.26734599e-02, -2.80039948e-33,\n",
       "        6.63919821e-02, -1.09028950e-01, -8.81158933e-02, -1.28263030e-02,\n",
       "        1.11041211e-01, -4.93050218e-02, -4.98779342e-02,  3.54215540e-02,\n",
       "        1.45865232e-02, -6.49610832e-02,  4.76694629e-02,  4.10349630e-02,\n",
       "        2.64078751e-02,  3.53562608e-02,  3.31399664e-02,  1.93262950e-03,\n",
       "       -2.39072688e-04,  1.54706120e-01, -5.46371564e-02,  1.73276979e-02,\n",
       "        4.05107811e-03,  3.33604366e-02, -1.96644082e-03,  1.45000163e-02,\n",
       "        2.32988764e-02,  5.52361161e-02,  4.27726209e-02,  1.04046641e-02,\n",
       "        4.11507301e-02,  3.32802422e-02,  3.19146784e-03,  8.81791580e-03,\n",
       "       -1.13890722e-01,  3.72794829e-02,  2.45108847e-02,  5.36929891e-02,\n",
       "        7.61173069e-02, -5.75024560e-02, -1.67466421e-03, -6.30363896e-02,\n",
       "       -4.93472517e-02, -3.32095101e-02, -1.94026355e-03,  4.91848495e-03,\n",
       "        8.26517344e-02, -4.25215550e-02, -1.84437111e-02,  4.38259691e-02,\n",
       "        1.99295348e-03,  4.43870686e-02,  7.37480773e-03,  5.58909923e-02,\n",
       "       -8.19001347e-03,  1.48938615e-02,  5.79175875e-02, -3.52482335e-03,\n",
       "        1.65752172e-02,  1.23094376e-02, -3.87936197e-02,  2.12581456e-02,\n",
       "       -2.14991849e-02,  2.97079682e-02, -3.37019265e-02, -4.24641883e-04,\n",
       "       -8.15022290e-02,  4.13977504e-02, -2.53981128e-02, -7.54314736e-02,\n",
       "        9.79387760e-03,  6.40216172e-02, -1.04709696e-02,  7.79836997e-02,\n",
       "       -8.14604387e-02,  3.00616715e-02, -2.51666848e-02, -2.56873481e-02,\n",
       "       -5.89298047e-02, -1.13533307e-02, -8.00041407e-02, -9.90290847e-03,\n",
       "       -4.74637002e-02, -9.23286974e-02, -2.18948256e-03,  9.71690938e-02,\n",
       "       -9.77888610e-03,  4.44700941e-02,  1.06474899e-01, -9.85487327e-02,\n",
       "       -7.70524889e-03, -4.01028991e-02, -8.34395364e-02,  2.50296271e-03,\n",
       "        8.20463896e-02, -1.56722348e-02,  6.76849764e-03,  1.29587829e-33,\n",
       "       -4.25987132e-02,  7.46305883e-02, -1.03496581e-01,  7.09901974e-02,\n",
       "        8.31751749e-02,  5.72428368e-02,  7.38585964e-02, -4.09917161e-02,\n",
       "        3.58231291e-02,  9.75834727e-02, -3.92476469e-02,  2.82021258e-02,\n",
       "       -3.26388925e-02, -2.57156827e-02, -2.94161998e-02, -7.15132952e-02,\n",
       "        9.90233198e-02, -1.19872130e-02,  5.72500238e-03,  8.20166618e-02,\n",
       "       -2.76764277e-02,  9.40457880e-02, -4.70949225e-02,  9.93890464e-02,\n",
       "       -1.02440633e-01,  6.33403212e-02, -5.50759807e-02,  2.32674181e-02,\n",
       "       -5.36913658e-03, -2.28415355e-02,  1.65493693e-02, -2.29860675e-02,\n",
       "        5.46361357e-02, -2.04260834e-02, -4.56198864e-02,  6.43496141e-02,\n",
       "        4.53842431e-02,  3.89323384e-02, -4.20170045e-03,  4.79232753e-04,\n",
       "        4.98789065e-02, -1.63128022e-02,  7.31864646e-02,  9.09626409e-02,\n",
       "       -1.66553371e-02, -4.71620448e-02, -1.41219450e-02,  5.26358001e-03,\n",
       "       -2.46573519e-02,  2.69089136e-02, -1.21962123e-01, -5.34177339e-03,\n",
       "       -9.43465903e-02,  1.63784027e-02, -8.28314573e-02,  3.08960932e-03,\n",
       "        6.44342322e-03,  1.51085621e-03,  2.80628130e-02,  2.99386159e-02,\n",
       "       -1.29397875e-02, -3.95908318e-02, -1.06110992e-02,  2.34505907e-02,\n",
       "        1.14182495e-02, -5.02234288e-02, -6.48946166e-02,  1.97307696e-03,\n",
       "       -2.89090141e-03,  3.77964675e-02,  5.26243001e-02, -5.37881814e-03,\n",
       "       -1.07957028e-01, -1.27434090e-01,  2.84550842e-02, -9.28850938e-03,\n",
       "       -1.55335050e-02, -1.13118123e-02, -5.75309955e-02, -6.76598027e-02,\n",
       "        5.27406204e-03, -1.72412992e-02, -5.55601995e-03,  3.61110233e-02,\n",
       "        1.35667836e-02,  3.85849215e-02,  3.90360989e-02, -4.14373837e-02,\n",
       "       -8.81708134e-03, -2.00044294e-03, -1.66951818e-03,  3.64336669e-02,\n",
       "        4.11987752e-02,  4.05567661e-02,  2.01658136e-03, -1.92158627e-08,\n",
       "       -4.29787152e-02,  1.98615734e-02,  7.82437176e-02, -1.87999867e-02,\n",
       "        6.47217557e-02,  2.59374958e-02,  2.78068297e-02,  9.14401747e-03,\n",
       "       -2.99965329e-02, -5.62813040e-03, -4.68847575e-03,  6.13934398e-02,\n",
       "        2.18968745e-03,  5.75601449e-03,  2.72776354e-02,  8.90700892e-02,\n",
       "       -9.26909316e-03, -2.71039177e-02, -3.02528758e-02, -3.63686238e-04,\n",
       "       -8.08116272e-02,  2.31744535e-02, -5.02368156e-03, -3.03553008e-02,\n",
       "        1.73135046e-02, -7.22131983e-04, -4.21658568e-02,  1.14622518e-01,\n",
       "        8.19588732e-03,  1.42654683e-02,  2.13178080e-02,  1.40068471e-01,\n",
       "        2.65556071e-02, -7.19913095e-03,  3.59423347e-02, -1.29500227e-02,\n",
       "       -1.91879403e-02, -2.53085932e-03,  3.15288976e-02, -4.59158346e-02,\n",
       "        5.00855930e-02, -1.45009719e-02, -2.42218096e-02,  2.27155611e-02,\n",
       "        5.53201437e-02,  6.17061853e-02, -2.60180552e-02, -8.85045677e-02,\n",
       "       -7.55289868e-02, -5.45522161e-02, -9.41037461e-02,  6.05216026e-02,\n",
       "        1.16640395e-02,  7.59334769e-03,  4.38008755e-02,  6.71608523e-02,\n",
       "        1.18983397e-02,  1.39058726e-02,  1.35199120e-02,  5.56969531e-02,\n",
       "        3.09518818e-02,  8.27031434e-02,  5.45060597e-02,  5.55219222e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e04d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (1, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_trial2 = [\"This is example 1\"]\n",
    "\n",
    "embeddings_trials = embedding_manager.generate_embeddings(text_trial2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1914bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.03826470e-03  2.22325660e-02  1.47959348e-02  5.55793904e-02\n",
      "  1.93927288e-02 -2.86618173e-02  7.33681917e-02 -6.11751620e-03\n",
      " -3.03268861e-02 -1.17029417e-02  4.74827848e-02 -2.76739988e-02\n",
      "  8.52716342e-02 -6.68997294e-04 -3.39977965e-02  1.82092097e-02\n",
      "  3.26722972e-02 -2.81444751e-02 -8.22492093e-02 -1.60418451e-02\n",
      "  1.11098230e-01 -2.68107168e-02 -2.23269947e-02  1.21751614e-02\n",
      " -3.89853958e-04 -7.64439106e-02  3.06463195e-03  8.84773582e-02\n",
      "  1.40486524e-01 -1.27440825e-01  1.06240846e-02  5.18650515e-03\n",
      "  3.44171971e-02  3.47072370e-02  5.72374370e-03  5.25662415e-02\n",
      "  1.13464743e-02  1.06352657e-01 -9.13371816e-02  6.94956183e-02\n",
      " -1.48934859e-03 -7.01464638e-02  3.17046884e-03 -1.56533532e-02\n",
      "  6.59741908e-02 -6.73016980e-02  8.53713416e-03  2.51697060e-02\n",
      " -4.86696512e-02 -1.35161430e-01 -9.16747525e-02 -2.18729172e-02\n",
      " -1.38859287e-01  1.23401331e-02  2.59330645e-02 -3.34522799e-02\n",
      " -5.77293038e-02  4.64849323e-02  5.85254887e-03  1.48342224e-02\n",
      "  4.99756001e-02 -8.93104449e-03 -5.42309023e-02 -3.13826017e-02\n",
      "  1.25603497e-01 -5.03558517e-02  4.77816397e-03  2.67675314e-02\n",
      " -4.73181941e-02  8.42662826e-02 -7.77666494e-02  3.72574441e-02\n",
      " -5.24177961e-02 -1.37338946e-02 -2.94873379e-02 -2.16076188e-02\n",
      " -3.94202583e-02 -1.33037809e-02  1.05779869e-02 -2.67069824e-02\n",
      " -1.25980750e-01 -5.20680472e-02  1.52092641e-02 -1.26871420e-02\n",
      " -5.51957265e-02  4.82018851e-02 -1.54441949e-02 -7.65924826e-02\n",
      "  6.62871748e-02  4.43681097e-03  2.76670121e-02  3.74702178e-03\n",
      "  4.95996922e-02 -3.97596508e-03 -9.93369799e-03  2.81516258e-02\n",
      "  1.34484181e-02 -4.58201393e-02 -3.69453281e-02  1.34771049e-01\n",
      " -4.13077213e-02  6.22363165e-02  2.49948297e-02  2.50774678e-02\n",
      " -1.77907683e-02 -8.95079523e-02  2.82076630e-03 -4.74042632e-02\n",
      "  3.68599780e-02  1.65777188e-02 -7.03259483e-02 -6.06956752e-03\n",
      " -3.95421423e-02 -5.00824349e-03 -6.22257870e-03 -1.06017701e-01\n",
      " -2.54036169e-02  3.58443856e-02 -1.48234107e-02  5.28468601e-02\n",
      "  3.85347009e-02  1.83195826e-02 -5.08540720e-02 -1.60177071e-02\n",
      " -3.71383131e-02 -6.55036047e-02 -1.26734599e-02 -2.80039948e-33\n",
      "  6.63919821e-02 -1.09028950e-01 -8.81158933e-02 -1.28263030e-02\n",
      "  1.11041211e-01 -4.93050218e-02 -4.98779342e-02  3.54215540e-02\n",
      "  1.45865232e-02 -6.49610832e-02  4.76694629e-02  4.10349630e-02\n",
      "  2.64078751e-02  3.53562608e-02  3.31399664e-02  1.93262950e-03\n",
      " -2.39072688e-04  1.54706120e-01 -5.46371564e-02  1.73276979e-02\n",
      "  4.05107811e-03  3.33604366e-02 -1.96644082e-03  1.45000163e-02\n",
      "  2.32988764e-02  5.52361161e-02  4.27726209e-02  1.04046641e-02\n",
      "  4.11507301e-02  3.32802422e-02  3.19146784e-03  8.81791580e-03\n",
      " -1.13890722e-01  3.72794829e-02  2.45108847e-02  5.36929891e-02\n",
      "  7.61173069e-02 -5.75024560e-02 -1.67466421e-03 -6.30363896e-02\n",
      " -4.93472517e-02 -3.32095101e-02 -1.94026355e-03  4.91848495e-03\n",
      "  8.26517344e-02 -4.25215550e-02 -1.84437111e-02  4.38259691e-02\n",
      "  1.99295348e-03  4.43870686e-02  7.37480773e-03  5.58909923e-02\n",
      " -8.19001347e-03  1.48938615e-02  5.79175875e-02 -3.52482335e-03\n",
      "  1.65752172e-02  1.23094376e-02 -3.87936197e-02  2.12581456e-02\n",
      " -2.14991849e-02  2.97079682e-02 -3.37019265e-02 -4.24641883e-04\n",
      " -8.15022290e-02  4.13977504e-02 -2.53981128e-02 -7.54314736e-02\n",
      "  9.79387760e-03  6.40216172e-02 -1.04709696e-02  7.79836997e-02\n",
      " -8.14604387e-02  3.00616715e-02 -2.51666848e-02 -2.56873481e-02\n",
      " -5.89298047e-02 -1.13533307e-02 -8.00041407e-02 -9.90290847e-03\n",
      " -4.74637002e-02 -9.23286974e-02 -2.18948256e-03  9.71690938e-02\n",
      " -9.77888610e-03  4.44700941e-02  1.06474899e-01 -9.85487327e-02\n",
      " -7.70524889e-03 -4.01028991e-02 -8.34395364e-02  2.50296271e-03\n",
      "  8.20463896e-02 -1.56722348e-02  6.76849764e-03  1.29587829e-33\n",
      " -4.25987132e-02  7.46305883e-02 -1.03496581e-01  7.09901974e-02\n",
      "  8.31751749e-02  5.72428368e-02  7.38585964e-02 -4.09917161e-02\n",
      "  3.58231291e-02  9.75834727e-02 -3.92476469e-02  2.82021258e-02\n",
      " -3.26388925e-02 -2.57156827e-02 -2.94161998e-02 -7.15132952e-02\n",
      "  9.90233198e-02 -1.19872130e-02  5.72500238e-03  8.20166618e-02\n",
      " -2.76764277e-02  9.40457880e-02 -4.70949225e-02  9.93890464e-02\n",
      " -1.02440633e-01  6.33403212e-02 -5.50759807e-02  2.32674181e-02\n",
      " -5.36913658e-03 -2.28415355e-02  1.65493693e-02 -2.29860675e-02\n",
      "  5.46361357e-02 -2.04260834e-02 -4.56198864e-02  6.43496141e-02\n",
      "  4.53842431e-02  3.89323384e-02 -4.20170045e-03  4.79232753e-04\n",
      "  4.98789065e-02 -1.63128022e-02  7.31864646e-02  9.09626409e-02\n",
      " -1.66553371e-02 -4.71620448e-02 -1.41219450e-02  5.26358001e-03\n",
      " -2.46573519e-02  2.69089136e-02 -1.21962123e-01 -5.34177339e-03\n",
      " -9.43465903e-02  1.63784027e-02 -8.28314573e-02  3.08960932e-03\n",
      "  6.44342322e-03  1.51085621e-03  2.80628130e-02  2.99386159e-02\n",
      " -1.29397875e-02 -3.95908318e-02 -1.06110992e-02  2.34505907e-02\n",
      "  1.14182495e-02 -5.02234288e-02 -6.48946166e-02  1.97307696e-03\n",
      " -2.89090141e-03  3.77964675e-02  5.26243001e-02 -5.37881814e-03\n",
      " -1.07957028e-01 -1.27434090e-01  2.84550842e-02 -9.28850938e-03\n",
      " -1.55335050e-02 -1.13118123e-02 -5.75309955e-02 -6.76598027e-02\n",
      "  5.27406204e-03 -1.72412992e-02 -5.55601995e-03  3.61110233e-02\n",
      "  1.35667836e-02  3.85849215e-02  3.90360989e-02 -4.14373837e-02\n",
      " -8.81708134e-03 -2.00044294e-03 -1.66951818e-03  3.64336669e-02\n",
      "  4.11987752e-02  4.05567661e-02  2.01658136e-03 -1.92158627e-08\n",
      " -4.29787152e-02  1.98615734e-02  7.82437176e-02 -1.87999867e-02\n",
      "  6.47217557e-02  2.59374958e-02  2.78068297e-02  9.14401747e-03\n",
      " -2.99965329e-02 -5.62813040e-03 -4.68847575e-03  6.13934398e-02\n",
      "  2.18968745e-03  5.75601449e-03  2.72776354e-02  8.90700892e-02\n",
      " -9.26909316e-03 -2.71039177e-02 -3.02528758e-02 -3.63686238e-04\n",
      " -8.08116272e-02  2.31744535e-02 -5.02368156e-03 -3.03553008e-02\n",
      "  1.73135046e-02 -7.22131983e-04 -4.21658568e-02  1.14622518e-01\n",
      "  8.19588732e-03  1.42654683e-02  2.13178080e-02  1.40068471e-01\n",
      "  2.65556071e-02 -7.19913095e-03  3.59423347e-02 -1.29500227e-02\n",
      " -1.91879403e-02 -2.53085932e-03  3.15288976e-02 -4.59158346e-02\n",
      "  5.00855930e-02 -1.45009719e-02 -2.42218096e-02  2.27155611e-02\n",
      "  5.53201437e-02  6.17061853e-02 -2.60180552e-02 -8.85045677e-02\n",
      " -7.55289868e-02 -5.45522161e-02 -9.41037461e-02  6.05216026e-02\n",
      "  1.16640395e-02  7.59334769e-03  4.38008755e-02  6.71608523e-02\n",
      "  1.18983397e-02  1.39058726e-02  1.35199120e-02  5.56969531e-02\n",
      "  3.09518818e-02  8.27031434e-02  5.45060597e-02  5.55219222e-03]\n",
      "--------------------------------------\n",
      "(384,)\n",
      "--------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "--------------------------------------\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_trials)\n",
    "print(\"--------------------------------------\")\n",
    "print(embeddings_trials.shape)\n",
    "print(\"--------------------------------------\")\n",
    "print(type(embeddings_trials))\n",
    "# print(\"--------------------------------------\")\n",
    "# print(embeddings_trials.tolist())\n",
    "print(\"--------------------------------------\")    \n",
    "embeddings_trials = embeddings_trials.tolist()\n",
    "print(type(embeddings_trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1536dfb",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfbce5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialized with collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1c9f4c87380>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    ''' \n",
    "    Manages the embedding in a ChromaDB Vector Store.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, collection_name: str = 'pdf_documents', persistent_directory: str = \"../data/vector_store\"):\n",
    "        '''\n",
    "        Initialize the Vector Store\n",
    "        \n",
    "        Args:\n",
    "            Collection _name (str): Name of chromaDB collection\n",
    "            persistent_directory (str): Directory to persist the vector store\n",
    "        '''\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "\n",
    "    def initialize_store(self):\n",
    "        ''' \n",
    "        Initialize ChromaDB client and collection\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persistent_directory)\n",
    "\n",
    "            # Create or get collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata= {\"description\": \"PDF Document Embeddings\",\n",
    "                           \"hnsw:space\": \"cosine\"   }\n",
    "            )\n",
    "            print(f\"Vector Store initialized with collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Vector Store: {e}\")\n",
    "            raise \n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        ''' \n",
    "        Add documents and their embedding to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Any]): List of documents to add\n",
    "            embeddings (np.ndarray): Corresponding embeddings\n",
    "        '''\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print (f\"Adding {len(documents)} documents to the vector store\")\n",
    "\n",
    "        #Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_texts = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_idx'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document text\n",
    "            document_texts.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids =ids,\n",
    "                embeddings= embeddings_list,\n",
    "                metadatas= metadatas,\n",
    "                documents= document_texts\n",
    "            )\n",
    "\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(f\"Total documents in collection now: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "# Initialize Vector Store\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17da595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d8b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for 98 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (98, 384)\n",
      "Adding 98 documents to the vector store\n",
      "Successfully added 98 documents to the vector store.\n",
      "Total documents in collection now: 98\n"
     ]
    }
   ],
   "source": [
    "### Converting teh chunked texts into embeddings\n",
    "\n",
    "texts = [doc.page_content for doc in chunked_pdf_documents]\n",
    "\n",
    "### Generate teh embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "### Store in Vector Store\n",
    "\n",
    "vector_store.add_documents(chunked_pdf_documents, embeddings)\n",
    "\n",
    "### Minor issues with current setup:\n",
    "# Will it realize duplicates?\n",
    "# No. Chroma only cares about id. With random UUIDs, every run looks “new”.\n",
    "\n",
    "# Will it store duplicates with new ids?\n",
    "# Yes, exactly that.\n",
    "\n",
    "# Is the current code capable of handling redundancies?\n",
    "# No. It always inserts new rows. To handle redundancies you need deterministic IDs + upsert() or a clear-then-rebuild strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "439cd74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a',\n",
       " 'By contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.',\n",
       " 'automatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.',\n",
       " 'infrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n•',\n",
       " '(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1',\n",
       " 'Start thinking about the content that might go into this personal knowledge base. For a portfolio Q&A\\nchatbot about you, the data could include things like:\\nA written bio or introduction: e.g. your background, education, skills, projects, interests.\\nResume or CV data: your work experience, accomplishments, maybe in a structured Q&A form\\n(“Where did I work in 2022?”, “What projects have I done in machine learning?”, etc.).\\nFrequently asked questions about you: This could be a list of questions and answers (FAQ) that you\\nanticipate someone might ask. For example: “What are your expertise areas?”, “What was your MSc\\nthesis about?”, “What hobbies do you have?” – along with the answers in your own words.\\nAny personal blog posts or writings: if relevant, these can provide context on your opinions or\\nknowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.',\n",
       " 'knowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.\\nSince you want the chatbot to answer based only on what “it knows” (i.e. your provided data, with no live\\ninternet connection), we will either be training a model on this data or feeding this data into a retrieval\\nsystem for the model. In either case, you’ll need to gather and curate the information about yourself. This\\ncan be done incrementally: once we decide on the approach, you can compile the data into the needed\\nformat (documents, Q&A pairs, etc.). \\nIf we go with a fine-tuning approach: you may need to format the data as a training dataset (for example, a\\nlist of prompt-response pairs where the prompt is a question about you and the response is the correct\\nanswer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –',\n",
       " 'answer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –\\nbut you do need enough coverage of facts about you so the model can learn them. Ensure the info is\\naccurate and expressed in the tone you want the answers to have.\\nIf we go with a retrieval-based approach: you might store the data as a set of documents or text passages.\\nThese could be chunks of a “About Me” document, or individual Q&A entries, etc. The quality of answers will\\ndepend on providing sufficient detail in these source texts. The nice thing is that you can start with a basic\\nset of documents (like a few paragraphs about you, plus a list of Q&A) and always update or expand it later\\nwithout retraining a model – we’ll discuss this more under the retrieval approach.\\nNow, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)',\n",
       " \"Now, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\",\n",
       " \"example, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\\nName] is an AI engineer with a focus on NLP and LLMs, currently working on...”) along with many other\\n• \\n• \\n• \\n• \\n• \\n2\",\n",
       " 'Q&As covering your background, skills, projects, etc. The fine-tuning process will adjust the model’s weights\\nso that it “learns” these specific facts and can respond in a conversational style about them.\\nThanks to techniques like Low-Rank Adaptation (LoRA), it’s feasible to fine-tune moderately large models\\non a single GPU. In fact, LoRA has been shown to allow fine-tuning a 7-billion-parameter model (such as\\nLLaMA-2 7B) on a single GPU\\n. One report notes: “LoRA allows us to finetune 7B parameter LLMs on a single\\nGPU. In [one case], using QLoRA (quantized LoRA) with optimal settings required ~17.8 GB GPU memory and about\\n3 hours on an A100 GPU for 50k training examples”\\n. This means that if you have access to a GPU with\\n~24GB VRAM (or use a cloud service), you could potentially fine-tune a model on a custom dataset without\\nhuge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training',\n",
       " 'huge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training\\nwould be faster (possibly an hour or two, depending on the model and hyperparameters).\\nState-of-the-art techniques you might consider for this include LoRA/QLoRA (to reduce memory and\\ncompute), and using an instruction-tuned base model. For example, starting with an instruction-following\\nmodel (like LLaMA-2-chat or Dolly, etc.) might yield better conversational answers after fine-tuning. You\\ncould also experiment with lightweight fine-tuning vs full fine-tuning. LoRA is nice because it keeps the\\noriginal model intact and just learns small adapter weights – this is efficient and you can revert to the base\\nmodel easily if needed.\\nBefore committing to fine-tuning, consider the pros and cons:\\nPros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).',\n",
       " 'Pros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).\\nIt can be run locally or on your server without external API calls, preserving privacy (important if\\nsome personal data is sensitive).\\nAs an AI engineer, you get the learning experience of doing a fine-tune with SOTA methods. You can\\nexperiment with parameters, try new fine-tuning optimizations, etc., which can be valuable\\nexperience.\\nThe inference might be slightly faster per query than a retrieval approach for small queries, since it’s\\njust the model response (no vector database lookup overhead) – though in practice the difference\\nmay be small.\\nCons of fine-tuning approach:\\nData requirements: Fine-tuning is effectively training, so if you have very little data about yourself,\\nthe model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-',\n",
       " 'the model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-\\ntune with too few examples or only extremely narrow phrasing. You might need to be creative in\\ngenerating enough Q&A pairs (possibly augmenting with rephrased questions) so the model sees\\nsufficient variety. As one discussion put it: don’t try to fine-tune when you have too little data or just to\\ninject a few facts – that’s what prompt context or retrieval is for; “Don’t use a bulldozer to kill a fly.”\\n.\\nFine-tuning a model to learn new knowledge (like personal facts it didn’t know) is possible but\\nworks best if you provide a reasonably sized corpus of that knowledge\\n.\\nMaintenance and updates: If your personal information changes or you want to add new data (say\\nyou start a new job or complete new projects), you’d have to fine-tune a new version of the model or\\nat least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6',\n",
       " 'at least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6\\n7\\n• \\n3',\n",
       " 'trained, the model’s knowledge is fixed. As one source notes, fine-tuning is “powerful on paper, but\\nexpensive, time-consuming, and a nightmare to maintain every time your data changes”\\n.\\nCompute and cost: While a small-scale fine-tune is much cheaper than training from scratch, it’s not\\nfree. You need suitable hardware. If using cloud GPUs, that could cost some money (though a single\\n1-3 hour run on an A100 or similar might be on the order of tens of dollars, which isn’t too bad). Still,\\nif you iterate multiple times, it adds up. Also hosting the final model (for inference) means you need\\na server (or at least something like a GPU or CPU instance) to run the model continuously for your\\nwebsite. A 7B model can run on CPU but might be slow; more likely you’d run on a GPU for snappier\\nresponses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger',\n",
       " 'responses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger\\nbase model. Open-source models are improving rapidly, but something like a fine-tuned 7B or 13B\\nparameter model will be less fluent and sometimes less accurate than, say, GPT-4. It might also\\nhallucinate answers if asked something outside of what it was trained on (or even confuse facts if the\\nprompt is tricky). Notably, fine-tuning doesn’t inherently fix the hallucination problem of LLMs\\n.\\nThe model might still “make up” an answer if asked a question it doesn’t know and you haven’t built\\nin a mechanism to handle that (like a “I don’t know” response).\\nExpertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s',\n",
       " 'Expertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s\\ndefinitely doable (especially since you’re an AI engineer), but it’s more involved than the retrieval\\napproach. One write-up on private knowledge chatbots explicitly pointed out that to fine-tune an\\nopen-source model for a knowledge base, you need “specialized talent and a large amount of time to\\nsolve the fine tuning challenge internally”\\n. In other words, be prepared for some experimentation\\nand debugging.\\nIn summary, training a personal mini-LLM using fine-tuning is feasible and would be our first choice to\\nexplore if you’re keen on it. It gives you a self-contained model that can answer questions about you.\\nHowever, be mindful that this approach is best if you have a decent amount of personal data to teach the',\n",
       " 'However, be mindful that this approach is best if you have a decent amount of personal data to teach the\\nmodel and if you’re ready to handle updates via retraining. Given the constraints (limited data and budget),\\nwe should compare this with Approach 2, which might be more efficient for the task.\\nApproach 2: Retrieval-Augmented Q&A (Using a Context File or\\nVector DB)\\nThe alternative (and increasingly common) approach is to avoid training altogether and instead use a pre-\\ntrained model with a retrieval mechanism. This is often called Retrieval-Augmented Generation (RAG). In\\nthis setup, you  provide the model with relevant information at query time by retrieving it from a\\nknowledge base of your documents. The model then generates an answer using that information as\\ncontext.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal',\n",
       " 'context.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal\\ndata (the same kind of data we discussed: your bio, Q&A pairs, etc.). When a user asks a question on your\\nportfolio site, the system will pull out the parts of that data that are most relevant to the question and feed\\nthose, along with the question, into the prompt for the LLM. The LLM (which could be a large pre-trained\\nmodel like GPT-3.5, GPT-4, or an open-source model you host) then answers based only on that provided\\ncontext.\\n8\\n• \\n• \\n9\\n• \\n9\\n4',\n",
       " 'Example architecture of a retrieval-augmented Q&A system. A user’s query is first used to fetch relevant knowledge\\n(documents or data) from a store (vector database or search index), and that knowledge is combined with the\\nquery as input to the LLM. The LLM then produces an answer grounded in the provided data\\n.\\nThis approach has several advantages for your use case: - No model training needed: The heavy lifting\\nhas been done by the base model. You don’t adjust its weights; you just give it information. This avoids the\\nexpense and complexity of fine-tuning. As one source explains, you “keep the model as it is and simply plug it\\ninto your knowledge sources… the model retrieves the latest info at runtime, answers in context, and stays\\naccurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the',\n",
       " 'accurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the\\ndocuments or knowledge base. The next question asked will then retrieve from the new data immediately.\\nNo need to retrain a model or deploy new weights. This makes the system much easier to keep up-to-date\\n(no “nightmare to maintain every time your data changes” as with constant fine-tuning\\n). -  Smaller\\ndeployment footprint: Instead of hosting a custom model, you could use an API (like OpenAI’s) or host a\\nsmaller model just for inference. The retrieval step might need a vector database, but there are lightweight\\noptions (even in-memory). Many real-world chatbot systems use this pattern because it scales well – you\\ncan  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and',\n",
       " 'can  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and\\ngrounding: Because the answers are drawn from provided text, you reduce the chance of the model\\nhallucinating incorrect facts about you. Essentially, the model is forced to base its answer on the snippets of\\ntext you supply. (Of course, the model could still do a poor job or hallucinate connections between facts, but\\nif prompted to only use the given info, it tends to stick to it). A Reddit user summarizing best practices\\nnoted that “Prompt design matters as much as retrieval. Instruct the model to stick to provided excerpts… This\\nreduces hallucinations and builds trust with users.”\\n. You can even have the model cite the sources (in a\\nmore advanced implementation), which is common in RAG setups for enterprise knowledge bases. - Speed\\nand cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper',\n",
       " 'and cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper\\nand faster. You’re not paying the cost of training. At query time, vector search is usually fast (milliseconds to\\na few tens of ms), and then you call the model to generate an answer. If using an API like OpenAI, you pay\\nper call (plus maybe a small cost for vector DB if using a cloud one). If using an open source model, the\\ncompute to run it is similar to if it were fine-tuned. Many have concluded that “RAG has become the go-to\\nchoice for many use cases: it’s faster, cheaper, and way more practical for real-world teams”\\n.\\nHow it can be implemented: The general flow is: 1. Indexing your personal data: Take your documents\\nor Q&A pairs about yourself and break them into chunks (e.g. paragraphs or individual Q&A entries). For\\neach chunk, generate an  embedding vector (a numeric representation of the text meaning) using an\\n10\\n8\\n8\\n11\\n12\\n5',\n",
       " 'embedding model\\n. Store these vectors in a vector database (or use a simple in-memory vector index if\\nthe data is small) along with the chunk text. There are many tools for this; libraries like  LangChain or\\nLlamaIndex can automate a lot of it, and vector DBs like Pinecone, Weaviate, or open-source ones like FAISS\\nor Qdrant can store the data\\n. A Reddit comment succinctly described this: “chunk your docs, embed them,\\nand store in a vector database. At query time, retrieve the most relevant chunks and pass them into the\\nLLM...”\\n. 2. Retrieval on query: When a question is asked (“How many years of experience does [Your\\nName] have in AI?” for example), the system creates an embedding for the query and searches for similar\\nvectors in your vector DB. It might return, say, the top 3 chunks that are most relevant – maybe one of those\\nchunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct',\n",
       " 'chunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct\\nprompt with context: The retrieved text chunks are then combined with the user’s question to form the\\nprompt for the LLM. For instance, the prompt might be something like:  “Context: [excerpt from your bio\\nsaying you have 5 years in AI]. Q: How many years of experience do I have in AI? A:” – and the model will\\nhopefully respond: “You have 5 years of experience in AI.” 4.  Model answer: The LLM (which could be\\nrunning via an API or a local model) generates an answer using the context. If all goes well, the answer will\\nbe correct, because the needed info was in the provided context. We can also instruct the model with a\\nsystem/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component',\n",
       " 'system/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component\\n(in that diagram, “Azure AI Search” plays the role of vector DB/search engine) which returns knowledge, and\\nthat knowledge + query go into the LLM to get a final answer\\n. Notably, this approach requires no fine-\\ntuning of the LLM’s weights – the model remains as-is (pretrained on general data), and we just augment\\nits input with relevant data at runtime\\n.\\nTools and options: Since you’re an AI engineer, you might enjoy building the pieces yourself, or you can use\\nexisting frameworks: - LangChain and LlamaIndex (formerly GPT Index) are high-level libraries that let you set\\nup a QA chain over your documents very quickly. They handle splitting text, embedding (you can choose\\nmodels like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow',\n",
       " 'models like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow\\n. For example, LangChain has a RetrievalQA  chain that does exactly this once you provide it a vector\\nstore and an LLM. These libraries also help with prompt management. - Vector database choices: If you\\nprefer not to rely on an external cloud service, you can use an open-source solution. FAISS (by Facebook)\\ncan run in-memory or on disk and is often used for small to medium cases. For larger scale or convenience,\\nservices like Pinecone or Weaviate can host it (though for a personal portfolio, that’s probably overkill).\\nThere are lightweight options like an SQLite + embeddings or even just computing cosine similarity on the\\nfly for small data. Given your data will be relatively small (maybe a few pages of text in total), even a simple\\napproach would work. The key is the concept, not the specific tech. - Model choice for answering: You',\n",
       " 'approach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to',\n",
       " 'capable at following instructions. - No live data needed: As you specified, this system does not need to\\nfetch real-time info from the internet. All knowledge is static in your provided data. That’s perfectly in line\\nwith RAG – the knowledge base is whatever you load into the vector store. It won’t go out and search\\nbeyond that. If a question is asked that isn’t answerable from your data, the ideal behavior is to say “I don’t\\n13\\n14\\n15\\n10\\n16\\n17\\n18\\n6',\n",
       " 'know” or some graceful fallback. You can program the prompt to encourage that (e.g., “If the answer is not\\nin the provided context, say you don’t know.”).\\nPotential downsides of retrieval approach: - It is a bit more moving parts: you have to set up an\\nembedding process and store. However, for a one-person project, this is fairly straightforward and many\\ntutorials exist. It’s arguably less work than doing a fine-tune from scratch. - At query time, the model’s\\nresponse is limited by what it sees in the context. If your context window (the prompt length) of the model\\nis, say, 4,000 tokens, and your entire personal knowledge base is larger than that, the retrieval step must be\\neffective at picking the right pieces of info. If it misses something relevant, the answer might be incomplete.\\nBut since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is',\n",
       " 'But since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is\\nmanageable. - If not properly instructed, the model could ignore the context and hallucinate. But in practice,\\nif you supply a relevant context chunk, models like GPT-3.5 or LLaMA-2-chat will use it when answering. -\\nOne consideration: if you want the Q&A to have some memory or multi-turn conversation about you, you’d\\nhave to include previous Q&A in context as well. But since it’s mostly fact-based about you, each question\\ncan probably be handled independently (stateless Q&A).\\nGiven the above, the retrieval-based approach is quite appealing for simplicity and robustness. It’s generally\\nthe  preferred method in industry for Q&A bots on custom data because of the maintenance and\\naccuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed',\n",
       " 'accuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed\\nthe model to deeply internalize a style or do complex transformations, or if retrieval latency was a big\\nissue).\\nComparison and Recommendation\\nBoth approaches can ultimately achieve your goal: a chatbot that answers questions about you, without\\nhooking into live external sources. The best choice depends on your priorities (learning experience vs.\\nsimplicity, one-time effort vs. ongoing flexibility).\\nTraining a Personal LLM might be your first inclination as an AI engineer because it’s an interesting\\nproject. It will let you experiment with the latest fine-tuning techniques (LoRA, QLoRA, etc.) and truly “own”\\nthe model that results. If you go this route, try to leverage existing models and do a relatively lightweight\\nfine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat',\n",
       " 'fine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat\\n(for instance, LLaMA-2 13B Chat has good performance). Use LoRA to fine-tune it on a curated set of Q&A\\nabout you. Monitor for overfitting – since your dataset might be small, you might only do one or two epochs\\nover it\\n. It’s even possible that just a few hundred training steps could suffice if using a high-quality base\\nmodel. - Ensure your fine-tuning dataset is high quality and diverse within the realm of “about you.” If\\nthere are specific phrasings or tricky factual questions (like dates, spellings of names, etc.), include those.\\nThe model will memorize those facts. Be cautious: the model might generalize in unintended ways (you\\nwouldn’t want it to start answering beyond your data and being wrong). - After training, you’ll need to\\ndeploy the model. For a portfolio site, you might run the model on a server that the site can send requests',\n",
       " 'deploy the model. For a portfolio site, you might run the model on a server that the site can send requests\\nto. Running a 7B model with int8 quantization on CPU is possible but may be slow (several seconds per\\nanswer). Running on a GPU (even a cheap one) or using a model served via an API (like HuggingFace\\nInference Endpoint or similar) could be better for snappy responses.\\nUsing Retrieval (RAG) is, in contrast, more of a software engineering solution than a model-training\\nsolution. My recommendation is to strongly consider this approach, because it aligns well with having\\nlimited data and budget but needing accurate results. Here’s how you might implement it step by step: 1.\\n19\\n7',\n",
       " 'Begin with an open-source model or an API. For example, try OpenAI’s GPT-3.5 Turbo on some manually\\ncrafted prompts using your data (even before setting up any vector DB) to see how it performs when given\\ncontext. This costs very little and gives a baseline. You can later swap to an open model if you want to self-\\nhost. 2. Use a library like LangChain to index your personal info. You could literally have a Python script\\nwhere you input a bunch of strings (your bio, some Q&As) and it uses an embedding model (say OpenAI’s\\nembeddings,  or  SentenceTransformers  locally)  to  create  vectors  and  store  them  in  something  like\\nChromaDB (which is an easy local vector store that LangChain supports). 3. Hook up a simple API route (if\\nusing  Next.js,  for  example)  that  takes  a  user’s  question,  does  the  retrieval,  and  returns  the  answer.\\nLangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only',\n",
       " 'LangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only\\na few dozen lines of code once the environment is set up. 4. Test and refine: see if the answers are accurate.\\nIf the bot ever says “I don’t know that” for something you did provide in the data, you might need to ensure\\nthe embedding is picking it up or add more context. You can also tweak prompts (e.g., add a system\\nmessage: “You are a chatbot that answers questions only using the provided context about [Your Name]. If\\nyou cannot find the answer in the context, say you do not know.”).\\nCost-wise, retrieval approach can be very cheap, especially if you use local models. If using an API, you pay\\nper call but the usage for a personal portfolio (with presumably low traffic) is negligible. Fine-tuning has an\\nupfront cost (compute for training) but then usage of the model is just the cost of running a server.',\n",
       " 'upfront cost (compute for training) but then usage of the model is just the cost of running a server.\\nIt’s worth noting one hybrid idea: you could fine-tune a smaller model and use retrieval with it. For example,\\nfine-tune a 7B model on a small dataset just to give it some familiarity with your style or key facts, but still\\nuse a vector store to feed it more detailed or less frequently used facts. This is probably unnecessary here –\\nit’s more complex and the pure retrieval method should suffice – but it’s an option if you find the fine-tuned\\nmodel alone isn’t reliable for less common questions.\\nIn conclusion, if we “look at both options” as you requested: - Option 1 (Personal LLM via fine-tuning):\\nFeasible with LoRA on a 7B/13B model; provides a self-contained model; requires more upfront work and\\ndoesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use',\n",
       " 'doesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use\\ncase if done right, but keep expectations reasonable in terms of model accuracy. - Option 2 (RAG with no\\ntraining): More straightforward and likely to give accurate, up-to-date answers; leverages powerful existing\\nmodels; minimal cost to maintain; easier to scale or improve incrementally. For a “simple ask questions\\nabout me and get answers” goal, this is arguably the best way to complete the task with least friction. The\\nfact that industry solutions favor RAG for Q&A on custom data\\n is a strong indicator.\\nGiven  the  constraints  (limited  data  and  budget)  and  the  desire  for  state-of-the-art  techniques,  my\\nsuggestion would be: Why not do both, sequentially? You could implement the RAG approach first to have\\na working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same',\n",
       " 'a working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same\\ndata to see how it compares. This way, your portfolio has a reliable Q&A function (backed by retrieval and a\\nrobust model), and you still get to play with training a model as a side project (which you can swap in if it\\nbecomes good enough, or at least blog about the process as an AI engineer!). This combined approach\\nleverages the strength of RAG for now, while keeping the door open for a custom LLM when it’s viable.\\nTo directly answer your question:  Yes, you can use Vercel and host on your own domain (just add a\\ncustom domain in Vercel settings and point your DNS records accordingly)\\n. For the chatbot, using\\nNext.js would streamline the integration of your AI backend and frontend. Start gathering your personal\\ndata in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and',\n",
       " 'data in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8',\n",
       " 'use LoRA to keep it cheap and see how well it performs. By comparing both, you’ll see which one meets\\nyour needs in practice. Many have found RAG to be “faster, cheaper, and way more practical” for Q&A\\nbots\\n, but with your expertise you might get a surprisingly good result with a tailored mini-LLM as well.\\nGood luck with building your personal AI-powered portfolio site!\\nSources:\\nVercel Custom Domain Documentation\\nContentful Blog – Advantages of Next.js (built-in routing and backend)\\nSebastian Raschka – LoRA Fine-tuning 7B models on single GPU\\nReddit (LocalLLaMA) – Advice on knowledge-base chatbots (RAG workflow)\\nStack AI Blog – Fine-tuning vs RAG for custom chatbots\\nSwirlAI Newsletter – Challenges with fine-tuning vs retrieval\\nMicrosoft Learn (Azure AI) – RAG architecture and description\\nNext.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain',\n",
       " 'Next.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview',\n",
       " 'RAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\\nWhat is the best way to create a knowledge-base specific LLM chatbot ? : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/\\n12\\n• \\n3\\n4\\n• \\n2\\n• \\n5\\n• \\n15\\n11\\n• \\n8\\n12\\n• \\n9\\n13\\n• \\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n19\\n8\\n12\\n9\\n13\\n10\\n16\\n17\\n11\\n14\\n15\\n18\\n9',\n",
       " 'Architecting the Agentic Portfolio: A \\nComprehensive Technical Report on \\nRAG, Knowledge Graphs, and \\nGenerative 3D Interfaces \\n \\n \\n1. Introduction: The Paradigm Shift to Agentic Personal \\nBranding \\n \\nThe digital portfolio has long served as the static representation of a developer\\'s capability—a \\npassive repository of resumes, project links, and code snippets waiting to be discovered. \\nHowever, the rapid ascent of Large Language Models (LLMs) and Retrieval-Augmented \\nGeneration (RAG) is forcing a fundamental re-evaluation of how professional competence is \\ndemonstrated. We are witnessing a transition from the \"Portfolio as Document\" to the \\n\"Portfolio as Agent.\" In this emerging paradigm, the portfolio is no longer an artifact to be \\nread but an active, intelligent system capable of reasoning about its owner\\'s history, \\nanswering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent.',\n",
       " 'answering interrogative queries with high-fidelity context, and dynamically generating user \\ninterfaces to suit the visitor\\'s intent. \\nThis transformation is not merely aesthetic; it is a direct response to the increasing complexity \\nof modern software engineering. A static list of skills (e.g., \"React, Python, AWS\") fails to \\nconvey the depth of application—how a candidate handled race conditions in a distributed \\nsystem or optimized a render loop in a graphics application. An Agentic Portfolio, powered by \\nadvanced RAG, bridges this gap by allowing recruiters and technical managers to interrogate \\nthe data: \"How did this candidate optimize database queries in their 2023 e-commerce \\nproject?\" \\nThis report provides an exhaustive architectural blueprint for constructing such a system. It \\nmoves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative',\n",
       " 'moves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative \\n3D User Interfaces (GenUI). It rigorously examines the data engineering required to prevent \\nhallucinations, the mathematical principles behind advanced retrieval strategies, and the \\nsoftware engineering practices—specifically unit testing and evaluation—necessary to deploy',\n",
       " 'a reliable AI agent. \\n \\n2. Foundational Data Engineering: The bedrock of RAG \\n \\nThe axiom \"Garbage In, Garbage Out\" is the governing law of Retrieval-Augmented \\nGeneration. A RAG system’s intelligence is deterministically limited by the structure and \\nsemantic clarity of its underlying data. While many Minimum Viable Products (MVPs) rely on \\nsimple text extraction from PDFs, this approach is fundamentally flawed for high-stakes \\napplications like professional portfolios, where precision is paramount. \\n \\n2.1 The Fallacy of Unstructured Ingestion \\n \\nStandard ingestion pipelines often utilize libraries such as PyMuPDF or pypdf to strip text from \\nPDF resumes.1 While computationally efficient, these tools discard the semantic signals \\nembedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a',\n",
       " 'embedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a \\nhierarchical relationship to the header above them. Linear text extraction flattens this \\nhierarchy. A column-based layout, for instance, might be read line-by-line across columns, \\nmerging a \"Skills\" list with a \"Work History\" description, creating nonsensical chunks such as \\n\"Python 2018-Present Manager at Company X.\" \\nThis loss of structure leads to \"Context Dissociation.\" When an LLM retrieves a chunk \\ncontaining \"reduced latency by 50%,\" but the header identifying the specific project or \\ncompany was stripped during ingestion, the model cannot accurately attribute the \\nachievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the',\n",
       " 'achievement. \\n \\n2.2 Vision-Language Models (VLMs) for Structural Parsing \\n \\nTo overcome the limitations of text-based parsers, the industry standard is shifting toward the \\nuse of Vision-Language Models (VLMs) like GPT-4o or Claude 3.5 Sonnet for document \\ningestion.3 Unlike OCR (Optical Character Recognition), which identifies characters, VLMs \\nunderstand layout semantics.',\n",
       " 'The ingestion pipeline for the Agentic Portfolio functions as follows: \\n1.\\u200b Rasterization: The PDF resume is converted into high-resolution images (e.g., 300 DPI \\nPNGs). \\n2.\\u200b Vision Prompting: The VLM is prompted to transcribe the image into a strictly defined \\nJSON schema. The prompt must explicitly instruct the model to respect visual hierarchy: \\n\"Identify the date ranges associated with each role and nest them within the \\'experience\\' \\nobject. Extract skills listed in sidebars and categorize them.\" \\n3.\\u200b Schema Validation: The output is validated against a Zod or Pydantic schema to ensure \\ntype safety before entering the database. \\nThis approach, while incurring a higher initial computational cost (token usage for image \\nprocessing), ensures a \"Golden Source\" of truth. It allows for the normalization of \\nentities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5',\n",
       " 'entities—converting \"React.js,\" \"ReactJS,\" and \"React\" into a single canonical Skill \\nentity—which is critical for downstream retrieval accuracy.5 \\n \\n2.3 Defining the Knowledge Schema \\n \\nTo facilitate advanced retrieval strategies like filtering and graph traversal, the unstructured \\nbio data must be mapped to a rigorous schema. We recommend adopting and extending the \\nJSON Resume standard, augmenting it with vector-specific fields. \\nTable 1: Proposed Data Schema for RAG Optimization \\nEntity Field \\nData Type \\nDescription & RAG Utility \\nbasics.summary \\nString \\nA high-level bio used for \\n\"Who are you?\" queries. \\nEmbedded as a single \\nchunk. \\nwork.highlights \\nArray<String> \\nGranular achievements \\n(e.g., \"Optimized SQL \\nqueries\"). Each string is an \\nindividual \"Child Chunk.\" \\nwork.tech_stack \\nArray<String> \\nA list of technologies used \\nin that specific role. Used \\nfor metadata filtering (e.g.,',\n",
       " 'filter: { tech_stack: { $in: \\n[\"Python\"] } }). \\nwork.context_blob \\nString \\nA synthetic paragraph \\ngenerated by an LLM that \\ncombines the role, \\ncompany, and dates into a \\nnarrative format. This \\nserves as the \"Parent \\nChunk\" for retrieval. \\nskills.keywords \\nArray<String> \\nSynonyms and related \\nterms (e.g., Skill: \"AWS\", \\nKeywords:). Enhances \\nrecall for varied user \\nqueries. \\nprojects.emb_text \\nString \\nA \"dense\" description \\nspecifically engineered for \\nembedding, removing stop \\nwords and focusing on \\nsemantic keywords. \\nThis structured approach allows for Metadata Filtering.7 Instead of relying solely on cosine \\nsimilarity, the system can execute a pre-filter step. If a user asks, \"What was your experience \\nat Google?\", the retriever creates a filter company == \"Google\" before performing the vector \\nsearch, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent',\n",
       " 'search, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent \\n \\nChunking—the process of breaking text into manageable pieces for embedding—is the single \\nmost critical hyperparameter in a RAG pipeline. For a technical portfolio, where the \\nconnection between a specific tool (e.g., Redis) and a specific outcome (e.g., caching layer) is \\nvital, standard fixed-size chunking (e.g., every 500 characters) is disastrous. It risks severing',\n",
       " 'the subject from the predicate.9 \\n \\n3.1 Semantic Chunking \\n \\nSemantic chunking moves beyond arbitrary character counts to respect the \"semantic \\nboundaries\" of the text. The algorithm operates by embedding sequential sentences and \\ncalculating the cosine similarity between them. \\nThe mechanism is as follows: \\n1.\\u200b Sentence Splitting: The text is broken into individual sentences using a natural language \\ntokenizer (e.g., NLTK or SpaCy). \\n2.\\u200b Sequential Embedding: Each sentence is embedded using a lightweight model (e.g., \\nall-MiniLM-L6-v2). \\n3.\\u200b Coherence Calculation: The algorithm calculates the similarity score $S$ between \\nSentence $N$ and Sentence $N+1$. \\n4.\\u200b Breakpoint Detection: If $S$ drops below a defined threshold (e.g., 0.7), it indicates a \\n\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point.',\n",
       " '\"topic shift\"—for instance, moving from discussing Frontend architecture to Backend \\ndatabase design. A chunk boundary is established at this inflection point. \\nThis ensures that each retrieved chunk represents a self-contained thought or topic, \\nsignificantly improving the \"Faithfulness\" metric of the generated answers.11 \\n \\n3.2 The Parent-Document Retrieval Pattern \\n \\nTechnical resumes often contain bullet points that are semantically dense but contextually \\nsparse. A bullet point might read: \"Migrated legacy codebase to TypeScript.\" \\nIf this single sentence is retrieved in isolation, the LLM lacks crucial context: Which company \\nwas this for? When did it happen? What was the impact? Embedding the entire project \\ndescription, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9',\n",
       " 'description, however, dilutes the vector signal of \"TypeScript,\" making it harder to retrieve. \\nThe Parent-Document Retrieval pattern (also known as Small-to-Big retrieval) solves this \\ndilemma.9 \\n●\\u200b Indexing: The system splits the document into small \"Child Chunks\" (individual bullet \\npoints) and embeds them. These are optimized for high-precision matching. \\n●\\u200b Storage: The full \"Parent Document\" (the entire project narrative or work history block) is',\n",
       " 'stored in a separate key-value store (e.g., Redis or the Supabase text column), referenced \\nby a parent_id. \\n●\\u200b Retrieval: The user query matches the dense Child Chunk. The system then uses the \\nparent_id to fetch the full Parent Document. \\n●\\u200b Generation: The LLM receives the full Parent Document as context. \\nThis architectural pattern decouples the retrieval unit (optimized for search) from the \\ngeneration unit (optimized for context), providing the best of both worlds. \\n \\n3.3 Post-Chunking and Late Interaction \\n \\nEmerging research suggests a shift toward \"Post-Chunking\" or Late Interaction models (like \\nColBERT).10 In this paradigm, the entire document is embedded at the token level, and \\ninteraction with the query happens before reducing to a single vector score. While highly \\neffective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic',\n",
       " 'effective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic \\nChunking combined with Parent-Document retrieval offers the optimal trade-off between \\nperformance and complexity. \\n \\n4. The Retrieval Engine: Moving Beyond Cosine \\nSimilarity \\n \\nA \"naive\" RAG system relies exclusively on dense vector search (embeddings). While powerful \\nfor semantic conceptual matching, dense retrieval struggles with precise keyword matching, \\nparticularly with technical acronyms (e.g., \"C#\" vs. \"C++\", \"AWS\" vs. \"GCP\"). To achieve \\nindustry standards, the portfolio must implement a Hybrid Search architecture. \\n \\n4.1 Hybrid Search Architecture \\n \\nHybrid search combines the strengths of two distinct retrieval algorithms: \\n1.\\u200b Dense Retrieval (Vector Search): Utilizes embeddings (e.g., OpenAI \\ntext-embedding-3-small) to understand semantic intent. It excels at queries like \"Tell me',\n",
       " 'about your leadership style\" where exact keyword matches are less important than \\nconceptual alignment. \\n2.\\u200b Sparse Retrieval (Keyword Search): Utilizes algorithms like BM25 (Best Matching 25) or \\nSPLADE to match exact tokens. It excels at technical queries like \"Do you know Redux \\nToolkit?\" where the presence of the specific term is non-negotiable.7 \\n \\n4.1.1 Reciprocal Rank Fusion (RRF) \\n \\nTo combine the results from these two disparate algorithms, we employ Reciprocal Rank \\nFusion (RRF). RRF does not rely on the absolute scores (which are incomparable between \\nCosine Similarity and BM25); instead, it relies on the rank of the document in each list. \\nThe RRF score for a document $d$ is calculated as: \\n \\n \\n$$RRFscore(d) = \\\\sum_{r \\\\in R} \\\\frac{1}{k + r(d)}$$ \\n \\nWhere: \\n●\\u200b $R$ is the set of rank lists (one from Dense, one from Sparse). \\n●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$.',\n",
       " '●\\u200b $k$ is a constant (typically 60) that mitigates the impact of high rankings by outliers. \\n●\\u200b $r(d)$ is the rank of document $d$ in list $r$. \\nThis mathematical fusion ensures that a document appearing in the top 5 results of both \\nsearch methods is prioritized over a document that is #1 in vector search but absent in \\nkeyword search.13 \\n \\n4.2 Vector Database Selection: Supabase vs. Pinecone \\n \\nSelecting the right vector store is a critical infrastructure decision. The market is divided \\nbetween specialized vector databases (Pinecone, Weaviate) and integrated vector extensions \\nfor traditional databases (Supabase/Postgres). \\nTable 2: Vector Database Comparison for Portfolio Use Case \\n \\nFeature \\nSupabase \\nPinecone \\nAnalysis for',\n",
       " '(pgvector) \\nPortfolio \\nArchitecture \\nIntegrated \\n(Postgres \\nExtension) \\nSpecialized \\nManaged Service \\nSupabase allows \\nstoring relational \\ndata (Projects, \\nUsers) alongside \\nvectors in a single \\nDB, simplifying the \\nstack.15 \\nFree Tier \\n500MB DB Size \\n(Millions of vectors) \\nLimited to 1 index, \\nrestricted \\nthroughput \\nSupabase\\'s 500MB \\nlimit is sufficient for \\nvast amounts of \\ntext data (resumes \\nare KBs). \\nPinecone\\'s free tier \\nis restrictive for \\nmultiple projects.17 \\nHybrid Search \\nNative support \\n(tsvector + \\npgvector) \\nSupported via \\n\"Sparse-Dense\" \\nvectors \\nSupabase allows \\nfusing SQL WHERE \\nclauses with vector \\nsearch naturally. \\nPinecone requires \\nmanaging \\nmetadata \\nseparately.19 \\nLatency \\n~150-200ms (Cold \\nboot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20',\n",
       " 'boot variance) \\n~40-80ms \\n(Optimized \\ncaching) \\nPinecone is faster, \\nbut for a chat \\ninterface, 200ms is \\nimperceptible. \\nSupabase\\'s \\n\"all-in-one\" value \\noutweighs the \\nmillisecond gain.20 \\nConclusion: For a personal portfolio, Supabase is the superior architectural choice. It \\neliminates the \"data synchronization\" problem—the risk that the metadata in the vector store \\ndrifts from the primary application database.',\n",
       " '4.3 Reranking: The Precision Layer \\n \\nEven with Hybrid Search, the \"Top K\" retrieved documents may include irrelevant noise. To \\nfilter this, we introduce a Reranking step using a Cross-Encoder model (e.g., \\nbge-reranker-v2-m3 or Cohere Rerank). \\nUnlike Bi-Encoders (embedding models) which process the query and document \\nindependently, Cross-Encoders process them simultaneously, outputting a single scalar score \\nindicating relevance. \\n●\\u200b Workflow: Retrieve top 25 documents via Hybrid Search -> Pass to Reranker -> Keep Top \\n5. \\n●\\u200b Impact: This dramatically increases the density of relevant information in the LLM\\'s \\ncontext window, reducing the likelihood of \"Lost in the Middle\" phenomenon where the \\nLLM ignores context buried in the center of the prompt.7 \\n \\n5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to',\n",
       " '5. Query Transformation and Advanced Reasoning \\n \\nUsers rarely formulate perfect queries. A query like \"backend?\" is ambiguous. Does it refer to \\ndatabase design, API development, or server management? Direct retrieval on such queries \\noften yields poor results. To address this, we implement Query Transformation strategies. \\n \\n5.1 Hypothetical Document Embeddings (HyDE) \\n \\nHyDE is a technique that leverages the LLM\\'s internal knowledge to bridge the gap between a \\nshort query and a detailed document.1 \\nMechanism: \\n1.\\u200b Prompt: The system prompts an LLM: \"Write a hypothetical resume entry for a senior \\nengineer answering the query: \\'backend experience\\'.\" \\n2.\\u200b Hallucination: The LLM generates a fictional but semantically rich paragraph: \\n\"Experienced in building RESTful APIs using Node.js and Express, managing PostgreSQL \\ndatabases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector.',\n",
       " 'databases, and optimizing query performance...\" \\n3.\\u200b Embedding: This hypothetical paragraph is embedded into a vector. \\n4.\\u200b Retrieval: The vector search is performed using this hypothetical vector.',\n",
       " 'Because the hypothetical document shares the same semantic structure (technical \\nvocabulary, sentence structure) as the actual resume chunks, the retrieval accuracy is \\nsignificantly higher than using the raw query \"backend experience.\" \\n \\n5.2 Multi-Query Decomposition \\n \\nFor complex questions like \"Compare your experience with React and Vue,\" a single vector \\nsearch often fails to retrieve sufficient context for both topics. \\nMechanism: \\n1.\\u200b Decomposition: An LLM splits the query into sub-queries:. \\n2.\\u200b Parallel Retrieval: Both queries are executed independently. \\n3.\\u200b Fusion: The unique documents from both retrieval sets are combined (deduplicated). \\n4.\\u200b Generation: The combined context is passed to the LLM to answer the comparison \\nquestion. \\nThis ensures that the system doesn\\'t latch onto one concept (e.g., React) at the expense of \\nthe other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured',\n",
       " 'the other.9 \\n \\n6. GraphRAG: The Knowledge Graph Layer \\n \\nWhile vectors excel at semantic similarity, they lack an understanding of structured \\nrelationships. A vector search for \"Frameworks\" might miss \"Next.js\" if the embedding model \\ndoesn\\'t explicitly map the \"is-a\" relationship. GraphRAG addresses this by modeling the \\nportfolio as a Knowledge Graph (KG).21 \\n \\n6.1 Graph Schema Design \\n \\nThe Knowledge Graph structures the portfolio into Entities and Relationships. \\n●\\u200b Nodes: Person, Skill, Project, Company, Role, Concept. \\n●\\u200b Edges: \\n○\\u200b (Project)-->(Skill) \\n○\\u200b (Skill)-->(Concept)',\n",
       " '○\\u200b (Person)-->(Company) \\n○\\u200b (Project)-->(Outcome) \\nThis structure enables Multi-hop Reasoning. A user might ask: \"Which projects used a \\ncloud-native database?\" \\n●\\u200b Vector Search: Finds \"Cloud-native database\" $\\\\rightarrow$ matches DynamoDB node. \\n●\\u200b Graph Traversal: (DynamoDB)<--(Project A) and (DynamoDB)<--(Project B). \\n●\\u200b Result: The system identifies Project A and Project B, even if the project descriptions \\nnever explicitly used the phrase \"cloud-native database.\" \\n \\n6.2 Implementation: NetworkX vs. Neo4j \\n \\nFor enterprise applications, graph databases like Neo4j are standard. However, for a personal \\nportfolio with a limited dataset (< 1000 nodes), running a dedicated Neo4j instance is often \\noverkill in terms of cost and maintenance. \\nLightweight Alternative: NetworkX \\nWe recommend using NetworkX (a Python library) to build and query the graph in-memory or \\nserialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the',\n",
       " 'serialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the \\nnode-edge list. This is saved as graph.json. \\n2.\\u200b Run Time: The Next.js API route loads graph.json. Simple traversal algorithms (e.g., \\nfinding all neighbors of a matched node) are executed in JavaScript/TypeScript. \\n3.\\u200b Integration: The graph results are textually formatted (e.g., \"Related Skills: X, Y, Z\") and \\nappended to the context window alongside vector results. \\nThis \"Client-Side Graph\" approach delivers 80% of the value of Neo4j with 0% of the \\ninfrastructure cost.25 \\n \\n7. Generative UI: The Interactive 3D Interface \\n \\nThe defining feature of the Agentic Portfolio is Generative UI—the ability of the AI to not just \\ntalk, but to show and control the interface. We utilize the Vercel AI SDK and React Server \\nComponents (RSC) to stream UI elements.',\n",
       " '7.1 Generative UI Architecture \\n \\nIn a standard chatbot, the output is Markdown. In a Generative UI system, the LLM can output \\nReact components. \\n●\\u200b Mechanism: The Vercel AI SDK\\'s streamUI function allows the server to stream \\nrenderable React components to the client.26 \\n●\\u200b Use Case: \\n○\\u200b User: \"Show me your design skills.\" \\n○\\u200b LLM Response: Instead of listing skills, it streams a <SkillCloud category=\"design\" /> \\ncomponent. The client renders this interactive 3D cloud immediately within the chat \\nstream. \\n○\\u200b Fallback: If the client doesn\\'t support the component, it falls back to a text \\ndescription. \\n \\n7.2 Controlling React Three Fiber via Tool Calling \\n \\nThe portfolio features a 3D scene (built with React Three Fiber) that acts as an immersive \\nbackground. The Chat Agent acts as the controller for this scene. \\nState Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28',\n",
       " \"State Management Bridge (Zustand): \\nTo bridge the gap between the imperatively driven Chat logic and the declarative 3D scene, \\nwe use a global state manager, Zustand.28 \\nImplementation Steps: \\n1.\\u200b Store Definition:\\u200b\\nTypeScript\\u200b\\ninterface State {\\u200b\\n  cameraTarget: Vector3;\\u200b\\n  setCameraTarget: (v: Vector3) => void;\\u200b\\n}\\u200b\\nconst useStore = create<State>((set) => ({... }));\\u200b\\n \\n2.\\u200b Tool Definition: The LLM is provided with a tool moveCamera.\\u200b\\nTypeScript\\u200b\\nconst tools = {\\u200b\\n  moveCamera: tool({\\u200b\\n    description: 'Move the 3D camera to focus on a specific section.',\\u200b\\n    parameters: z.object({ section: z.enum(['home', 'projects', 'about']) }),\\u200b\",\n",
       " 'execute: async ({ section }) => {\\u200b\\n       const coords = SECTION_COORDINATES[section];\\u200b\\n       useStore.getState().setCameraTarget(coords);\\u200b\\n       return `Moved camera to ${section}.`;\\u200b\\n    }\\u200b\\n  })\\u200b\\n};\\u200b\\n \\n3.\\u200b Scene Reaction: Inside the R3F Canvas, a component subscribes to the store.\\u200b\\nTypeScript\\u200b\\nfunction CameraController() {\\u200b\\n  const target = useStore((state) => state.cameraTarget);\\u200b\\n  useFrame((state, delta) => {\\u200b\\n    easing.damp3(state.camera.position, target, 0.5, delta); // Smooth interpolation\\u200b\\n  });\\u200b\\n  return null;\\u200b\\n}\\u200b\\n \\nThis architecture creates a seamless \"Magical\" experience where the user\\'s conversation \\nphysically drives the exploration of the 3D space.30 \\n \\n8. Coding Agents and Prompt Engineering \\n \\nDeveloping this complex system requires leveraging \"Coding Agents\" (like Cursor or GitHub \\nCopilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy',\n",
       " 'Copilot) effectively. The key to success is providing these agents with adequate context. \\n \\n8.1 The .cursorrules Strategy \\n \\nTo ensure the coding agent generates code that adheres to the specific architectural \\nconstraints (Next.js 14, Tailwind, Supabase), we place a .cursorrules file in the project root.32 \\nSample Rule Configuration:',\n",
       " 'Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy.',\n",
       " 'are non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy. \\n \\n9.1 Tier 1: Deterministic Unit Testing (pytest) \\n \\nWe mock the LLM and Vector Store to test the logic surrounding the AI.36 \\n●\\u200b Test Case: Verify that the moveCamera tool is correctly invoked when the input text \\ncontains directional intent. \\n●\\u200b Test Case: Verify that the PDF parser correctly extracts email addresses from a mock \\nresume file.',\n",
       " '●\\u200b Mocking: Use unittest.mock to simulate the Supabase client, ensuring that the retrieval \\nlogic handles empty result sets gracefully without crashing. \\n \\n9.2 Tier 2: LLM-based Evaluation (Ragas & DeepEval) \\n \\nTo evaluate the quality of the AI\\'s answers, we use the Ragas framework (Retrieval \\nAugmented Generation Assessment). Ragas uses an \"LLM-as-a-Judge\" (typically GPT-4) to \\nscore the system on specific metrics.38 \\nKey Metrics: \\n1.\\u200b Context Recall: Does the retrieved context contain the ground truth answer? (Measures \\nRetrieval quality). \\n2.\\u200b Faithfulness: Is the generated answer derived solely from the retrieved context, or did \\nthe model hallucinate? (Measures Generation quality). \\n3.\\u200b Answer Relevance: Does the answer directly address the user\\'s query? \\nCI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions.',\n",
       " 'CI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions. \\n●\\u200b Assertion: assert ragas_score[\\'context_recall\\'] > 0.8 \\n●\\u200b Outcome: If a code change (e.g., changing the chunking size) causes the score to drop \\nbelow 0.8, the PR is blocked. This prevents regressions in the agent\\'s intelligence.40 \\n \\n10. Conclusion \\nThe architecture described in this report represents the convergence of modern Full Stack \\nEngineering and Applied AI. By moving from naive text extraction to Vision-based Parsing, \\nfrom simple vector search to Hybrid Graph Retrieval, and from static text output to \\nGenerative 3D Interfaces, we transform the personal portfolio into a sophisticated Agentic \\nSystem. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate\\'s history. But more importantly, the existence of the',\n",
       " \"System. \\nThis system serves a dual purpose. Functionally, it provides recruiters with a frictionless, \\ndeep-dive interface into the candidate's history. But more importantly, the existence of the \\nsystem itself serves as the ultimate proof of competence. It demonstrates mastery not just of \\ncode, but of data engineering, system architecture, AI orchestration, and user experience \\ndesign—the defining skills of the next generation of software engineers. The Agentic Portfolio \\nis not just a showcase of work; it is the work itself.\",\n",
       " \"Works cited \\n1.\\u200b Optimizing RAG. RAG Demystified: A Hands-On Guide to… | by Skanda Vivek | \\nEMAlpha, accessed November 20, 2025, \\nhttps://medium.com/emalpha/optimizing-rag-bd65ebc5e51a \\n2.\\u200b Developing Retrieval Augmented Generation (RAG) based LLM Systems from \\nPDFs: An Experience Report - arXiv, accessed November 20, 2025, \\nhttps://arxiv.org/html/2410.15944v1 \\n3.\\u200b Using Azure OpenAI GPT-4o to extract structured JSON data from PDF \\ndocuments, accessed November 20, 2025, \\nhttps://learn.microsoft.com/en-us/samples/azure-samples/azure-openai-gpt-4-vi\\nsion-pdf-extraction-sample/using-azure-openai-gpt-4o-to-extract-structured-js\\non-data-from-pdf-documents/ \\n4.\\u200b How can I process a pdf using OpenAI's APIs (GPTs)? - Stack Overflow, accessed \\nNovember 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL\",\n",
       " 'November 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL \\nquerying, and agents with Amazon Bedrock, accessed November 20, 2025, \\nhttps://aws.amazon.com/blogs/machine-learning/boosting-rag-based-intelligent-\\ndocument-assistants-using-entity-extraction-sql-querying-and-agents-with-am\\nazon-bedrock/ \\n6.\\u200b Conversion of entire PDF into JSON Format - API - OpenAI Developer \\nCommunity, accessed November 20, 2025, \\nhttps://community.openai.com/t/conversion-of-entire-pdf-into-json-format/1066\\n551 \\n7.\\u200b 9 advanced RAG techniques to know & how to implement them - Meilisearch, \\naccessed November 20, 2025, https://www.meilisearch.com/blog/rag-techniques \\n8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025,',\n",
       " '8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025, \\nhttps://www.datasciencecentral.com/best-practices-for-structuring-large-datas\\nets-in-retrieval-augmented-generation-rag/ \\n9.\\u200b Top 13 Advanced RAG Techniques for Your Next Project - Analytics Vidhya, \\naccessed November 20, 2025, \\nhttps://www.analyticsvidhya.com/blog/2025/04/advanced-rag-techniques/ \\n10.\\u200bBeyond Basic Chunking: The Critical Timing Decision in RAG Systems That \\nEveryone Is Getting Wrong, accessed November 20, 2025, \\nhttps://skngrp.medium.com/beyond-basic-chunking-the-critical-timing-decision-\\nin-rag-systems-that-everyone-is-getting-wrong-19febb2ee062 \\n11.\\u200bComparing Chunking Strategies for RAG: From Naive Splits to Striding Windows | \\nby Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116',\n",
       " 'by Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116 \\n12.\\u200bMastering Chunking Strategies for RAG: Best Practices & Code Examples - \\nDatabricks Community, accessed November 20, 2025,',\n",
       " 'https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/',\n",
       " \"17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/ \\n19.\\u200bpgvector vs Pinecone: cost and performance - Supabase, accessed November \\n20, 2025, https://supabase.com/blog/pgvector-vs-pinecone \\n20.\\u200bSupabase vs Pinecone: I Migrated My Production AI System and Here's What \\nActually Matters - Dee, accessed November 20, 2025, \\nhttps://deeflect.medium.com/supabase-vs-pinecone-i-migrated-my-production-\\nai-system-and-heres-what-actually-matters-7b2f2ebd59ee \\n21.\\u200bGraphRAG for Devs: Graph-Code Demo Overview - Memgraph, accessed \\nNovember 20, 2025, \\nhttps://memgraph.com/blog/graphrag-for-devs-coding-assistant \\n22.\\u200bThe GraphRAG Manifesto: Adding Knowledge to GenAI - Neo4j, accessed \\nNovember 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025,\",\n",
       " 'November 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber',\n",
       " 'accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber \\n29.\\u200bAccessing the Camera in React Three Fiber out of the canvas - Questions, \\naccessed November 20, 2025,',\n",
       " 'https://discourse.threejs.org/t/accessing-the-camera-in-react-three-fiber-out-of\\n-the-canvas/39137 \\n30.\\u200bMove camera to face an object in React Three Fiber! - Questions, accessed \\nNovember 20, 2025, \\nhttps://discourse.threejs.org/t/move-camera-to-face-an-object-in-react-three-fi\\nber/81269 \\n31.\\u200bCamera Controls - Wawa Sensei, accessed November 20, 2025, \\nhttps://wawasensei.dev/courses/react-three-fiber/lessons/camera-controls \\n32.\\u200bPatrickJS/awesome-cursorrules: Configuration files that enhance Cursor AI editor \\nexperience with custom rules and behaviors - GitHub, accessed November 20, \\n2025, https://github.com/PatrickJS/awesome-cursorrules \\n33.\\u200bHow Cursor project rules can improve Next.js app development - LogRocket \\nBlog, accessed November 20, 2025, \\nhttps://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025,',\n",
       " 'https://blog.logrocket.com/cursor-project-rules-improve-next-js-app-developm\\nent/ \\n34.\\u200bNext.js 15 (React 19, Vercel AI, Tailwind) | Cursor Rules Guide ..., accessed \\nNovember 20, 2025, \\nhttps://cursorrules.org/article/nextjs15-react19-vercelai-tailwind-cursorrules-pro\\nmpt-file \\n35.\\u200bThe ultimate .cursorrules for TypeScript, React 19, Next.js 15, Vercel AI SDK, \\nShadcn UI, Radix UI, and Tailwind CSS : r/cursor - Reddit, accessed November 20, \\n2025, \\nhttps://www.reddit.com/r/cursor/comments/1gjd96h/the_ultimate_cursorrules_for\\n_typescript_react_19/ \\n36.\\u200bHow to Properly Mock LangChain LLM Execution in Unit Tests | Python - Medium, \\naccessed November 20, 2025, \\nhttps://medium.com/@matgmc/how-to-properly-mock-langchain-llm-execution-\\nin-unit-tests-python-76efe1b8707e \\n37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-',\n",
       " '37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG',\n",
       " 'Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025',\n",
       " 'Master of Science in Artificial Intelligence (Concentration: Machine Learning), GPA – 4.0                                                                                                                               Denton,TX  \\nCoursework : Machine Learning, Deep Learning, NLP, Information Retrieval, Big Data & Data Science, AI Software Development \\n.    \\nBirla Institute of Technology and Science                                                                                                                                                            June 2019 \\nBachelor of Science in Mechanical Engineering with Honours, GPA – 3.7                                                                                                                                                        Dubai, UAE  \\n \\nTECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering',\n",
       " 'TECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present',\n",
       " '• Cut BA story update effort by 80-90% by deploying a multi-agent pipeline (Fal.ai STT + GPT-4.1 in n8n) that updates Jira Cloud tickets end-to-end \\n(Description + Acceptance Criteria + Status/Assignee) via REST APIs. \\n• Built a Chrome extension (JS/HTML/CSS + REST) to capture voice → transcript → LLM output → structured Jira updates, improving time-to-update \\nfrom ~12 min → ~2 min. \\n• Authored an Acceptance Criteria (AC) Playbook and an NLP variance/coverage harness (Jaccard/F1) to benchmark LLM AC vs. senior BA standards; \\nimproved consistency and completeness of ACs. \\n• Tracked latency/cost per run; instrumented token usage and error categories to guide prompt and model selection decisions. \\nArtificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025',\n",
       " 'Artificial Intelligence IA – Deep Learning & Fundamentals of AI                                                                                University of North Texas, TX | Jan 2024 – May.2025 \\n• Assessed and provided actionable feedback on peer reviews of key AI research papers, enhancing students’ critical thinking and technical writing across \\nmultiple evaluation cycles. \\n• Led design and experimentation of ML solutions including GANs, transformers, and optimized CNNs for NLP and GenAI tasks. Applied CRISP-DM \\nmethodology and collaborated with academic teams on scalable prototype development for potential deployment.  \\n• Mentored students on applied AI projects, delivering personalized technical guidance that improved project success rates and supported milestone \\nachievement in coursework and capstone deliverables.',\n",
       " 'achievement in coursework and capstone deliverables. \\n \\nML/AI Engineer – AI Document Workflow Automation                                                                                            Strawberry Labs, Dubai | Jan.2023 – Jan 2024 \\n• Deployed an OCR + extraction pipeline (Tesseract + CNN post-processing) processing docs in ~20s each, delivering 96% OCR accuracy and 93% \\nfield extraction precision; manual data-entry time −75% (5 min → <1 min). \\n• In controlled tests, achieved 4× faster completion (20s vs 90s) and −70% QA checks, with 96% data correctness vs 85% in manual entry. \\n• Implemented validation rules (format, range, cross-field), exception routing, and confidence scoring to reduce human review and raise straight-through \\nprocessing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS',\n",
       " 'processing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS \\n  \\nConditional GAN for Ethnicity-Based Face Generation                                                                                                                             UNT, Denton, TX | Sep2024 – Dec 2024 \\n• Engineered a generative adversarial network (GAN) from the ground up to create facial images across six ethnicities (4,000 color images each), enabling data-\\ndriven insights into cross-cultural face generation.  \\n• Transitioned from an initial linear-layer design to convolutional layers for more robust feature extraction, improving model fidelity and output quality. \\n• Leveraged AWS to meet high computational (GPU) demands, streamlining large-scale model training and accelerating development cycles. \\nAutomated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024',\n",
       " 'Automated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024 \\n• Developed a pipeline that automatically identifies and classifies product components as “good” or “bad,” reducing manual inspection time and cost. \\n• Implemented advanced edge detection (Sobel, Laplacian of Gaussian) and custom convolution kernels to achieve a 100% detection rate (mIoU=0.8925). \\n• Leveraged both spatial (Probabilistic Hough Transform) and frequency domain (Fourier magnitude) methods to capture subtle texture patterns. \\n• Achieved 77% accuracy, 87% precision, and AUC of 0.85 on real-world data, with future improvements planned via data augmentation and refined \\nlighting/occlusion handling.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63456b1e",
   "metadata": {},
   "source": [
    "### Retriver Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a28406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    ''' Handles query based retrival from the vectore store'''\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \n",
    "        ''' Initializes teh retriver\n",
    "\n",
    "        Args:\n",
    "            vectorestore: VectoreStore containing the document embeddings\n",
    "            embedding manager: Manager for generating query embeddings\n",
    "        '''\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        '''\n",
    "        Retrieve relevant documents for a given query\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            top_k (int): Number of top retrieved results\n",
    "            score_threshold (float): Minimum similarity score threshold\n",
    "            \n",
    "        Return:\n",
    "            List of dictionries containing retrieved documents and metadata'''\n",
    "        \n",
    "        print(f\"Generating embedding for query: {query}\")\n",
    "        print(f\"Top_k: {top_k}, Score Threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search the vectore store\n",
    "\n",
    "        try:\n",
    "\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings= [query_embedding.tolist()],\n",
    "                n_results= top_k\n",
    "            )\n",
    "\n",
    "            # Process Result\n",
    "\n",
    "            retrived_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                print(f\"The distances retrieved are: {distances}\")\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas,distances)):\n",
    "                    # Convert distance to similarity scores\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score>= score_threshold:\n",
    "                        retrived_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1 \n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrived_docs)} documents after applying score threshold\")\n",
    "\n",
    "            else:\n",
    "                print(\"No documents retrieved from vector store.\")\n",
    "            \n",
    "            return retrived_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriver = RAGRetriever(vector_store,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f087490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1c9f4df9e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4a334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is RAG?\n",
      "Top_k: 10, Score Threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (1, 384)\n",
      "The distances retrieved are: [0.612400233745575, 0.6758878231048584, 0.6773046255111694, 0.6946336030960083, 0.6964412927627563, 0.738471269607544, 0.7537255883216858, 0.7552218437194824, 0.7956720590591431, 0.8051016330718994]\n",
      "Retrieved 10 documents after applying score threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_a9c74e25_89',\n",
       "  'content': '37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG',\n",
       "  'metadata': {'author': '',\n",
       "   'file_path': 'pdf',\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'total_pages': 18,\n",
       "   'page': 17,\n",
       "   'creationDate': '',\n",
       "   'trapped': '',\n",
       "   'moddate': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'modDate': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'keywords': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creationdate': '',\n",
       "   'subject': '',\n",
       "   'doc_idx': 89,\n",
       "   'content_length': 939,\n",
       "   'creator': ''},\n",
       "  'similarity_score': 0.38759976625442505,\n",
       "  'distance': 0.612400233745575,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_151eeb7b_83',\n",
       "  'content': 'https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/',\n",
       "  'metadata': {'author': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creator': '',\n",
       "   'creationdate': '',\n",
       "   'trapped': '',\n",
       "   'moddate': '',\n",
       "   'content_length': 925,\n",
       "   'format': 'PDF 1.4',\n",
       "   'modDate': '',\n",
       "   'total_pages': 18,\n",
       "   'file_path': 'pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'keywords': '',\n",
       "   'doc_idx': 83,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'creationDate': '',\n",
       "   'subject': '',\n",
       "   'page': 16},\n",
       "  'similarity_score': 0.3241121768951416,\n",
       "  'distance': 0.6758878231048584,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_b564b847_53',\n",
       "  'content': 'search, guaranteeing that no hallucinations from other work experiences contaminate the \\ncontext window. \\n \\n3. Advanced Chunking Strategies for Technical \\nContent \\n \\nChunking—the process of breaking text into manageable pieces for embedding—is the single \\nmost critical hyperparameter in a RAG pipeline. For a technical portfolio, where the \\nconnection between a specific tool (e.g., Redis) and a specific outcome (e.g., caching layer) is \\nvital, standard fixed-size chunking (e.g., every 500 characters) is disastrous. It risks severing',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'moddate': '',\n",
       "   'content_length': 537,\n",
       "   'author': '',\n",
       "   'page': 3,\n",
       "   'creationDate': '',\n",
       "   'creator': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'trapped': '',\n",
       "   'file_path': 'pdf',\n",
       "   'subject': '',\n",
       "   'modDate': '',\n",
       "   'total_pages': 18,\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'doc_idx': 53,\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.32269537448883057,\n",
       "  'distance': 0.6773046255111694,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_8c5fa007_81',\n",
       "  'content': '8.\\u200b Best practices for structuring large datasets in Retrieval-Augmented Generation \\n(RAG) - DataScienceCentral.com, accessed November 20, 2025, \\nhttps://www.datasciencecentral.com/best-practices-for-structuring-large-datas\\nets-in-retrieval-augmented-generation-rag/ \\n9.\\u200b Top 13 Advanced RAG Techniques for Your Next Project - Analytics Vidhya, \\naccessed November 20, 2025, \\nhttps://www.analyticsvidhya.com/blog/2025/04/advanced-rag-techniques/ \\n10.\\u200bBeyond Basic Chunking: The Critical Timing Decision in RAG Systems That \\nEveryone Is Getting Wrong, accessed November 20, 2025, \\nhttps://skngrp.medium.com/beyond-basic-chunking-the-critical-timing-decision-\\nin-rag-systems-that-everyone-is-getting-wrong-19febb2ee062 \\n11.\\u200bComparing Chunking Strategies for RAG: From Naive Splits to Striding Windows | \\nby Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116',\n",
       "  'metadata': {'moddate': '',\n",
       "   'creationDate': '',\n",
       "   'modDate': '',\n",
       "   'keywords': '',\n",
       "   'content_length': 993,\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'creationdate': '',\n",
       "   'file_path': 'pdf',\n",
       "   'page': 15,\n",
       "   'total_pages': 18,\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'trapped': '',\n",
       "   'author': '',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'doc_idx': 81,\n",
       "   'creator': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.3053663969039917,\n",
       "  'distance': 0.6946336030960083,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_1710e978_82',\n",
       "  'content': 'by Mert Şükrü Pehlivan | Sep, 2025, accessed November 20, 2025, \\nhttps://medium.com/@mertsukrupehlivan/comparing-chunking-strategies-for-ra\\ng-from-naive-splits-to-striding-windows-26a75e8ee116 \\n12.\\u200bMastering Chunking Strategies for RAG: Best Practices & Code Examples - \\nDatabricks Community, accessed November 20, 2025,',\n",
       "  'metadata': {'modDate': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'total_pages': 18,\n",
       "   'moddate': '',\n",
       "   'content_length': 320,\n",
       "   'creationdate': '',\n",
       "   'trapped': '',\n",
       "   'page': 15,\n",
       "   'author': '',\n",
       "   'creationDate': '',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'file_path': 'pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'subject': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'keywords': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creator': '',\n",
       "   'doc_idx': 82},\n",
       "  'similarity_score': 0.30355870723724365,\n",
       "  'distance': 0.6964412927627563,\n",
       "  'rank': 5},\n",
       " {'id': 'doc_c1400910_47',\n",
       "  'content': 'a reliable AI agent. \\n \\n2. Foundational Data Engineering: The bedrock of RAG \\n \\nThe axiom \"Garbage In, Garbage Out\" is the governing law of Retrieval-Augmented \\nGeneration. A RAG system’s intelligence is deterministically limited by the structure and \\nsemantic clarity of its underlying data. While many Minimum Viable Products (MVPs) rely on \\nsimple text extraction from PDFs, this approach is fundamentally flawed for high-stakes \\napplications like professional portfolios, where precision is paramount. \\n \\n2.1 The Fallacy of Unstructured Ingestion \\n \\nStandard ingestion pipelines often utilize libraries such as PyMuPDF or pypdf to strip text from \\nPDF resumes.1 While computationally efficient, these tools discard the semantic signals \\nembedded in the visual layout of a document. A resume is a highly structured visual \\ndocument: dates are aligned to the right, role titles are bolded, and bullet points imply a',\n",
       "  'metadata': {'format': 'PDF 1.4',\n",
       "   'content_length': 917,\n",
       "   'subject': '',\n",
       "   'total_pages': 18,\n",
       "   'moddate': '',\n",
       "   'modDate': '',\n",
       "   'creationdate': '',\n",
       "   'page': 1,\n",
       "   'file_path': 'pdf',\n",
       "   'author': '',\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'creator': '',\n",
       "   'keywords': '',\n",
       "   'doc_idx': 47,\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creationDate': '',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer'},\n",
       "  'similarity_score': 0.26152873039245605,\n",
       "  'distance': 0.738471269607544,\n",
       "  'rank': 6},\n",
       " {'id': 'doc_a08c7d9c_74',\n",
       "  'content': 'Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy.',\n",
       "  'metadata': {'format': 'PDF 1.4',\n",
       "   'moddate': '',\n",
       "   'total_pages': 18,\n",
       "   'creationDate': '',\n",
       "   'doc_idx': 74,\n",
       "   'modDate': '',\n",
       "   'keywords': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'creator': '',\n",
       "   'content_length': 971,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'page': 13,\n",
       "   'file_path': 'pdf',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'trapped': '',\n",
       "   'creationdate': ''},\n",
       "  'similarity_score': 0.2462744116783142,\n",
       "  'distance': 0.7537255883216858,\n",
       "  'rank': 7},\n",
       " {'id': 'doc_f0cf26b8_76',\n",
       "  'content': '●\\u200b Mocking: Use unittest.mock to simulate the Supabase client, ensuring that the retrieval \\nlogic handles empty result sets gracefully without crashing. \\n \\n9.2 Tier 2: LLM-based Evaluation (Ragas & DeepEval) \\n \\nTo evaluate the quality of the AI\\'s answers, we use the Ragas framework (Retrieval \\nAugmented Generation Assessment). Ragas uses an \"LLM-as-a-Judge\" (typically GPT-4) to \\nscore the system on specific metrics.38 \\nKey Metrics: \\n1.\\u200b Context Recall: Does the retrieved context contain the ground truth answer? (Measures \\nRetrieval quality). \\n2.\\u200b Faithfulness: Is the generated answer derived solely from the retrieved context, or did \\nthe model hallucinate? (Measures Generation quality). \\n3.\\u200b Answer Relevance: Does the answer directly address the user\\'s query? \\nCI/CD Integration: \\nWe create a \"Golden Dataset\" of 50 Q&A pairs about the candidate. During the CI/CD pipeline \\n(GitHub Actions), a script runs the RAG pipeline against these questions.',\n",
       "  'metadata': {'creationDate': '',\n",
       "   'modDate': '',\n",
       "   'doc_idx': 76,\n",
       "   'moddate': '',\n",
       "   'author': '',\n",
       "   'page': 14,\n",
       "   'trapped': '',\n",
       "   'content_length': 957,\n",
       "   'subject': '',\n",
       "   'total_pages': 18,\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'creator': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'file_path': 'pdf',\n",
       "   'creationdate': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.4'},\n",
       "  'similarity_score': 0.24477815628051758,\n",
       "  'distance': 0.7552218437194824,\n",
       "  'rank': 8},\n",
       " {'id': 'doc_310f3c52_58',\n",
       "  'content': 'effective, the computational overhead is significant. For a personal portfolio, where data \\nvolume is low (typically <50 pages of total text), a rigorous implementation of Semantic \\nChunking combined with Parent-Document retrieval offers the optimal trade-off between \\nperformance and complexity. \\n \\n4. The Retrieval Engine: Moving Beyond Cosine \\nSimilarity \\n \\nA \"naive\" RAG system relies exclusively on dense vector search (embeddings). While powerful \\nfor semantic conceptual matching, dense retrieval struggles with precise keyword matching, \\nparticularly with technical acronyms (e.g., \"C#\" vs. \"C++\", \"AWS\" vs. \"GCP\"). To achieve \\nindustry standards, the portfolio must implement a Hybrid Search architecture. \\n \\n4.1 Hybrid Search Architecture \\n \\nHybrid search combines the strengths of two distinct retrieval algorithms: \\n1.\\u200b Dense Retrieval (Vector Search): Utilizes embeddings (e.g., OpenAI \\ntext-embedding-3-small) to understand semantic intent. It excels at queries like \"Tell me',\n",
       "  'metadata': {'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'keywords': '',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'format': 'PDF 1.4',\n",
       "   'trapped': '',\n",
       "   'doc_idx': 58,\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'total_pages': 18,\n",
       "   'content_length': 989,\n",
       "   'page': 5,\n",
       "   'modDate': '',\n",
       "   'creationDate': '',\n",
       "   'moddate': '',\n",
       "   'creator': '',\n",
       "   'creationdate': '',\n",
       "   'file_path': 'pdf'},\n",
       "  'similarity_score': 0.20432794094085693,\n",
       "  'distance': 0.7956720590591431,\n",
       "  'rank': 9},\n",
       " {'id': 'doc_13af9585_79',\n",
       "  'content': \"Works cited \\n1.\\u200b Optimizing RAG. RAG Demystified: A Hands-On Guide to… | by Skanda Vivek | \\nEMAlpha, accessed November 20, 2025, \\nhttps://medium.com/emalpha/optimizing-rag-bd65ebc5e51a \\n2.\\u200b Developing Retrieval Augmented Generation (RAG) based LLM Systems from \\nPDFs: An Experience Report - arXiv, accessed November 20, 2025, \\nhttps://arxiv.org/html/2410.15944v1 \\n3.\\u200b Using Azure OpenAI GPT-4o to extract structured JSON data from PDF \\ndocuments, accessed November 20, 2025, \\nhttps://learn.microsoft.com/en-us/samples/azure-samples/azure-openai-gpt-4-vi\\nsion-pdf-extraction-sample/using-azure-openai-gpt-4o-to-extract-structured-js\\non-data-from-pdf-documents/ \\n4.\\u200b How can I process a pdf using OpenAI's APIs (GPTs)? - Stack Overflow, accessed \\nNovember 20, 2025, \\nhttps://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-\\nopenais-apis-gpts \\n5.\\u200b Boosting RAG-based intelligent document assistants using entity extraction, SQL\",\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': '',\n",
       "   'doc_idx': 79,\n",
       "   'modDate': '',\n",
       "   'creationDate': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '',\n",
       "   'content_length': 943,\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'file_path': 'pdf',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'format': 'PDF 1.4',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'page': 15,\n",
       "   'trapped': '',\n",
       "   'total_pages': 18},\n",
       "  'similarity_score': 0.19489836692810059,\n",
       "  'distance': 0.8051016330718994,\n",
       "  'rank': 10}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"What is RAG?\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8523d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What all has Piyush worked on?\n",
      "Top_k: 10, Score Threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (1, 384)\n",
      "The distances retrieved are: [0.7024816274642944, 0.7451099157333374, 0.7696207761764526, 0.7750554084777832, 0.8066123127937317, 0.8125970363616943, 0.8330579400062561, 0.8337186574935913, 0.8364413380622864, 0.8369515538215637]\n",
      "Retrieved 10 documents after applying score threshold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_db3818c1_90',\n",
       "  'content': 'Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025',\n",
       "  'metadata': {'keywords': '',\n",
       "   'title': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'trapped': '',\n",
       "   'file_path': 'pdf',\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'doc_idx': 90,\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'subject': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'content_length': 942,\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'total_pages': 1,\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'page': 0,\n",
       "   'author': 'Hemnani, Piyush',\n",
       "   'creationDate': \"D:20251007204636-04'00'\"},\n",
       "  'similarity_score': 0.29751837253570557,\n",
       "  'distance': 0.7024816274642944,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_8afebe56_92',\n",
       "  'content': 'TECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present',\n",
       "  'metadata': {'subject': '',\n",
       "   'file_path': 'pdf',\n",
       "   'total_pages': 1,\n",
       "   'content_length': 855,\n",
       "   'page': 0,\n",
       "   'creationDate': \"D:20251007204636-04'00'\",\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'title': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'doc_idx': 92,\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'keywords': '',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'trapped': '',\n",
       "   'author': 'Hemnani, Piyush',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'format': 'PDF 1.7'},\n",
       "  'similarity_score': 0.2548900842666626,\n",
       "  'distance': 0.7451099157333374,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_a9c74e25_89',\n",
       "  'content': '37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG',\n",
       "  'metadata': {'creationDate': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'doc_idx': 89,\n",
       "   'author': '',\n",
       "   'moddate': '',\n",
       "   'page': 17,\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'file_path': 'pdf',\n",
       "   'modDate': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'content_length': 939,\n",
       "   'keywords': '',\n",
       "   'total_pages': 18,\n",
       "   'creationdate': '',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'creator': '',\n",
       "   'subject': '',\n",
       "   'trapped': ''},\n",
       "  'similarity_score': 0.23037922382354736,\n",
       "  'distance': 0.7696207761764526,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_151eeb7b_83',\n",
       "  'content': 'https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/',\n",
       "  'metadata': {'author': '',\n",
       "   'modDate': '',\n",
       "   'total_pages': 18,\n",
       "   'content_length': 925,\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'moddate': '',\n",
       "   'subject': '',\n",
       "   'creator': '',\n",
       "   'page': 16,\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'creationDate': '',\n",
       "   'file_path': 'pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'doc_idx': 83,\n",
       "   'creationdate': '',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.2249445915222168,\n",
       "  'distance': 0.7750554084777832,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_d35f613b_42',\n",
       "  'content': 'Next.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview',\n",
       "  'metadata': {'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'page': 8,\n",
       "   'moddate': '',\n",
       "   'producer': 'WeasyPrint 65.1',\n",
       "   'creator': 'ChatGPT',\n",
       "   'creationDate': '',\n",
       "   'modDate': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'title': 'Building a Personal Portfolio Q&A Chatbot',\n",
       "   'author': 'ChatGPT Deep Research',\n",
       "   'doc_idx': 42,\n",
       "   'trapped': '',\n",
       "   'subject': '',\n",
       "   'content_length': 973,\n",
       "   'total_pages': 9,\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationdate': '',\n",
       "   'file_path': 'pdf'},\n",
       "  'similarity_score': 0.1933876872062683,\n",
       "  'distance': 0.8066123127937317,\n",
       "  'rank': 5},\n",
       " {'id': 'doc_ce340c7b_46',\n",
       "  'content': 'moves beyond the rudimentary \"Chat with PDF\" MVP to detail a production-grade \\narchitecture that integrates Knowledge Graphs (GraphRAG), Hybrid Search, and Generative \\n3D User Interfaces (GenUI). It rigorously examines the data engineering required to prevent \\nhallucinations, the mathematical principles behind advanced retrieval strategies, and the \\nsoftware engineering practices—specifically unit testing and evaluation—necessary to deploy',\n",
       "  'metadata': {'format': 'PDF 1.4',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'doc_idx': 46,\n",
       "   'keywords': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'moddate': '',\n",
       "   'page': 0,\n",
       "   'trapped': '',\n",
       "   'modDate': '',\n",
       "   'creationDate': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creator': '',\n",
       "   'file_path': 'pdf',\n",
       "   'author': '',\n",
       "   'creationdate': '',\n",
       "   'total_pages': 18,\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'content_length': 443,\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.18740296363830566,\n",
       "  'distance': 0.8125970363616943,\n",
       "  'rank': 6},\n",
       " {'id': 'doc_c9e032e5_69',\n",
       "  'content': 'serialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the \\nnode-edge list. This is saved as graph.json. \\n2.\\u200b Run Time: The Next.js API route loads graph.json. Simple traversal algorithms (e.g., \\nfinding all neighbors of a matched node) are executed in JavaScript/TypeScript. \\n3.\\u200b Integration: The graph results are textually formatted (e.g., \"Related Skills: X, Y, Z\") and \\nappended to the context window alongside vector results. \\nThis \"Client-Side Graph\" approach delivers 80% of the value of Neo4j with 0% of the \\ninfrastructure cost.25 \\n \\n7. Generative UI: The Interactive 3D Interface \\n \\nThe defining feature of the Agentic Portfolio is Generative UI—the ability of the AI to not just \\ntalk, but to show and control the interface. We utilize the Vercel AI SDK and React Server \\nComponents (RSC) to stream UI elements.',\n",
       "  'metadata': {'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'trapped': '',\n",
       "   'creator': '',\n",
       "   'file_path': 'pdf',\n",
       "   'moddate': '',\n",
       "   'author': '',\n",
       "   'doc_idx': 69,\n",
       "   'total_pages': 18,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'page': 10,\n",
       "   'keywords': '',\n",
       "   'modDate': '',\n",
       "   'content_length': 880,\n",
       "   'creationDate': '',\n",
       "   'creationdate': '',\n",
       "   'subject': '',\n",
       "   'format': 'PDF 1.4'},\n",
       "  'similarity_score': 0.1669420599937439,\n",
       "  'distance': 0.8330579400062561,\n",
       "  'rank': 7},\n",
       " {'id': 'doc_a08c7d9c_74',\n",
       "  'content': 'Project Context \\n \\n●\\u200b Framework: Next.js 14 (App Router) \\n●\\u200b UI Library: Shadcn/UI + Tailwind CSS \\n●\\u200b State: Zustand \\n●\\u200b AI: Vercel AI SDK \\n \\nRules \\n \\n1.\\u200b Always use functional components with TypeScript interfaces. \\n2.\\u200b When using Vercel AI SDK, prefer streamText over generateText for latency. \\n3.\\u200b For 3D components, separate logic (hooks) from view (meshes). \\n4.\\u200b Never use useEffect for 3D animations; use useFrame instead.\\u200b\\nThis \"Meta-Prompting\" ensures that the generated code is production-ready and aligns \\nwith the project\\'s specific tech stack, preventing the agent from suggesting outdated \\npatterns (like Pages router or class components).34 \\n \\n9. Evaluation and Testing: Ensuring Reliability \\n \\nDeployment of an AI agent without rigorous testing is professional negligence. RAG systems \\nare non-deterministic, making them prone to \"silent failures\" (hallucinations) that traditional \\nunit tests cannot catch. We must implement a two-tiered testing strategy.',\n",
       "  'metadata': {'page': 13,\n",
       "   'author': '',\n",
       "   'modDate': '',\n",
       "   'creationDate': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'creator': '',\n",
       "   'moddate': '',\n",
       "   'content_length': 971,\n",
       "   'total_pages': 18,\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'doc_idx': 74,\n",
       "   'creationdate': '',\n",
       "   'file_path': 'pdf',\n",
       "   'keywords': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'format': 'PDF 1.4',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.1662813425064087,\n",
       "  'distance': 0.8337186574935913,\n",
       "  'rank': 8},\n",
       " {'id': 'doc_05ad2a38_85',\n",
       "  'content': 'November 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber',\n",
       "  'metadata': {'title': 'Building an Advanced RAG Portfolio',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'moddate': '',\n",
       "   'creationDate': '',\n",
       "   'content_length': 976,\n",
       "   'keywords': '',\n",
       "   'page': 16,\n",
       "   'doc_idx': 85,\n",
       "   'subject': '',\n",
       "   'creator': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creationdate': '',\n",
       "   'author': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'total_pages': 18,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'modDate': '',\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.4'},\n",
       "  'similarity_score': 0.16355866193771362,\n",
       "  'distance': 0.8364413380622864,\n",
       "  'rank': 9},\n",
       " {'id': 'doc_6fa77e32_28',\n",
       "  'content': 'approach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to',\n",
       "  'metadata': {'keywords': '',\n",
       "   'subject': '',\n",
       "   'modDate': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'trapped': '',\n",
       "   'moddate': '',\n",
       "   'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'producer': 'WeasyPrint 65.1',\n",
       "   'creator': 'ChatGPT',\n",
       "   'author': 'ChatGPT Deep Research',\n",
       "   'file_path': 'pdf',\n",
       "   'title': 'Building a Personal Portfolio Q&A Chatbot',\n",
       "   'total_pages': 9,\n",
       "   'creationDate': '',\n",
       "   'page': 5,\n",
       "   'doc_idx': 28,\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationdate': '',\n",
       "   'content_length': 952},\n",
       "  'similarity_score': 0.16304844617843628,\n",
       "  'distance': 0.8369515538215637,\n",
       "  'rank': 10}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"What all has Piyush worked on?\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5545757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: Who is piyush?\n",
      "Top_k: 10, Score Threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (1, 384)\n",
      "The distances retrieved are: [0.7671207189559937, 0.8440251350402832, 0.8935506343841553, 0.9024507999420166, 0.9025073647499084, 0.9193494915962219, 0.9202035069465637, 0.9387513399124146, 0.9439467191696167, 0.9444816708564758]\n",
      "Retrieved 10 documents after applying score threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_db3818c1_90',\n",
       "  'content': 'Piyush Hemnani | Artificial Intelligence Graduate \\nPiyushdeepak97@gmail.com | https://www.linkedin.com/in/piyush-hemnani-05b328189/ | +1-940-843-8403 | Redmond, WA \\n \\nPERSONAL SUMMARY \\n AI/ML Engineer (MS, 4.0 GPA) focused on GenAI, NLP, and Computer Vision with a track record of productionizing models into enterprise workflows. \\nBuilt a multi-agent LLM pipeline (Fal.ai STT + GPT-4.1 + n8n + Jira) that cuts BA ticket update time by 80-90%; delivered OCR automation with 96% \\naccuracy / 93% field precision and 4× throughput vs. manual entry. Comfortable across PyTorch/TensorFlow/Hugging Face, MLflow/Docker/K8s, and \\nAWS/GCP; strong at turning ambiguous requirements into measurable business impact.             \\nEDUCATION \\nUniversity of North Texas                                                                                                                                                                                    May 2025',\n",
       "  'metadata': {'creationDate': \"D:20251007204636-04'00'\",\n",
       "   'title': '',\n",
       "   'doc_idx': 90,\n",
       "   'file_path': 'pdf',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'content_length': 942,\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'page': 0,\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'keywords': '',\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'total_pages': 1,\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'subject': '',\n",
       "   'author': 'Hemnani, Piyush'},\n",
       "  'similarity_score': 0.23287928104400635,\n",
       "  'distance': 0.7671207189559937,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_ce9032ae_91',\n",
       "  'content': 'Master of Science in Artificial Intelligence (Concentration: Machine Learning), GPA – 4.0                                                                                                                               Denton,TX  \\nCoursework : Machine Learning, Deep Learning, NLP, Information Retrieval, Big Data & Data Science, AI Software Development \\n.    \\nBirla Institute of Technology and Science                                                                                                                                                            June 2019 \\nBachelor of Science in Mechanical Engineering with Honours, GPA – 3.7                                                                                                                                                        Dubai, UAE  \\n \\nTECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering',\n",
       "  'metadata': {'file_path': 'pdf',\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'title': '',\n",
       "   'content_length': 936,\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'doc_idx': 91,\n",
       "   'page': 0,\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'total_pages': 1,\n",
       "   'subject': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'author': 'Hemnani, Piyush',\n",
       "   'creationDate': \"D:20251007204636-04'00'\",\n",
       "   'trapped': '',\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf'},\n",
       "  'similarity_score': 0.1559748649597168,\n",
       "  'distance': 0.8440251350402832,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_ea63a461_96',\n",
       "  'content': 'processing. \\n• Containerized training/inference; captured runs with MLflow; packaged for deployment on AWS. \\nPROJECTS \\n  \\nConditional GAN for Ethnicity-Based Face Generation                                                                                                                             UNT, Denton, TX | Sep2024 – Dec 2024 \\n• Engineered a generative adversarial network (GAN) from the ground up to create facial images across six ethnicities (4,000 color images each), enabling data-\\ndriven insights into cross-cultural face generation.  \\n• Transitioned from an initial linear-layer design to convolutional layers for more robust feature extraction, improving model fidelity and output quality. \\n• Leveraged AWS to meet high computational (GPU) demands, streamlining large-scale model training and accelerating development cycles. \\nAutomated Image Segmentation & Classification System  \\n \\n \\n        \\n \\n                                       UNT, Denton, TX | Jan2024 – May 2024',\n",
       "  'metadata': {'page': 0,\n",
       "   'creationDate': \"D:20251007204636-04'00'\",\n",
       "   'file_path': 'pdf',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'title': '',\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'trapped': '',\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'keywords': '',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'total_pages': 1,\n",
       "   'format': 'PDF 1.7',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'author': 'Hemnani, Piyush',\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'content_length': 989,\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'subject': '',\n",
       "   'doc_idx': 96},\n",
       "  'similarity_score': 0.10644936561584473,\n",
       "  'distance': 0.8935506343841553,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_dea75283_68',\n",
       "  'content': '○\\u200b (Person)-->(Company) \\n○\\u200b (Project)-->(Outcome) \\nThis structure enables Multi-hop Reasoning. A user might ask: \"Which projects used a \\ncloud-native database?\" \\n●\\u200b Vector Search: Finds \"Cloud-native database\" $\\\\rightarrow$ matches DynamoDB node. \\n●\\u200b Graph Traversal: (DynamoDB)<--(Project A) and (DynamoDB)<--(Project B). \\n●\\u200b Result: The system identifies Project A and Project B, even if the project descriptions \\nnever explicitly used the phrase \"cloud-native database.\" \\n \\n6.2 Implementation: NetworkX vs. Neo4j \\n \\nFor enterprise applications, graph databases like Neo4j are standard. However, for a personal \\nportfolio with a limited dataset (< 1000 nodes), running a dedicated Neo4j instance is often \\noverkill in terms of cost and maintenance. \\nLightweight Alternative: NetworkX \\nWe recommend using NetworkX (a Python library) to build and query the graph in-memory or \\nserialized to JSON.23 \\n1.\\u200b Build Time: During the build process, an LLM analyzes the profile.json and generates the',\n",
       "  'metadata': {'content_length': 992,\n",
       "   'creationdate': '',\n",
       "   'creationDate': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creator': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'modDate': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'total_pages': 18,\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'trapped': '',\n",
       "   'doc_idx': 68,\n",
       "   'author': '',\n",
       "   'moddate': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'page': 10},\n",
       "  'similarity_score': 0.0975492000579834,\n",
       "  'distance': 0.9024507999420166,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_a9c74e25_89',\n",
       "  'content': '37.\\u200bHow to Use Pytest Fixtures in a RAG-Based LangChain Streamlit App? - Stack \\nOverflow, accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/79717950/how-to-use-pytest-fixtures-in-a-\\nrag-based-langchain-streamlit-app \\n38.\\u200bEvaluate RAG pipeline using Ragas in Python with watsonx - IBM, accessed \\nNovember 20, 2025, \\nhttps://www.ibm.com/think/tutorials/evaluate-rag-pipeline-using-ragas-in-python\\n-with-watsonx \\n39.\\u200bRun your first experiment - Ragas, accessed November 20, 2025, \\nhttps://docs.ragas.io/en/stable/getstarted/experiments_quickstart/ \\n40.\\u200bRAG Evaluation: The Definitive Guide to Unit Testing ... - Confident AI, accessed \\nNovember 20, 2025, \\nhttps://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pi\\npelines-with-deepeval \\n41.\\u200bA Complete Guide to Unit Testing RAG in Continuous Development Workflow, \\naccessed November 20, 2025, \\nhttps://blog.griffinai.io/news/complete-guide-unit-testing-RAG',\n",
       "  'metadata': {'subject': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creationDate': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'page': 17,\n",
       "   'creator': '',\n",
       "   'moddate': '',\n",
       "   'keywords': '',\n",
       "   'total_pages': 18,\n",
       "   'modDate': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'creationdate': '',\n",
       "   'content_length': 939,\n",
       "   'doc_idx': 89,\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'author': ''},\n",
       "  'similarity_score': 0.09749263525009155,\n",
       "  'distance': 0.9025073647499084,\n",
       "  'rank': 5},\n",
       " {'id': 'doc_e2198753_86',\n",
       "  'content': 'accessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber \\n29.\\u200bAccessing the Camera in React Three Fiber out of the canvas - Questions, \\naccessed November 20, 2025,',\n",
       "  'metadata': {'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'creationDate': '',\n",
       "   'file_path': 'pdf',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'format': 'PDF 1.4',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'creationdate': '',\n",
       "   'keywords': '',\n",
       "   'trapped': '',\n",
       "   'modDate': '',\n",
       "   'moddate': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'doc_idx': 86,\n",
       "   'content_length': 234,\n",
       "   'creator': '',\n",
       "   'total_pages': 18,\n",
       "   'page': 16},\n",
       "  'similarity_score': 0.08065050840377808,\n",
       "  'distance': 0.9193494915962219,\n",
       "  'rank': 6},\n",
       " {'id': 'doc_77324881_8',\n",
       "  'content': \"Now, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\",\n",
       "  'metadata': {'doc_idx': 8,\n",
       "   'file_path': 'pdf',\n",
       "   'content_length': 990,\n",
       "   'creationDate': '',\n",
       "   'creationdate': '',\n",
       "   'source_file': 'Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'modDate': '',\n",
       "   'title': 'Building a Personal Portfolio Q&A Chatbot',\n",
       "   'subject': '',\n",
       "   'total_pages': 9,\n",
       "   'moddate': '',\n",
       "   'page': 1,\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf',\n",
       "   'creator': 'ChatGPT',\n",
       "   'keywords': '',\n",
       "   'producer': 'WeasyPrint 65.1',\n",
       "   'author': 'ChatGPT Deep Research'},\n",
       "  'similarity_score': 0.07979649305343628,\n",
       "  'distance': 0.9202035069465637,\n",
       "  'rank': 7},\n",
       " {'id': 'doc_8afebe56_92',\n",
       "  'content': 'TECHNICAL SKILLS \\n  \\nML/GenAI: Transformers, LLM prompt orchestration, RAG, GANs, CNNs, classical CV (FFT, Hough), feature engineering \\nNLP/CV Tooling: PyTorch, TensorFlow, Hugging Face, scikit-learn, spaCy, NLTK, OpenAI APIs \\nMLOps/Infra: MLflow, Docker, Kubernetes, AWS (EC2, S3, Step Functions), GCP; CI/CD fundamentals \\nData/Analytics: SQL, MySQL, Spark/Hadoop basics, pandas, NumPy; EDA, A/B thinking, regression/classification metrics \\nViz/Apps: Tableau, Power BI, matplotlib, Plotly; Chrome extension (JS/HTML/CSS), REST APIs \\nLanguages: Python (primary), R, SQL; basic JS for extensions/front-end integration \\n \\nEXPERIENCE \\n  \\nJr. Developer – AI Automation Intern                                                                                                                                                 Cardinality.ai, MD | Jun 2025 – Present',\n",
       "  'metadata': {'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'source_file': 'Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'subject': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'total_pages': 1,\n",
       "   'keywords': '',\n",
       "   'author': 'Hemnani, Piyush',\n",
       "   'content_length': 855,\n",
       "   'page': 0,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Piyush Hemnani_MLE_AI_Automation_Microsoft.pdf',\n",
       "   'modDate': \"D:20251007204636-04'00'\",\n",
       "   'doc_idx': 92,\n",
       "   'moddate': '2025-10-07T20:46:36-04:00',\n",
       "   'title': '',\n",
       "   'creationdate': '2025-10-07T20:46:36-04:00',\n",
       "   'trapped': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creationDate': \"D:20251007204636-04'00'\"},\n",
       "  'similarity_score': 0.06124866008758545,\n",
       "  'distance': 0.9387513399124146,\n",
       "  'rank': 8},\n",
       " {'id': 'doc_05ad2a38_85',\n",
       "  'content': 'November 20, 2025, https://neo4j.com/blog/genai/graphrag-manifesto/ \\n23.\\u200bExtensive Research into Knowledge Graph Traversal Algorithms for LLMs : r/Rag - \\nReddit, accessed November 20, 2025, \\nhttps://www.reddit.com/r/Rag/comments/1ok8mjr/extensive_research_into_knowl\\nedge_graph_traversal/ \\n24.\\u200bKnowledge Graph Creation with NetworkX | Python Tutorial - YouTube, accessed \\nNovember 20, 2025, https://www.youtube.com/watch?v=o5USzpzKm6o \\n25.\\u200bTiny GraphRAG (Part 1) - Stephen Diehl, accessed November 20, 2025, \\nhttps://www.stephendiehl.com/posts/graphrag1/ \\n26.\\u200bAI SDK - Vercel, accessed November 20, 2025, https://vercel.com/docs/ai-sdk \\n27.\\u200bGenerative User Interfaces - AI SDK UI, accessed November 20, 2025, \\nhttps://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces \\n28.\\u200bHow do you animate the camera with react-three-fiber? - Stack Overflow, \\naccessed November 20, 2025, \\nhttps://stackoverflow.com/questions/75562296/how-do-you-animate-the-camer\\na-with-react-three-fiber',\n",
       "  'metadata': {'content_length': 976,\n",
       "   'keywords': '',\n",
       "   'total_pages': 18,\n",
       "   'doc_idx': 85,\n",
       "   'creationDate': '',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'subject': '',\n",
       "   'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'author': '',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'moddate': '',\n",
       "   'creator': '',\n",
       "   'modDate': '',\n",
       "   'page': 16,\n",
       "   'trapped': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creationdate': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer'},\n",
       "  'similarity_score': 0.0560532808303833,\n",
       "  'distance': 0.9439467191696167,\n",
       "  'rank': 9},\n",
       " {'id': 'doc_151eeb7b_83',\n",
       "  'content': 'https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chun\\nking-strategies-for-rag-applications/ba-p/113089 \\n13.\\u200bAdvanced RAG Techniques for High-Performance LLM Applications - Graph \\nDatabase & Analytics - Neo4j, accessed November 20, 2025, \\nhttps://neo4j.com/blog/genai/advanced-rag-techniques/ \\n14.\\u200bAdvanced Techniques to Build Your RAG System - MachineLearningMastery.com, \\naccessed November 20, 2025, \\nhttps://machinelearningmastery.com/advanced-techniques-to-build-your-rag-sy\\nstem/ \\n15.\\u200bAbout billing on Supabase, accessed November 20, 2025, \\nhttps://supabase.com/docs/guides/platform/billing-on-supabase \\n16.\\u200bPostgres vs. Pinecone | Lantern Blog, accessed November 20, 2025, \\nhttps://lantern.dev/blog/postgres-vs-pinecone \\n17.\\u200bPricing & Fees - Supabase, accessed November 20, 2025, \\nhttps://supabase.com/pricing \\n18.\\u200bPricing - Pinecone, accessed November 20, 2025, \\nhttps://www.pinecone.io/pricing/',\n",
       "  'metadata': {'source_file': 'Building an Advanced RAG Portfolio.pdf',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\Building an Advanced RAG Portfolio.pdf',\n",
       "   'producer': 'Skia/PDF m144 Google Docs Renderer',\n",
       "   'title': 'Building an Advanced RAG Portfolio',\n",
       "   'moddate': '',\n",
       "   'creator': '',\n",
       "   'file_path': 'pdf',\n",
       "   'creationdate': '',\n",
       "   'creationDate': '',\n",
       "   'total_pages': 18,\n",
       "   'keywords': '',\n",
       "   'content_length': 925,\n",
       "   'modDate': '',\n",
       "   'doc_idx': 83,\n",
       "   'page': 16,\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'format': 'PDF 1.4',\n",
       "   'trapped': ''},\n",
       "  'similarity_score': 0.05551832914352417,\n",
       "  'distance': 0.9444816708564758,\n",
       "  'rank': 10}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"Who is piyush?\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610d6f1",
   "metadata": {},
   "source": [
    "### RAG Pipeline - Vector Store to LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d67c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8ef89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_LLM:\n",
    "    ''' Handle Output generation using OpenAI LLMs '''\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4.1-mini\", api_key: str = None):\n",
    "\n",
    "        '''\n",
    "        Initializing the OpenAI LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: str - Name of the model to use\n",
    "            api_key: str - OpenAPI Key\n",
    "        '''\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAI API key not provided. Set it via argument or environment variable\")\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=self.model_name, \n",
    "            openai_api_key=self.api_key,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "            )\n",
    "        \n",
    "        print (f\"Initialized OpenAI LLM with model: {self.model_name}\")\n",
    "\n",
    "        self.system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "            '''You are Piyush Hemnani's Personal Portfolio Agent.\n",
    "Your job is to respond using only the retrieved context from the vector store.\n",
    "\n",
    "Rules:\n",
    "\n",
    "Ground every answer ONLY in retrieved text. No guessing or hallucinating.\n",
    "\n",
    "If the question asks about personal details not found in retrieved docs, reply:\n",
    "\n",
    "“That information is not available in my current portfolio dataset.”\n",
    "\n",
    "Be structured and professional when answering recruiter or job-related queries.\n",
    "\n",
    "When the user asks about “projects,” “skills,” or “experience,” summarize the retrieved info clearly and concisely.\n",
    "\n",
    "If multiple retrieved chunks overlap, merge them into a clean narrative.\n",
    "\n",
    "NEVER add new facts not found in retrieval.\n",
    "\n",
    "If asked non-portfolio questions (e.g., unrelated trivia), politely redirect:\n",
    "\n",
    "“I can only answer questions about Piyush based on the portfolio data I have.”\n",
    "\n",
    "Tone:\n",
    "\n",
    "Professional\n",
    "\n",
    "Helpful\n",
    "\n",
    "Concise\n",
    "\n",
    "Resume-aware (structured, keyword-rich)'''\n",
    "        )\n",
    "\n",
    "        self.human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "                        '''Use the following context to answer the question.\n",
    "                        ------------------------------------\n",
    "                        Question:\n",
    "                        {query}\n",
    "                        ------------------------------------\n",
    "                        Retrived Context:\n",
    "                        {context}\n",
    "                        ------------------------------------\n",
    "            '''\n",
    "        )\n",
    "\n",
    "\n",
    "        self.chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            [self.system_prompt, self.human_prompt]\n",
    "        )\n",
    "\n",
    "        self.chains: RunnableSequence = self.chat_prompt | self.llm \n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        ''' \n",
    "        Generate response from LLM with retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user query\n",
    "            context (str): The retrieved context to condition the response\n",
    "            max_length (int): Maximum length of the response\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response from the LLM\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            response = self.chains.invoke({\"query\": query, \"context\": context})\n",
    "            return response.content\n",
    "    \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d09476db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized OpenAI LLM with model: gpt-4.1-mini\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.OpenAI_LLM at 0x1c9fa2f9fd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_llm = OpenAI_LLM()\n",
    "openai_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_docs = rag_retriver.retrieve(\"Who is Piyush Hemnani?\", top_k=5)\n",
    "for i, doc in enumerate(ret_docs):\n",
    "    print(i)\n",
    "    print(doc['content'])\n",
    "\n",
    "context = \"\\n\\n\".join([doc['content'] for doc in ret_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d54e7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        return \"\\n\\n\".join(doc[\"content\"] for doc in docs)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to format docs: {e}\")  # don't pollute LLM context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acbe019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: Who is piyush?\n",
      "Top_k: 5, Score Threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 74.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding with shape: (1, 384)\n",
      "The distances retrieved are: [0.7671207189559937, 0.8440251350402832, 0.8935506343841553, 0.9024507999420166, 0.9025073647499084]\n",
      "Retrieved 5 documents after applying score threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n",
      "\n",
      "Piyush Hemnani is an Artificial Intelligence graduate with a Master of Science in Artificial Intelligence (concentration in Machine Learning) from the University of North Texas, maintaining a 4.0 GPA. He is an AI/ML Engineer specializing in Generative AI, Natural Language Processing (NLP), and Computer Vision, with experience in productionizing models into enterprise workflows. Piyush has developed solutions such as a multi-agent large language model pipeline that significantly reduces business analyst ticket update time and delivered OCR automation with high accuracy and throughput. He is proficient in technologies including PyTorch, TensorFlow, Hugging Face, MLflow, Docker, Kubernetes, AWS, and GCP. His background also includes a Bachelor of Science in Mechanical Engineering with Honors from Birla Institute of Technology and Science.\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_runnable = RunnableLambda(lambda q: rag_retriver.retrieve(q, top_k=5))\n",
    "\n",
    "# RAG Chain\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough()\n",
    "    |{\n",
    "        \"query\":RunnablePassthrough(),\n",
    "        \"context\": RunnablePassthrough()\n",
    "        | retriever_runnable\n",
    "        | RunnableLambda(format_docs),\n",
    "    }\n",
    "    | openai_llm.chains\n",
    ")\n",
    "\n",
    "rag_chain_text = rag_chain | RunnableLambda(lambda msg: msg.content)\n",
    "\n",
    "while True:\n",
    "    q = input(\"Insert query (or type 'exit'): \")\n",
    "\n",
    "    if q.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    answer = rag_chain_text.invoke(q)\n",
    "    print(\"\\n--- ANSWER ---\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05ee89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077cca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
