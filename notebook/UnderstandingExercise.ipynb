{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a94a5ea",
   "metadata": {},
   "source": [
    "### DATA INGESTION\n",
    "\n",
    "What all is goin to happen in the Data ingestion pipeline\n",
    "- Main aim is to load some data\n",
    "- Apply some chunking\n",
    "- Convert into embeddings \n",
    "- Store into vector DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bc99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we need to understand document datastructure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46440aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'First_RAG_example.txt', 'pages': 1, 'author': 'Piyush Hemnani', 'date_created': '2025-11-14'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is the content of the document.\", \n",
    "    metadata={\n",
    "        \"source\": \"First_RAG_example.txt\", \n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Piyush Hemnani\",\n",
    "        \"date_created\": \"2025-11-14\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4435249",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653926db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample files created successfully.\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "print(\"Sample files created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01cfe9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e8c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 149.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\", \n",
    "    glob=\"*.txt\",                       ## Pattern to match files\n",
    "    loader_cls=TextLoader,              ## Loader class to use (if there are multiple options we can make this a list of loaders)\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True\n",
    "    )\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0b5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content=\"Start thinking about the content that might go into this personal knowledge base. For a portfolio Q&A\\nchatbot about you, the data could include things like:\\nA written bio or introduction: e.g. your background, education, skills, projects, interests.\\nResume or CV data: your work experience, accomplishments, maybe in a structured Q&A form\\n(“Where did I work in 2022?”, “What projects have I done in machine learning?”, etc.).\\nFrequently asked questions about you: This could be a list of questions and answers (FAQ) that you\\nanticipate someone might ask. For example: “What are your expertise areas?”, “What was your MSc\\nthesis about?”, “What hobbies do you have?” – along with the answers in your own words.\\nAny personal blog posts or writings: if relevant, these can provide context on your opinions or\\nknowledge areas.\\nProjects portfolio details: short descriptions of key projects you’ve done, which the chatbot could\\ndraw on if asked “Tell me about project X”.\\nSince you want the chatbot to answer based only on what “it knows” (i.e. your provided data, with no live\\ninternet connection), we will either be training a model on this data or feeding this data into a retrieval\\nsystem for the model. In either case, you’ll need to gather and curate the information about yourself. This\\ncan be done incrementally: once we decide on the approach, you can compile the data into the needed\\nformat (documents, Q&A pairs, etc.). \\nIf we go with a fine-tuning approach: you may need to format the data as a training dataset (for example, a\\nlist of prompt-response pairs where the prompt is a question about you and the response is the correct\\nanswer). You don’t necessarily need thousands of examples – a smaller high-quality dataset could suffice –\\nbut you do need enough coverage of facts about you so the model can learn them. Ensure the info is\\naccurate and expressed in the tone you want the answers to have.\\nIf we go with a retrieval-based approach: you might store the data as a set of documents or text passages.\\nThese could be chunks of a “About Me” document, or individual Q&A entries, etc. The quality of answers will\\ndepend on providing sufficient detail in these source texts. The nice thing is that you can start with a basic\\nset of documents (like a few paragraphs about you, plus a list of Q&A) and always update or expand it later\\nwithout retraining a model – we’ll discuss this more under the retrieval approach.\\nNow, let’s explore the two main implementation options for the Q&A system:\\nApproach 1: Training a Personal LLM (Fine-Tuning)\\nAs an AI engineer, the idea of training your own small-scale language model for this purpose is exciting. The\\nconcept here is to  fine-tune a language model on data about yourself so that it can directly answer\\nquestions  about  you.  This  fine-tuned  model  would  essentially  internalize  your  personal  data  into  its\\nweights.\\nHow it can be done: Rather than training from scratch (which would require enormous data and compute),\\nyou would take a pre-trained model (e.g., an open-source LLM like Meta’s LLaMA-2, GPT-J, GPT-NeoX, etc. or\\na smaller one depending on resource constraints) and fine-tune it on a custom dataset about you. This\\ndataset could be a collection of question-answer pairs, or even just a formatted text with instructions. For\\nexample, you could create a dataset of pairs like (“What is [Your Name]'s primary field of expertise?”, “[Your\\nName] is an AI engineer with a focus on NLP and LLMs, currently working on...”) along with many other\\n• \\n• \\n• \\n• \\n• \\n2\"),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Q&As covering your background, skills, projects, etc. The fine-tuning process will adjust the model’s weights\\nso that it “learns” these specific facts and can respond in a conversational style about them.\\nThanks to techniques like Low-Rank Adaptation (LoRA), it’s feasible to fine-tune moderately large models\\non a single GPU. In fact, LoRA has been shown to allow fine-tuning a 7-billion-parameter model (such as\\nLLaMA-2 7B) on a single GPU\\n. One report notes: “LoRA allows us to finetune 7B parameter LLMs on a single\\nGPU. In [one case], using QLoRA (quantized LoRA) with optimal settings required ~17.8 GB GPU memory and about\\n3 hours on an A100 GPU for 50k training examples”\\n. This means that if you have access to a GPU with\\n~24GB VRAM (or use a cloud service), you could potentially fine-tune a model on a custom dataset without\\nhuge expense. Since your dataset about yourself will likely be much smaller than 50k examples, the training\\nwould be faster (possibly an hour or two, depending on the model and hyperparameters).\\nState-of-the-art techniques you might consider for this include LoRA/QLoRA (to reduce memory and\\ncompute), and using an instruction-tuned base model. For example, starting with an instruction-following\\nmodel (like LLaMA-2-chat or Dolly, etc.) might yield better conversational answers after fine-tuning. You\\ncould also experiment with lightweight fine-tuning vs full fine-tuning. LoRA is nice because it keeps the\\noriginal model intact and just learns small adapter weights – this is efficient and you can revert to the base\\nmodel easily if needed.\\nBefore committing to fine-tuning, consider the pros and cons:\\nPros of training your own model:\\nThe model will have your data baked in. It won’t need to look anything up at runtime; it “knows”\\nthe info (within the limits of what it was trained on).\\nIt can be run locally or on your server without external API calls, preserving privacy (important if\\nsome personal data is sensitive).\\nAs an AI engineer, you get the learning experience of doing a fine-tune with SOTA methods. You can\\nexperiment with parameters, try new fine-tuning optimizations, etc., which can be valuable\\nexperience.\\nThe inference might be slightly faster per query than a retrieval approach for small queries, since it’s\\njust the model response (no vector database lookup overhead) – though in practice the difference\\nmay be small.\\nCons of fine-tuning approach:\\nData requirements: Fine-tuning is effectively training, so if you have very little data about yourself,\\nthe model might not generalize well or might overfit. The rule of thumb is you don’t want to fine-\\ntune with too few examples or only extremely narrow phrasing. You might need to be creative in\\ngenerating enough Q&A pairs (possibly augmenting with rephrased questions) so the model sees\\nsufficient variety. As one discussion put it: don’t try to fine-tune when you have too little data or just to\\ninject a few facts – that’s what prompt context or retrieval is for; “Don’t use a bulldozer to kill a fly.”\\n.\\nFine-tuning a model to learn new knowledge (like personal facts it didn’t know) is possible but\\nworks best if you provide a reasonably sized corpus of that knowledge\\n.\\nMaintenance and updates: If your personal information changes or you want to add new data (say\\nyou start a new job or complete new projects), you’d have to fine-tune a new version of the model or\\nat least do an incremental update. This is non-trivial. The fine-tuning approach is “static” – once\\n5\\n5\\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n6\\n6\\n7\\n• \\n3'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='trained, the model’s knowledge is fixed. As one source notes, fine-tuning is “powerful on paper, but\\nexpensive, time-consuming, and a nightmare to maintain every time your data changes”\\n.\\nCompute and cost: While a small-scale fine-tune is much cheaper than training from scratch, it’s not\\nfree. You need suitable hardware. If using cloud GPUs, that could cost some money (though a single\\n1-3 hour run on an A100 or similar might be on the order of tens of dollars, which isn’t too bad). Still,\\nif you iterate multiple times, it adds up. Also hosting the final model (for inference) means you need\\na server (or at least something like a GPU or CPU instance) to run the model continuously for your\\nwebsite. A 7B model can run on CPU but might be slow; more likely you’d run on a GPU for snappier\\nresponses, which has an ongoing cost if in the cloud.\\nQuality and hallucinations: A fine-tuned smaller model may not match the raw power of a larger\\nbase model. Open-source models are improving rapidly, but something like a fine-tuned 7B or 13B\\nparameter model will be less fluent and sometimes less accurate than, say, GPT-4. It might also\\nhallucinate answers if asked something outside of what it was trained on (or even confuse facts if the\\nprompt is tricky). Notably, fine-tuning doesn’t inherently fix the hallucination problem of LLMs\\n.\\nThe model might still “make up” an answer if asked a question it doesn’t know and you haven’t built\\nin a mechanism to handle that (like a “I don’t know” response).\\nExpertise required: Setting up the fine-tuning (data preparation, choosing hyperparameters, using\\nlibraries like Hugging Face’s Trainer or LoRA implementations) requires some ML ops work. It’s\\ndefinitely doable (especially since you’re an AI engineer), but it’s more involved than the retrieval\\napproach. One write-up on private knowledge chatbots explicitly pointed out that to fine-tune an\\nopen-source model for a knowledge base, you need “specialized talent and a large amount of time to\\nsolve the fine tuning challenge internally”\\n. In other words, be prepared for some experimentation\\nand debugging.\\nIn summary, training a personal mini-LLM using fine-tuning is feasible and would be our first choice to\\nexplore if you’re keen on it. It gives you a self-contained model that can answer questions about you.\\nHowever, be mindful that this approach is best if you have a decent amount of personal data to teach the\\nmodel and if you’re ready to handle updates via retraining. Given the constraints (limited data and budget),\\nwe should compare this with Approach 2, which might be more efficient for the task.\\nApproach 2: Retrieval-Augmented Q&A (Using a Context File or\\nVector DB)\\nThe alternative (and increasingly common) approach is to avoid training altogether and instead use a pre-\\ntrained model with a retrieval mechanism. This is often called Retrieval-Augmented Generation (RAG). In\\nthis setup, you  provide the model with relevant information at query time by retrieving it from a\\nknowledge base of your documents. The model then generates an answer using that information as\\ncontext.\\nIn practical terms, you’d maintain a database (or simply a collection of texts) that contains all your personal\\ndata (the same kind of data we discussed: your bio, Q&A pairs, etc.). When a user asks a question on your\\nportfolio site, the system will pull out the parts of that data that are most relevant to the question and feed\\nthose, along with the question, into the prompt for the LLM. The LLM (which could be a large pre-trained\\nmodel like GPT-3.5, GPT-4, or an open-source model you host) then answers based only on that provided\\ncontext.\\n8\\n• \\n• \\n9\\n• \\n9\\n4'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Example architecture of a retrieval-augmented Q&A system. A user’s query is first used to fetch relevant knowledge\\n(documents or data) from a store (vector database or search index), and that knowledge is combined with the\\nquery as input to the LLM. The LLM then produces an answer grounded in the provided data\\n.\\nThis approach has several advantages for your use case: - No model training needed: The heavy lifting\\nhas been done by the base model. You don’t adjust its weights; you just give it information. This avoids the\\nexpense and complexity of fine-tuning. As one source explains, you “keep the model as it is and simply plug it\\ninto your knowledge sources… the model retrieves the latest info at runtime, answers in context, and stays\\naccurate without retraining”\\n. This is a big win for maintainability. -  Easy to update information: If\\nsomething about your personal data changes or you want to add more content, you just update the\\ndocuments or knowledge base. The next question asked will then retrieve from the new data immediately.\\nNo need to retrain a model or deploy new weights. This makes the system much easier to keep up-to-date\\n(no “nightmare to maintain every time your data changes” as with constant fine-tuning\\n). -  Smaller\\ndeployment footprint: Instead of hosting a custom model, you could use an API (like OpenAI’s) or host a\\nsmaller model just for inference. The retrieval step might need a vector database, but there are lightweight\\noptions (even in-memory). Many real-world chatbot systems use this pattern because it scales well – you\\ncan  swap  in  a  better  base  model  or  improve  your  knowledge  store  independently.  -  Accuracy  and\\ngrounding: Because the answers are drawn from provided text, you reduce the chance of the model\\nhallucinating incorrect facts about you. Essentially, the model is forced to base its answer on the snippets of\\ntext you supply. (Of course, the model could still do a poor job or hallucinate connections between facts, but\\nif prompted to only use the given info, it tends to stick to it). A Reddit user summarizing best practices\\nnoted that “Prompt design matters as much as retrieval. Instruct the model to stick to provided excerpts… This\\nreduces hallucinations and builds trust with users.”\\n. You can even have the model cite the sources (in a\\nmore advanced implementation), which is common in RAG setups for enterprise knowledge bases. - Speed\\nand cost: For a small personal chatbot, the difference might be minor, but in general RAG can be cheaper\\nand faster. You’re not paying the cost of training. At query time, vector search is usually fast (milliseconds to\\na few tens of ms), and then you call the model to generate an answer. If using an API like OpenAI, you pay\\nper call (plus maybe a small cost for vector DB if using a cloud one). If using an open source model, the\\ncompute to run it is similar to if it were fine-tuned. Many have concluded that “RAG has become the go-to\\nchoice for many use cases: it’s faster, cheaper, and way more practical for real-world teams”\\n.\\nHow it can be implemented: The general flow is: 1. Indexing your personal data: Take your documents\\nor Q&A pairs about yourself and break them into chunks (e.g. paragraphs or individual Q&A entries). For\\neach chunk, generate an  embedding vector (a numeric representation of the text meaning) using an\\n10\\n8\\n8\\n11\\n12\\n5'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='embedding model\\n. Store these vectors in a vector database (or use a simple in-memory vector index if\\nthe data is small) along with the chunk text. There are many tools for this; libraries like  LangChain or\\nLlamaIndex can automate a lot of it, and vector DBs like Pinecone, Weaviate, or open-source ones like FAISS\\nor Qdrant can store the data\\n. A Reddit comment succinctly described this: “chunk your docs, embed them,\\nand store in a vector database. At query time, retrieve the most relevant chunks and pass them into the\\nLLM...”\\n. 2. Retrieval on query: When a question is asked (“How many years of experience does [Your\\nName] have in AI?” for example), the system creates an embedding for the query and searches for similar\\nvectors in your vector DB. It might return, say, the top 3 chunks that are most relevant – maybe one of those\\nchunks is a part of your bio stating “... has 5 years of experience in AI and machine learning...”. 3. Construct\\nprompt with context: The retrieved text chunks are then combined with the user’s question to form the\\nprompt for the LLM. For instance, the prompt might be something like:  “Context: [excerpt from your bio\\nsaying you have 5 years in AI]. Q: How many years of experience do I have in AI? A:” – and the model will\\nhopefully respond: “You have 5 years of experience in AI.” 4.  Model answer: The LLM (which could be\\nrunning via an API or a local model) generates an answer using the context. If all goes well, the answer will\\nbe correct, because the needed info was in the provided context. We can also instruct the model with a\\nsystem/message prompt to only use that info and not deviate.\\nThe architecture image above illustrates this flow in a generic way: the query goes to a retrieval component\\n(in that diagram, “Azure AI Search” plays the role of vector DB/search engine) which returns knowledge, and\\nthat knowledge + query go into the LLM to get a final answer\\n. Notably, this approach requires no fine-\\ntuning of the LLM’s weights – the model remains as-is (pretrained on general data), and we just augment\\nits input with relevant data at runtime\\n.\\nTools and options: Since you’re an AI engineer, you might enjoy building the pieces yourself, or you can use\\nexisting frameworks: - LangChain and LlamaIndex (formerly GPT Index) are high-level libraries that let you set\\nup a QA chain over your documents very quickly. They handle splitting text, embedding (you can choose\\nmodels like OpenAI’s text-embedding-ada or local ones), vector store integration, and the query workflow\\n. For example, LangChain has a RetrievalQA  chain that does exactly this once you provide it a vector\\nstore and an LLM. These libraries also help with prompt management. - Vector database choices: If you\\nprefer not to rely on an external cloud service, you can use an open-source solution. FAISS (by Facebook)\\ncan run in-memory or on disk and is often used for small to medium cases. For larger scale or convenience,\\nservices like Pinecone or Weaviate can host it (though for a personal portfolio, that’s probably overkill).\\nThere are lightweight options like an SQLite + embeddings or even just computing cosine similarity on the\\nfly for small data. Given your data will be relatively small (maybe a few pages of text in total), even a simple\\napproach would work. The key is the concept, not the specific tech. - Model choice for answering: You\\nhave options here too. If you want to keep everything self-hosted, you could run an open-source model (for\\nexample, a 7B or 13B parameter model that’s been instruction-tuned, like LLaMA-2 Chat or Dolly or FLAN-\\nT5-XXL, etc.). The model doesn’t need to be fine-tuned on your data, because the data comes in via the\\nprompt. If you have budget and are okay with relying on an external API, you could call OpenAI’s GPT-3.5 or\\nGPT-4 with the prompt. Since the domain is narrow (just info about you), even GPT-3.5 Turbo might handle it\\nwell and is quite cheap per call. There are also open APIs like Cohere or others that could work. But using a\\nlocal model would align with the “build my own” spirit more, and new open models (like LLaMA 2) are quite\\ncapable at following instructions. - No live data needed: As you specified, this system does not need to\\nfetch real-time info from the internet. All knowledge is static in your provided data. That’s perfectly in line\\nwith RAG – the knowledge base is whatever you load into the vector store. It won’t go out and search\\nbeyond that. If a question is asked that isn’t answerable from your data, the ideal behavior is to say “I don’t\\n13\\n14\\n15\\n10\\n16\\n17\\n18\\n6'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='know” or some graceful fallback. You can program the prompt to encourage that (e.g., “If the answer is not\\nin the provided context, say you don’t know.”).\\nPotential downsides of retrieval approach: - It is a bit more moving parts: you have to set up an\\nembedding process and store. However, for a one-person project, this is fairly straightforward and many\\ntutorials exist. It’s arguably less work than doing a fine-tune from scratch. - At query time, the model’s\\nresponse is limited by what it sees in the context. If your context window (the prompt length) of the model\\nis, say, 4,000 tokens, and your entire personal knowledge base is larger than that, the retrieval step must be\\neffective at picking the right pieces of info. If it misses something relevant, the answer might be incomplete.\\nBut since personal data likely isn’t huge, and questions tend to focus on one aspect at a time, this is\\nmanageable. - If not properly instructed, the model could ignore the context and hallucinate. But in practice,\\nif you supply a relevant context chunk, models like GPT-3.5 or LLaMA-2-chat will use it when answering. -\\nOne consideration: if you want the Q&A to have some memory or multi-turn conversation about you, you’d\\nhave to include previous Q&A in context as well. But since it’s mostly fact-based about you, each question\\ncan probably be handled independently (stateless Q&A).\\nGiven the above, the retrieval-based approach is quite appealing for simplicity and robustness. It’s generally\\nthe  preferred method in industry for Q&A bots on custom data because of the maintenance and\\naccuracy benefits. Fine-tuning is usually only chosen if the use case demands it (for example, if you needed\\nthe model to deeply internalize a style or do complex transformations, or if retrieval latency was a big\\nissue).\\nComparison and Recommendation\\nBoth approaches can ultimately achieve your goal: a chatbot that answers questions about you, without\\nhooking into live external sources. The best choice depends on your priorities (learning experience vs.\\nsimplicity, one-time effort vs. ongoing flexibility).\\nTraining a Personal LLM might be your first inclination as an AI engineer because it’s an interesting\\nproject. It will let you experiment with the latest fine-tuning techniques (LoRA, QLoRA, etc.) and truly “own”\\nthe model that results. If you go this route, try to leverage existing models and do a relatively lightweight\\nfine-tune: - You could start with a 7B or 13B parameter model that is known to perform well in Q&A/chat\\n(for instance, LLaMA-2 13B Chat has good performance). Use LoRA to fine-tune it on a curated set of Q&A\\nabout you. Monitor for overfitting – since your dataset might be small, you might only do one or two epochs\\nover it\\n. It’s even possible that just a few hundred training steps could suffice if using a high-quality base\\nmodel. - Ensure your fine-tuning dataset is high quality and diverse within the realm of “about you.” If\\nthere are specific phrasings or tricky factual questions (like dates, spellings of names, etc.), include those.\\nThe model will memorize those facts. Be cautious: the model might generalize in unintended ways (you\\nwouldn’t want it to start answering beyond your data and being wrong). - After training, you’ll need to\\ndeploy the model. For a portfolio site, you might run the model on a server that the site can send requests\\nto. Running a 7B model with int8 quantization on CPU is possible but may be slow (several seconds per\\nanswer). Running on a GPU (even a cheap one) or using a model served via an API (like HuggingFace\\nInference Endpoint or similar) could be better for snappy responses.\\nUsing Retrieval (RAG) is, in contrast, more of a software engineering solution than a model-training\\nsolution. My recommendation is to strongly consider this approach, because it aligns well with having\\nlimited data and budget but needing accurate results. Here’s how you might implement it step by step: 1.\\n19\\n7'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Begin with an open-source model or an API. For example, try OpenAI’s GPT-3.5 Turbo on some manually\\ncrafted prompts using your data (even before setting up any vector DB) to see how it performs when given\\ncontext. This costs very little and gives a baseline. You can later swap to an open model if you want to self-\\nhost. 2. Use a library like LangChain to index your personal info. You could literally have a Python script\\nwhere you input a bunch of strings (your bio, some Q&As) and it uses an embedding model (say OpenAI’s\\nembeddings,  or  SentenceTransformers  locally)  to  create  vectors  and  store  them  in  something  like\\nChromaDB (which is an easy local vector store that LangChain supports). 3. Hook up a simple API route (if\\nusing  Next.js,  for  example)  that  takes  a  user’s  question,  does  the  retrieval,  and  returns  the  answer.\\nLangChain’s RetrievalQA can do the retrieval and call the LLM for you in one go. This can be done with only\\na few dozen lines of code once the environment is set up. 4. Test and refine: see if the answers are accurate.\\nIf the bot ever says “I don’t know that” for something you did provide in the data, you might need to ensure\\nthe embedding is picking it up or add more context. You can also tweak prompts (e.g., add a system\\nmessage: “You are a chatbot that answers questions only using the provided context about [Your Name]. If\\nyou cannot find the answer in the context, say you do not know.”).\\nCost-wise, retrieval approach can be very cheap, especially if you use local models. If using an API, you pay\\nper call but the usage for a personal portfolio (with presumably low traffic) is negligible. Fine-tuning has an\\nupfront cost (compute for training) but then usage of the model is just the cost of running a server.\\nIt’s worth noting one hybrid idea: you could fine-tune a smaller model and use retrieval with it. For example,\\nfine-tune a 7B model on a small dataset just to give it some familiarity with your style or key facts, but still\\nuse a vector store to feed it more detailed or less frequently used facts. This is probably unnecessary here –\\nit’s more complex and the pure retrieval method should suffice – but it’s an option if you find the fine-tuned\\nmodel alone isn’t reliable for less common questions.\\nIn conclusion, if we “look at both options” as you requested: - Option 1 (Personal LLM via fine-tuning):\\nFeasible with LoRA on a 7B/13B model; provides a self-contained model; requires more upfront work and\\ndoesn’t update easily; might be chosen for the learning experience and autonomy. It will work for your use\\ncase if done right, but keep expectations reasonable in terms of model accuracy. - Option 2 (RAG with no\\ntraining): More straightforward and likely to give accurate, up-to-date answers; leverages powerful existing\\nmodels; minimal cost to maintain; easier to scale or improve incrementally. For a “simple ask questions\\nabout me and get answers” goal, this is arguably the best way to complete the task with least friction. The\\nfact that industry solutions favor RAG for Q&A on custom data\\n is a strong indicator.\\nGiven  the  constraints  (limited  data  and  budget)  and  the  desire  for  state-of-the-art  techniques,  my\\nsuggestion would be: Why not do both, sequentially? You could implement the RAG approach first to have\\na working chatbot quickly, and then, in parallel or later, experiment with fine-tuning a model on the same\\ndata to see how it compares. This way, your portfolio has a reliable Q&A function (backed by retrieval and a\\nrobust model), and you still get to play with training a model as a side project (which you can swap in if it\\nbecomes good enough, or at least blog about the process as an AI engineer!). This combined approach\\nleverages the strength of RAG for now, while keeping the door open for a custom LLM when it’s viable.\\nTo directly answer your question:  Yes, you can use Vercel and host on your own domain (just add a\\ncustom domain in Vercel settings and point your DNS records accordingly)\\n. For the chatbot, using\\nNext.js would streamline the integration of your AI backend and frontend. Start gathering your personal\\ndata in a structured way, then pursue a retrieval-based solution as the primary path (it’s quick to set up and\\nvery effective). Meanwhile, plan out a fine-tuning experiment on a small LLM as an educational first choice –\\n12\\n3\\n8'),\n",
       " Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='use LoRA to keep it cheap and see how well it performs. By comparing both, you’ll see which one meets\\nyour needs in practice. Many have found RAG to be “faster, cheaper, and way more practical” for Q&A\\nbots\\n, but with your expertise you might get a surprisingly good result with a tailored mini-LLM as well.\\nGood luck with building your personal AI-powered portfolio site!\\nSources:\\nVercel Custom Domain Documentation\\nContentful Blog – Advantages of Next.js (built-in routing and backend)\\nSebastian Raschka – LoRA Fine-tuning 7B models on single GPU\\nReddit (LocalLLaMA) – Advice on knowledge-base chatbots (RAG workflow)\\nStack AI Blog – Fine-tuning vs RAG for custom chatbots\\nSwirlAI Newsletter – Challenges with fine-tuning vs retrieval\\nMicrosoft Learn (Azure AI) – RAG architecture and description\\nNext.js vs. React: The difference and which framework to choose | Contentful\\nhttps://www.contentful.com/blog/next-js-vs-react/\\nAdding & Configuring a Custom Domain\\nhttps://vercel.com/docs/domains/working-with-domains/add-a-domain\\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\\nA comprehensive overview of everything I know about fine-tuning. : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ilkamr/a_comprehensive_overview_of_everything_i_know/\\nHow to Build an AI Chatbot with Custom Knowledge Base RAG\\nhttps://www.stack-ai.com/blog/how-to-build-ai-chatbot-with-knowledge-base\\nSAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\\nhttps://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\\nRAG and generative AI - Azure AI Search | Microsoft Learn\\nhttps://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\\nWhat is the best way to create a knowledge-base specific LLM chatbot ? : r/LocalLLaMA\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/\\n12\\n• \\n3\\n4\\n• \\n2\\n• \\n5\\n• \\n15\\n11\\n• \\n8\\n12\\n• \\n9\\n13\\n• \\n10\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n19\\n8\\n12\\n9\\n13\\n10\\n16\\n17\\n11\\n14\\n15\\n18\\n9')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdfs\", \n",
    "    glob=\"*.pdf\",                       ## Pattern to match files\n",
    "    loader_cls=PyMuPDFLoader,              ## Loader class to use (if there are multiple options we can make this a list of loaders)\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a662c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'WeasyPrint 65.1', 'creator': 'ChatGPT', 'creationdate': '', 'source': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'file_path': '..\\\\data\\\\pdfs\\\\Building a Personal Portfolio Q&A Chatbot.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'Building a Personal Portfolio Q&A Chatbot', 'author': 'ChatGPT Deep Research', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Building a Personal Portfolio Q&A Chatbot\\nFramework and Hosting Considerations\\nChoosing the right framework for your portfolio site is important for ease of development and deployment.\\nNext.js is a React-based framework that provides server-side rendering (SSR), static site generation (SSG),\\nbuilt-in routing, and easy integration of backend logic via API routes\\n. These features can improve\\nperformance and SEO (since pages can be pre-rendered or SSR) and simplify development (routing and\\nconfiguration work out of the box). For example, Next.js allows you to “easily create your custom backend\\nfunctionalities with API Routes to power your own front end”, all without extra client-side bloat\\n. This\\nmeans you could host your Q&A model’s API or inference logic within the same Next.js project if needed. \\nBy contrast, React (without Next.js) typically means using a tool like Create React App or Vite to build a\\nsingle-page  application.  React  alone  is  just  a  frontend  library;  you  would  handle  routing,  server-side\\nrendering, and any backend services separately. A pure React app would run entirely on the client (client-\\nside rendering), which is fine for interactivity but less ideal for SEO and requires additional setup if you need\\na server (for example, to interface with an AI model or database). If your portfolio is mostly static or you\\ndon’t need SSR, a React SPA could work, but you’d likely need to set up a separate backend for the chatbot’s\\nlogic (or use cloud functions). In summary:\\nNext.js Pros: SSR/SSG for fast, SEO-friendly pages; file-system routing; API routes for backend;\\nautomatic code-splitting and other performance optimizations\\n. Perfect if you want an all-in-\\none solution (frontend + backend) and plan to deploy on Vercel.\\nReact Pros: Simpler library if you only need a purely client-side app. However, you’ll write more\\nboilerplate for things Next.js provides out of the box. You might choose this if you want complete\\ncontrol over tooling or if SSR isn’t a concern.\\nSince you are open to using Vercel: Vercel is actually the company behind Next.js, and it excels at hosting\\nNext.js apps, though it can also host any static or Node.js app (including a React SPA). One big advantage is\\nthat Vercel makes it trivial to add a custom domain to your project. Yes – you can absolutely use Vercel’s\\ninfrastructure with your own domain. By default, deployments get a your-project.vercel.app  URL,\\nbut you can add your own domain name in your Vercel dashboard and point DNS records to it\\n.\\nVercel’s docs note that this provides “greater personalization and flexibility” for your project by allowing you\\nto use a custom domain instead of the default URL\\n. In practice, you’ll add the domain in Vercel, then\\nupdate your domain registrar’s DNS (usually adding an A record or CNAME) as instructed. Once configured,\\nyour portfolio will be accessible at your own domain, even though it’s hosted via Vercel’s servers.\\nPreparing Personal Data for Q&A\\nYou indicated you do not have the personal data prepared yet, which is fine. The approach we choose will\\ndetermine what kind of data and in what format you should prepare it. The goal is to enable the system\\n(whether a fine-tuned model or a prompt-based system) to accurately answer questions about you using\\nauthentic information that you provide.\\n1\\n2\\n2\\n• \\n1\\n2\\n• \\n3\\n4\\n3\\n1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af3021",
   "metadata": {},
   "source": [
    "### embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1438ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b294d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\Categorized Folders\\RAG_Trial\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1b7380323c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    '''Handles document embedding generation using Sentence Transformer'''\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "\n",
    "        '''\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name for the sentence embedding.\n",
    "        '''\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        '''Load the sentence transformer model'''\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        '''\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to embed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of embeddings. numpy array of embeddings with shape (len(texts), embedding_dimension)\n",
    "        '''\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "### Initialize Embedding Manager and generate embeddings for sample documents\n",
    "embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ad25cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (2, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"This is a test.\", \"RAG pipelines are fun.\"]\n",
    "embs = embedding_manager.generate_embeddings(docs)\n",
    "embs.shape   # (2, 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8beb8d",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31dfda23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized with collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1b73bd2c1a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    ''' Manages document embeddings in ChromaDB vector store '''\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", presist_directory: str = \"../data/vector_store\"):\n",
    "        '''\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection.\n",
    "            presist_directory (str): Directory to persist the vector store data.\n",
    "        '''\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = presist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        '''Initialize Chromadb client and collection'''\n",
    "\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path = self.persist_directory)\n",
    "\n",
    "            # Create or get collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name, \n",
    "                metadata={\"description\": \"Collection for PDF document embeddings\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized with collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: list[Any], embeddings:np.ndarray):\n",
    "        '''\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of Langchain Document \n",
    "            embeddings: Corresponding embeddings for the dcuments\n",
    "        '''\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"The number of documents must match the number of embeddings.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_texts = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique IDs\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['context_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document text\n",
    "            documents_texts.append(doc.page_content)\n",
    "\n",
    "            # Embedding \n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "\n",
    "            # Add to collection\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids =ids,\n",
    "                    metadatas=metadatas,\n",
    "                    documents = documents_texts,\n",
    "                    embeddings = embeddings_list\n",
    "                )\n",
    "                print(f\"Successfully added {len(documents)} documents.\")\n",
    "                print(f\"Total documents in collection now: {self.collection.count()}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding documents to vector store: {e}\")\n",
    "                raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e82de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdoc\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63830463",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchunks\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be741f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
